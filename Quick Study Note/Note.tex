% default 12pt
\documentclass[12pt]{article}
 
% \usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,scrextend}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{mathtools}
% \usepackage{fancyhdr}
\usepackage[margin=0.20in]{geometry}
\usepackage{pgfpages}
% % For shrinking 4 pages to 1
% \pgfpagesuselayout{4 on 1}[a4paper, border shrink=0mm]
% 2 in 1
\pgfpagesuselayout{2 on 1}[a4paper, border shrink=0mm, landscape]

\setlength{\parskip}{0em}
\setlist[enumerate]{itemsep=0mm}
\setlist[itemize]{itemsep = 0mm}

\theoremstyle{definition}
\newtheorem{definition}{DEFINITION}[subsection]


\pagestyle{fancy}

\let\newproof\proof
\renewenvironment{proof}{\begin{addmargin}[1em]{0em}\begin{newproof}}{\end{newproof}\end{addmargin}\qed}

\newtheorem{theorem}{THEOREM}[subsection]

\DeclarePairedDelimiterX{\infdivx}[2]{(}{)}{%
  #1\;\delimsize\|\;#2%
}
% \DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\uline}[1]{\rule[0pt]{#1}{0.4pt}}
\newcommand{\trace}[1]{\text{tr}(#1)}
\newcommand{\Hom}{\text{Hom}}
\newcommand{\Maps}{\text{Maps}}
\newcommand{\image}{\text{im}}
\newcommand{\Mat}{\text{Mat}}
\newcommand{\suchthat}{\textit{ s.t. }}
\newcommand{\sgn}{\textbf{sgn}}
\newcommand{\adj}{\text{adj}}
\newcommand{\diag}{\text{diag}}
\newcommand{\transpose}[1]{#1^\mathsf{T}}

%Set the course name here
\newcommand{\coursename}{Honours Algebra}

\usepackage{tcolorbox}
\tcbuselibrary{theorems}

\newtcbtheorem[number within=section]{mytheo}{Theorem}
{colback=red!5,colframe=red!35!black,fonttitle=\bfseries}{th}
\newtheorem{lemma}{LEMMA}[subsection]
\newtheorem{prop}{PROPOSITION}[subsection]
\newtheorem{corollary}{COROLLARY}[subsection]
\newtheorem{example}{EXAMPLE}[subsection]
 
\lhead{\coursename}
\rhead{Quick Notes}

\title{\coursename\\Quick Notes}
\author{Ian S.W. Ma}

 
 
\begin{document}
\maketitle
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
\pagenumbering{gobble}
% \tableofcontents
% \newpage

\pagenumbering{arabic}
\setcounter{page}{1}
\section{Vector Spaces}

% Section 1.1
\subsection{Solution of Simultianeous Linear Equations}
Assume \(F = \mathbb{Q},\mathbb{R} \text{ or } \mathbb{C} \), where $a_{ij},b_i \in F$, then
\[
    \sum_{j=1}^m{a_{ij}x_j} = b_i \quad \forall i \in [1,n]:i \in \mathbb{Z}
\]
is a \textbf{system of linear Equations}

\begin{itemize}
    \item if all $b$'s are $0$ then the system is \textbf{homogenous}
    \item $L = \{x_1,...,x_m\}$ is the \textbf{solution set} of Equations
\end{itemize}

% Section 1.2
\subsection{Fields and Vector Spaces}

\begin{definition}
    \quad
    \begin{enumerate}
        \item
              A \textbf{field} $F$ is a set with functions:
              \begin{itemize}
                  \item \textbf{addition} $= +:F\times F \rightarrow F; (\lambda,\mu)\mapsto\lambda+\mu$
                  \item \textbf{multiplication} $= \cdot:F\times F \rightarrow F; (\lambda,\mu)\mapsto\lambda\mu$
              \end{itemize}
              such that $(F,+)$ ans $(F\backslash\{0\},\cdot)$ are abelian groups, with:
              $$\lambda(\mu + \nu) = \lambda\mu + \lambda\nu \in F \quad \forall \lambda,\mu,\nu\in F$$
              The neutral elements are called $0_F,1_F$, in particular for all $\lambda,\mu\in F$
              \begin{itemize}
                  \item $\lambda + \mu = \mu + \lambda \in F$
                  \item $\lambda \cdot \mu = \mu \cdot \lambda \in F$
                  \item $\lambda + 0_F = \lambda \in F$
                  \item $\lambda\cdot 1_F = \lambda \in F$
              \end{itemize}
              For all $\lambda \in F$ there exists $-\lambda\in F$ such that $\lambda + (-\lambda) = 0_F \in F$\\
              For all $\lambda \neq 0 \in F$ tehre exists $\lambda^{-1}\neq 0 \in F$ such that $\lambda(\lambda^{-1}) = 1_F\in F$
        \item
              A \textbf{vector space} $V$ \textbf{over a field} $F$ is  a pair consisting of an abelian group $V = (V,\dot{+})$ and a mapping
              $$F\times V \rightarrow V; (\lambda,\overrightarrow{v})$$
              such that for all $\lambda,\mu\in F$ and $\overrightarrow{v},\overrightarrow{w}\in V$ the following identities hold:
              \begin{itemize}
                  \item $\lambda(\overrightarrow{v}+\overrightarrow{w}) = \lambda\overrightarrow{v} + \lambda\overrightarrow{w}$ \textbf{Distributive Law}
                  \item $(\lambda+\mu)\overrightarrow{v} = \lambda\overrightarrow{v} + \mu\overrightarrow{v}$ \textbf{Distributive Law}
                  \item $\lambda(\mu\overrightarrow{v}) = (\lambda\mu)\overrightarrow{v}$ \textbf{Associativity Law}
                  \item $1_F\overrightarrow{v} = \overrightarrow{v}$
              \end{itemize}
    \end{enumerate}
\end{definition}
\begin{lemma}
    If $V$ is a vector space then $\forall \overrightarrow{v} \in V,\quad 0\overrightarrow{v} = \overrightarrow{0}$
\end{lemma}
\begin{lemma}
    If $V$ is a vector space then $\forall \overrightarrow{v} \in V, \quad (-1)\overrightarrow{v} = -\overrightarrow{v}$
\end{lemma}
\begin{lemma}
    If $V$ is a vector space over a field $F$ then $\lambda\overrightarrow{0} = \overrightarrow{0} \quad \forall \lambda\in F$
\end{lemma}

% Section 1.3
\subsection{Product of Sets and of Vector Spaces}
\begin{itemize}
    \item
          \textbf{Cartesian product} of sets: $X_1 \times ... \times X_n := \{(x_1,...,x_n):x_i\in X_i \text{ for } 1 \leq i \leq n\}$, an element of this product is known as a \textbf{product n-tuples}.\newline
\end{itemize}
There are special mappings called \textbf{projections} for a cartesian product
\[\begin{split}
        \text{pr}_i: X_1\times ... \times X_n &\rightarrow X_i\\
        (x_1,...,x_n) &\mapsto x_i
    \end{split}\]
The cartesian product of $n$ copies of a set $X$ is written in short as $X^n$\\
$\forall n,m \geq 0$, $X^n\times X^m\xrightarrow{\sim} X^{n+m}; ((x_1,...,x_n),(x_{n+1},...,x_{n+m}))\mapsto(x_1,...,x_n,x_{n+1},x_{n+m})$

\begin{example}[Examples of tuples]
    \quad
    \begin{itemize}
        \item Vector spacse of $n$-tubles over $F$: $V = F^n$
        \item Vector space as space around us
    \end{itemize}
\end{example}

% Section 1.4
\subsection{Vector Subspaces}
\begin{definition}
    A subset $U$ of a vector space $V$ is called a \textbf{vector subspace} or \textbf{subspace} if $U$ contains the zero vector ($\overrightarrow{0}$) and whenever $\overrightarrow{u},\overrightarrow{v}$ and $\lambda \in F$ we have $\overrightarrow{u}+\overrightarrow{v}\in U$ and $\lambda\overrightarrow{u}\in U$
\end{definition}
\begin{prop}
    Let $T$ be a subset of vector space $V$ over a field $F$. Then amongst all vector subspaces of $V$ that include $T$ there is a smallest vector subspace
    $$\langle T \rangle = \langle T \rangle_F \subseteq V$$
\end{prop}
\begin{definition}
    A subset $S$ of a vector space $V$ is called a \textbf{generating set} of a $V$ if its span is all of the vector space. A vector space that has a finite generating set is \textbf{finitely generated}
    $$ S \subseteq V \wedge \text{span}(S) = V \Rightarrow \text{$S$ is a generating set of $V$}$$
\end{definition}
\begin{definition}
    If $X$ is a set, then the set of all subsets $\mathcal{P}(X) = \{U:U \subseteq X\}$ of $X$ is called \textbf{power set} of $X$. We call the subset of $\mathcal{P}(X)$ a $\textbf{system of subsets of $X$}$. Given such a system $\mathcal{U \subseteq \mathcal{P}(X)}$ we can create two new subsets of $X$, the \textbf{union} and the \textbf{intersection} of the set of our system $\mathcal{U}$ as follows:
    \[
        \begin{split}
            \bigcup_{U \in \mathcal{U}} U &= \{x \in X: \text{there is } U \in \mathcal{U} \text{ with } x \in U\}\\
            \bigcap_{U \in \mathcal{U}} U &= \{x \in X: x \in U \quad\forall U \in \mathcal{U}\}
        \end{split}
    \]
\end{definition}

% Section 1.5
\subsection{Linear Independence and Bases}
\begin{definition}
    A subset $L = \{\overrightarrow{v_1},...,\overrightarrow{v_n}\}$ of a vector subspace $V$ is \textbf{linearly independent} if for all arbitrary scalars $\alpha_1, ..., \alpha_n \in F$:
    $$\sum_{i=1}^n{\alpha_i\overrightarrow{v_i}} = 0 \rightarrow \alpha_1 = ... = \alpha_n = 0$$
\end{definition}
\begin{definition}
    A subset $L = \{\overrightarrow{v_1},...,\overrightarrow{v_n}\}$ of a vector subspace $V$ is \textbf{linearly dependent} if it's not \textbf{linearly independent}, whcih mean there exist some $\alpha_j \in \{a_1, ..., a_n\}, \alpha_j \neq 0$ such that  $$\sum_{i=1}^n{\alpha_i\overrightarrow{v_i}} = 0$$
\end{definition}
\begin{definition}
    A \textbf{basis of a vector space} $B$ of a vector space $V$ is a linearly independent generating set in $V$
    \[
        \text{span}(B) = V \wedge B \text{ is linearly independent} \Rightarrow \text{$B$ is a basis of $V$}
    \]
\end{definition}
\begin{definition}
    Let $F$ be a field, $V$ a vector space over $F$ and $\overrightarrow{v_1},...,\overrightarrow{v_r} \in V$ vectors. The \textit{family} $(\overrightarrow{v_i})_{1\leq i\leq r}$ is a basis of $V$ if and only if the following "\textit{evaluation}"
    \[
        \begin{split}
            \Phi : F^r &\rightarrow V\\
            (\alpha_1,...,\alpha_r) &\mapsto \alpha_1\overrightarrow{v}_1 + ... + \alpha_r\overrightarrow{v}_r
        \end{split}
    \]
    is a bijection
\end{definition}
\begin{definition}
    The following for a subset $E$ of a vector space $V$ are equivalent:An isomorphism of a vector space to itself is called \textbf{anautomorphism} of our vector space.
    \begin{itemize}
        \item Our subset $E$ is a basis, ie. a linearly independent generating set;
        \item Our subset $E$ is a \textbf{minimal} among all generating sets, meaning that $E\backslash\{\overrightarrow{v}\}$ does not generate $V$
        \item Our subset $E$ is a \textbf{maximal} among all linearly independent subsets, meaning that $E\cup\{\overrightarrow{v}\}$ is not linearly independent for any $\overrightarrow{v} \in V$
    \end{itemize}
\end{definition}
\begin{corollary}[The existence of a basis]
    Let $V$ be a finitely generated vector space over a vield $F$, then $V$ is a basis
\end{corollary}

\begin{theorem}
    Let $V$ be a vector space.
    \begin{itemize}
        \item If $L \subset V$ is a linearly independent subset and $E$ is a minimal amongst all generating sets of our vector with $L \subseteq E$, then $E$ is a basis.
        \item If $L \subseteq V$ is a generating set and if $L$ is maximal amongst all linearly independent subsets of vector space with $L \subseteq E$, then $L$ is a basis.
    \end{itemize}
\end{theorem}

\begin{definition}[To infinity but not beyond]
    Let $X$ be a set and $F$ a field. The set $\Maps(X,F)$ od all mappings $f: X \rightarrow F$ becomes an $F$-vector space with the operations of pointwise addition and multiplication by a scalar. The subset of all mappings with send almost all elements of $X$ to 0 is a vector subspace
\end{definition}

\begin{theorem}
    Let field $F$, $F$-vector space $V$ and family of vectors $(\overrightarrow{v_i})_{i\in I}$ from $V$, The following are equivalent:
    \begin{itemize}
        \item The family $(\overrightarrow{v_i})_{i\in I}$ is a basis of $V$;
        \item For each vector $\overrightarrow{v} \in V$ there is percisely one family $(a_i)_{i\in I}$ of elements of field $F$, almost all of which are zero and such that: $$\overrightarrow{v} = \sum_{i\in I} {a_i\overrightarrow{v_i}}$$
    \end{itemize}
\end{theorem}

% 1.6
\subsection{Dimension of a vector space}
\begin{theorem}[Fundamental Estimate of Linear Algebra]
    No linearly independent subset of a given vector space has more elements then a generating set. Thus if $V$ is a vector space, $L \subset V$ a linearly independent subset and $E \subseteq V$ a generating set, then $|L| \leq |E|$
\end{theorem}

\begin{theorem}[Steinitz Exchange Theorem]
    Let $V$ be a vector space, $L \subset V$ a finite linearly independent subset and $E \subseteq V$ a generating set. Then there is an injection $\phi: L \hookrightarrow E$ such that $(E \backslash \phi (L)) \cup L$ is also a generating set for $V$
\end{theorem}

\begin{lemma}[Exchange Lemma]
    Let $V$ be a vector space, $M \subseteq V$ a linearly independent subset, and $E \subseteq V$ a generating subset, such that $M \subseteq E$. If $\overrightarrow{w} \in V\backslash M$ is a vector not belonging to $M$ such that $M \cup \{\overrightarrow{w}\}$ is linearly independent, then there exists $\overrightarrow{e} \in E\backslash M$ such that $\{E\backslash\{\overrightarrow{e}\}\}\cup \{\overrightarrow{w}\}$ is a generating set for $V$
\end{lemma}

\begin{corollary}[Cardinality of Bases]
    Let $V$ be a finitely generated vector space.
    \begin{itemize}
        \item $V$ has a finite basis
        \item $V$ cannot have an infinite basis
        \item Any two bases of $V$ have the same number of elements
    \end{itemize}
\end{corollary}

\begin{definition}
    The cardinality of one (and by Cardinality of Bases each) basis of a finitely generated vector space $V$ is called the \textbf{dimension} of $V$ and will be denoted by $\dim(V)$. If the vector space is not finitely generated, then we write $\dim(V) = \infty$ and call $V$ infinite dimensional. As usual, we will ignore the difference between infinities.
\end{definition}

\begin{corollary}[Cardinality Criterion for Bases]
    Let $V$ be a finitely generated vector space.
    \begin{itemize}
        \item Each linearly independent subset $L \subset V$ has at most $\dim(V)$ elements, and if $|L|=\dim(V)$ then $L$ is actually a basis
        \item Each generating set $E \subseteq V$ has at least $\dim(V)$ elements, and if $|E| = \dim(V)$ the $E$ is actually a basis.
    \end{itemize}
\end{corollary}

\begin{corollary}[Dimension Estimate for Vector Subspaces]
    A proper vector subspace of a finite dimensional vector space has itself a strictly smaller dimension.
\end{corollary}

\begin{theorem}[The Dimension Theorem]
    Let $V$ be vectpr space containing vector subspaces $U,W \subseteq V$. Then
    $$\dim(U + W) + \dim(U\cap W) = \dim(U) + \dim(W)$$
\end{theorem}

% 1.7
\subsection{Linear Mappings}
\begin{definition}[Definition od homomorphism, isomorphism, and automorphism]
    Let $V,W$ be a vector spaces over a field $F$. A mapping $f: V \rightarrow W$ is called \textbf{linear} or more percisely $\mathbf{F}\textbf{-linearly}$ or even a \textbf{homomorphism of $F$-vector spaces} if for all $\overrightarrow{v}_1, \overrightarrow{v}_2 \in V$ and $\lambda \in F$ we have \[\begin{split}
        f(\overrightarrow{v}_1 + \overrightarrow{v}_2) &= f(\overrightarrow{v}_1) + f(\overrightarrow{v}_2)\\
        f(\lambda\overrightarrow{v}_1) &= \lambda f(\overrightarrow{v}_1) 
    \end{split}\]
\end{definition}

\begin{itemize}
    \item A \textbf{bijective linear mapping} is called as \textbf{isomorphism} of vector spaces. If there is an isomorphism bwetween two vector spaces we say them \textbf{isomorphic}.
    \item A \textbf{homomorphism} from one vector space to itself is called anendomorphismof our vector space.
    \item An isomorphism of a vector space to itself is called an \textbf{automorphism} of our vector space.
\end{itemize}

\begin{definition}
    A point that is sent to itself br a mapping is called a \textbf{fixed point} of the mapping.
    Given a mapping $f: X \rightarrow X$, we donote the set of fixed points by $$X^f = \{x \in X: f(x) = x\}$$
\end{definition}

\begin{definition}
    Two vector subspaces $V_1,V_2$ of a vector space $V$ are called \textbf{complementary} of addition defines a bijection $V_1 \times V_2 {\stackrel{\sim}{\rightarrow}} V$
\end{definition}

\begin{theorem}[The Classification of Vector Spaces by their Dimension]
    Let $n\in \mathbb{N}$. Then a vector space $V$ over a field $F$ is isomorphic to $F^n$ i.f.f. $\dim(V) = n$
\end{theorem}

\begin{lemma}[Linear Mappings and Bases]
    Let $V,W$ be vector spaces over $F$ and let $B \subset V$ be a basis. Then restriction of a mapping gives a bijection
    \[\begin{split}
        \Hom_F(V,W) &\stackrel{\sim}{\rightarrow} \Maps(B,W)\\
        f &\mapsto f|_B
    \end{split}\]
\end{lemma}

\begin{prop}
    \quad
    \begin{itemize}
        \item Every injective linar mapping $f:V \hookrightarrow W$ has a \textbf{left inverse}, in other words $\exists$ a linear mapping $g: W \rightarrow V$ such that $g \circ f = \text{id}_V$
        \item Every surjective linear mapping $f:V \twoheadrightarrow W$ has a \textbf{right inverse}, in other words $\exists$ a linear mapping $g: W \rightarrow V$ such that $g \circ f = \text{id}_W$
    \end{itemize}
\end{prop}

\subsection{Rank-Nullity Throrem}
\begin{definition}
    The \textbf{image} of a linear mapping $f: V \rightarrow W$ is the subset $\image(f) = f(V) \subseteq W$. The \textbf{preimage} of the zero vector (\textbf{kernel}) of a linear mapping $f: V \rightarrow W$ is denoted by $$\ker(f) := f^{-1}(0) = \{v \in V: f(v) = 0\}$$
    The kernel is a vector subspace if $V$
\end{definition}

\begin{lemma}
    A linear mapping $f: V \rightarrow W$ is injective if an only if it's kernel is zero.
\end{lemma}

\begin{theorem}[Rank-Nullity Theorem]
    Let $f: V \rightarrow W$ be a linear mapping between vector spaces, then $\dim(V) = \dim(\ker{f}) + \dim(\image(f))$
\end{theorem}

% Chapter 2
% \newpage
% 2.1
\section{Linear Mappings and Matrices}
\subsection{Linear Mappings $F^m \rightarrow F^n$ and Matrices}
\begin{theorem}[Linear mappings $F^m \rightarrow F^n$ and Matrices]
    Let $F$ be a field and let $m,n \in \mathbb{N}$ be neutral numbers. There is a nijection between the space of linear mappings $F^m \rightarrow F^n$ and the set of matrices woth $n$ rows anf $m$ columns and entries in $F$:
    \[\begin{split}
        \textbf{M}: \Hom_F(F^m,F^n) &\stackrel{\sim}{\rightarrow} \Mat(n \times m; F)\\
        f &\mapsto [f]
    \end{split}\]
    This attaches to each linear mapping$f$ its \textbf{representing matrix} $\textbf{M}(f):=[f]$. The column of this matrux are the images under $f$ of the standard basis elements if $F^m$:
    $$[f] = (f(\overrightarrow{e}_1)|f(\overrightarrow{e}_2)|\dots|f(\overrightarrow{e}_m))$$
\end{theorem}

\begin{definition}
    Let $n,m,l\in \mathbb{N}$, $F$ a field and let $A \in \Mat(n\times m; F)$ and $B \in \Mat(m\times l; F)$ be matrices. The \textbf{product} $A \circ B = AB \in \Mat(n\times l; F)$ is the matrix defined by $$(AB)_{ik} = \sum_{j=1}^m{A_{ij}B_{jk}}$$
\end{definition}

\begin{theorem}[Composition of Linear Mapping and Products of Matrices]
    Let $g:F^l \rightarrow F^m$ and $f:F^m \rightarrow F^n$ be linear mappings. Then $[f \circ g] = [f] \circ [g]$
\end{theorem}

% 2.2
\subsection{Basic Properties of Matrices}
\begin{definition}
    A matrix $A$ is called \textbf{invertable} if there exist matrices such tht $BA = I$ and $AC = I$
\end{definition}

\begin{definition}
    will define an \textbf{elementary matrix} to be any square matrix that differs from the identity matrix in at most one entry.
\end{definition}

\begin{theorem}
    Every square matrix with entries in a field can be written as a product if elementary matrices.
\end{theorem}

\begin{definition}
    Any matrix whose only non-zero entries lies on the diagonal, and which has first 1's along the diagonal and then 0's, is said to be in \textbf{Smith Normal Form}:
    \[A_{ij} = \begin{cases}\begin{split}
        0 &\text{ if } i \neq j\\
        1 &\text{ if } A_{(i+1)(j+1)} = 1\\
        0 &\text{ otherwise}
    \end{split}\end{cases}\]
\end{definition}

\begin{theorem}[Transformation of a Matrix into Smith Normal Form]
    For each matrix $A \in \Mat(n\times m;F)$ there exist invertable matrices $P,Q$ such that $PAQ$ is a matrix in Smith Normal Form.
\end{theorem}

\begin{definition}
    The \textbf{column rank} of a matrix $A \in \Mat(n\times m;F)$ is the dimension of the subsequence of $F^n$ generated by the columns of $A$. Simmilarly, the \textbf{row rank} of $A$ is the dimension of the subspace of $F^m$ generated by the rows of A.
\end{definition}

\begin{theorem}
    The column rank and the row rank of any matrix are equal.
\end{theorem}

Let's now refer the column and row rank as \textbf{rank} for the sake of not losing any generality.

\begin{definition}
    When the rank is as big as possible, meaning that it's equal to either the number of rows or number of columns (whichever is smaller), then the matrix has \textbf{full rank}
    \[
        \text{rank}(M) = \min(\{\text{rowrank}(M), \text{colrank}(M)\}) \Rightarrow M \text{ has full rank}
    \]
\end{definition}

% 2.3
\subsection{Abstract Linear Mappings and Matrices}
\begin{theorem}[Abstract  Linear  Mappings  and  Matrices]
    Let $F$ be a field, $V,W$ vector spaces over $F$ with ordered bases $\mathcal{A} = (\vec{v}_1,...,\vec{v}_m)$ and $\mathcal{B} = (\vec{w}_1,...,\vec{w}_n)$. Then to each linear mapping $f:V\rightarrow W$ we assosiate bases a \textbf{representing matrix} $_\mathcal{B}[f]_\mathcal{A}$ whose entried $a_ij$ are defined by the identity $$f(\vec{v}_j) = \sum_{i=1}^n{a_{ij}\vec{w}_i}\in W$$
    This produces a bijection, which is event an isomorphismof vector spaces:
    \[\begin{split}
        \textbf{M}_\mathcal{B}^\mathcal{A}: \Hom_F(V,W) &\stackrel{\sim}{\rightarrow} \Mat(n \times m; F)\\
        f &\mapsto _\mathcal{B}[f]_\mathcal{A} 
    \end{split}\]
\end{theorem}
We call $\textbf{M}_\mathcal{B}^\mathcal{A}(f) = _\mathcal{B}[f]_\mathcal{A}$ the \textbf{epresenting matrix of the mapping with respect to the bases $\mathcal{A}$ and $\mathcal{B}$}

\begin{theorem}[The Representing Matrix of a Composition of Linear Mappings]
    Let $F$ be a field and $U,V,W$ finite dimensional vector spaces over $F$ with ordered bases $\mathcal{A,B,C}$. If $f:U\rightarrow V$ and $g:V \rightarrow W$ are linear mappings, then the representing matrixof the composition $g \circ f: U \rightarrow W$ is the matrix product of the representing matrix of $f$ and $g$: $$_\mathcal{C}[g\circ f]_\mathcal{A} = _\mathcal{C}[g]_\mathcal{B} \circ _\mathcal{B}[f]_\mathcal{A}$$
\end{theorem}

\begin{definition}
    Let $V$ be a finite dimensional vector space with an ordered basis $\mathcal{A} = (\vec{v}_1,...,\vec{v}_m)$. We will denote the invese to the bijection of $\Phi_\mathcal{A} \stackrel{\sim}{\rightarrow} V,(\alpha_a,...,\alpha_m)^T \mapsto \sum_{i=1}^m{\alpha_i\vec{v}_i}$ by
    $$\vec{v} \mapsto _\mathcal{A}[\vec{v}]$$
\end{definition}

\begin{theorem}[Representation of the Image of a Vector]
    Let $V,W$ be finite dimensional vector spaces over $F$ with ordered bases $\mathcal{A,B}$ and let $f:V\rightarrow W$ be a linear mapping. The following holds for $\vec{v} \in V$:
    $$_\mathcal{B}[f(\vec{v})] = _\mathcal{B}[f]_\mathcal{A} \circ _\mathcal{A}[\vec{v}]$$
\end{theorem}

% 2.4
\subsection{Change of Matrix by Change of Basis}
\begin{definition}
    Let $\mathcal{A} = (\vec{v}_1,...,\vec{v}_n), \mathcal{B} = (\vec{w}_1,...,\vec{w}_n)$ be ordered bses of the same $F$-vector space $V$. Then the matrix representing the identity mapping with respect to the bases $_\mathcal{B}[\text{id}_V]_\mathcal{A}$ is called a \textbf{change of basis matrix}. By definition, its entries are given by the equalities $\vec{v}_j = \sum_{i=1}^n{a_{ij}\vec{w}_i}$
\end{definition}

\begin{theorem}[Change of Basis]
    Let $V,W$ be finite dimensional vector spaces over $F$ and let $f:V \rightarrow W$ br a linear mapping. Suppose that $\mathcal{A,A'}$ are order bases of $V$ and $\mathcal{B,B'}$ are ordered bases of $W$. Then:
    $$_\mathcal{B'}[f]_\mathcal{A'} = _\mathcal{B'}[\text{id}_W]_\mathcal{B} \circ _\mathcal{B}[f]_\mathcal{A} \circ _\mathcal{B}[\text{id}_V]_\mathcal{A'}$$
\end{theorem}

\begin{corollary}
    Let $V$ be a finite dimensional vector space and let $f:V\rightarrow V$ be an endomorphism of $V$. Suppose that $\mathcal{A,A'}$ are ordered bases od $V$. Then 
    $$_\mathcal{A'}[f]_\mathcal{A'} = _\mathcal{A}[\text{id}_V]_\mathcal{A'}^{-1} \circ _\mathcal{A}[f]_\mathcal{A} \circ _\mathcal{A}[\text{id}_V]_\mathcal{A'}$$
\end{corollary}

\begin{theorem}[Smith Normal Form]
    Let $f:V\rightarrow W$ be a linear mapping between finite dimensional $F$-vector spaces. There exist an order basis $\mathcal{A}$ of $V$ and an ordered basis $\mathcal{B}$ of $W$, such that the representing matrix $_\mathcal{B}[f]_\mathcal{A}$ has zero entries everywhere except possibly on one diagonal, and along the diagonal there are 1's first, followed by 0's  
\end{theorem}

\begin{definition}[Trace]
    The trace of a square matrix is defined to be the sum of its diagonal entries:
    $$\text{tr}(A) = \sum_{i=1}^{n}{a_{ii}}$$\\
    \begin{itemize}
        \item Let $A \in \Mat(n \times m, F)$, $B \in \Mat(m \times n, F)$, then $\trace{AB} = \trace{BA}$
        \item Let $f: V \rightarrow W$ and $g: W \rightarrow V$ two linear mappings where $V$ and $W$ are both finite dimensional $F$-vector spaces, then $\trace{fg} = \trace{gf}$
        \item Let $V$ be a finite dimensional $F$-vector space and let $f:V \rightarrow V$ be an idempotent, that is $f^2 = f$, then $\trace{f} = \dim(\image{f})$
        \item Let $V$ be a finite dimensional $F$-vector space and $f: v \rightarrow V$ a linear mapping, then\\ $\trace{((f\circ)|\text{End}_F(V)) = (\dim_F V)\trace{f|V}}$
    \end{itemize}
\end{definition}

% Chapter 3
\section{Rings and Modules}
% 3.1
\subsection{Rings}
\begin{definition}
    A \textbf{ring} is a set with two operations $(R,+,\cdot)$ that satisfy:
    \begin{enumerate}
        \item $(R,+)$ is an abelian group
        \item $(R,\cdot)$ is a \textbf{monoid}, meaning that the second operation $\cdot$: $R\times R\rightarrow R$ is assosiative and that there is an \textbf{identity element} $1 = 1_R \in R$, often called just the \textbf{identity}, with the peoperty that $1\cdot a = a \cdot 1 = a \quad\forall a \in R$
        \item The Distributive laws hold, meaning that for all $a,b,c \in R$ \[\begin{split}
            a\cdot (b+c) &= (a\cdot b)+(a \cdot c)\quad \textbf{addition}\\
            (a+b) \cdot c &= (a \cdot c) + (b \cdot c) \quad \textbf{multiplication}
        \end{split}\]
    A ring which element is commutative, that means $a \cdot b = b \cdot a \quad \forall a,b \in R$, is a \textbf{commutative ring}
    \end{enumerate}
\end{definition}

\begin{prop}[Divisbility by Sum]
    A natural number is divisible by 3(respectively by 9) percisely when the sum of its digits is divisible by 3 (or 9)
\end{prop}

\begin{definition}
    A \textbf{field} is a non-zero commutative ring $F$ in which every non-zero element $a\in F$ has an inverse $a^{-1} \in F$, that is an element $a^{-1}$ with the property that $a\cdot a^{-1} = a^{-1}\cdot a = 1$
\end{definition}

\begin{prop}
    Let $m$ be a positive integer. The commutative ring $\mathbb{Z}/m\mathbb{Z}$ is a field i.f.f. $m$ is prime.
\end{prop}

% 3.2
\subsection{Peoperties of Rings}
\begin{lemma}[Mutiplying by zero and negatives]
    Let $R$ be a ring and let $a,b \in R$. Then:
    \begin{enumerate}
        \item $0a = 0 = a0$
        \item $(-a)b = -(ab) = a(-b)$
        \item $(-a)(-b) = ab$
    \end{enumerate}
\end{lemma}

\begin{definition}
    Let $m \in \mathbb{Z}$. The \textbf{$m$-th multiple $ma$ of an element} $a$ in an abelian group $R$ is:
    $ma = a + a +... + a$ if $m > 0$ (sum of $m$ $a$'s). Otherwise $0a = 0$ and $(-m)a = -(ma)$
\end{definition}

\begin{lemma}[Rules of multiples]
    Let $R$ be a ring, let $a,b\in R$ and $m.n \in \mathbb{Z}$. Then:
    \begin{enumerate}
        \item $m(a+b) = ma + mb$
        \item $(m+n)a = ma + na$
        \item $m(na) = (mn)a$
        \item $m(ab) = (ma)b = a(mb)$
        \item $(ma)(nb) = (mn)(ab)$
    \end{enumerate}
\end{lemma}

\begin{definition}
    Let $R$ be a ring. An element $a \in R$ is called a \textbf{unit} if it is \textbf{invertable} in $R$ or in other words \textbf{has a multiplicative inverse in} $R$, meaning that $\exists a^{-1} \in R$ such that $$aa^{-1} = 1 = a^{-1}a$$
\end{definition}

\begin{prop}
    The set $R^{\times}$ of units in a ring $R$ forms a group under multiplication.
\end{prop}

\begin{definition}[Zero-divisor]
    In a ring $R$ a non-zero element $a$ is called a \textbf{zero-divisor} or \textbf{divisor of zero} if there exists a non-zero element $b$ such that either $ab = 0$ or $ba = 0$
    $${a \neq 0 \in R, \exists b \neq 0 \in R \suchthat ab = 0 \cup ba = 0} \Rightarrow {a\text{ is a zero divisor}}$$
\end{definition}

\begin{definition}[Integral domain]
    An \textbf{integral domain} is a non-zero commutative ring that has no zero-divisors, therefore if $D$ is an integral domain then:
    \begin{enumerate}
        \item $ab = 0 \Rightarrow  a = 0$ or $b = 0$, and
        \item $a,b\neq 0 \Rightarrow ab \neq 0$
    \end{enumerate}
\end{definition}

\begin{prop}[Cancellation Law for Integral Domains]
    Let $R$ be an integral domain and let $a,b,c \in R$. If $ab = ac \wedge a \neq 0 \Rightarrow b = c$
\end{prop}

\begin{prop}
    Let $m$ be a natural number. The $\mathbb{Z}/m\mathbb{Z}$ is an integral domain i.f.f. $m$ is prime
\end{prop}

\begin{theorem}
    Every \textbf{fnite} integral domain is a field
\end{theorem}


% 3.3
\subsection{Polynomials}
\begin{definition}
    Let $R$ be a ring. A \textbf{polynomial} over $R$ is an expression of the form
    $$P = a_0 + a_1X + a_2X^2 + ... + a_mX^m$$
    for some $m \in \mathbb{N}\backslash 0$ and elements $a_i \in R$ for $0 \leq i \leq m$. The set of all Polynomials over $R$ is denoted by $R[X]$. Incase $a_m$ is not zero, the polynomial $P$ has a degree of $m$, written $\text{deg}(P) = m$, where $a_m$ is the leading coefficient.\\
    When the leading coefficient is 1 the polynomial is a \textbf{monic} polynomial, linear for $a_1$, quardratic for $a_2$, then cubic for $a_3$.
\end{definition}

\begin{definition}
    Whith the definition in the set $R[X]$ becomes a ring called the \textbf{ring of polynomials with coefficients in $R$, or over $R$}. The zero and the indentity of $R[X]$ are the zero and identity of $R$ resp.
\end{definition}

\begin{lemma}
    \quad
    \begin{enumerate}
        \item If a ring $R$ with no zero-divisors, then $R[X]$ has no zero-divisors and $\deg(PQ) = \deg(P) + \deg(Q)$ for all non-zero $P,Q\in R[X]$
        \item If $R$ is an integral domain then so is $R[X]$, so if $R^\times$ is an integral domain then so is $R[X]^\times$
    \end{enumerate}
\end{lemma}

\begin{theorem}[Division and Remainder]
    Let $R$ be an integral domain and let $P,Q\in R[X]$ with $Q$ monic. Then $\exists$ unique $A,B\in R[X]$ such that $P = AQ + B$ and $\deg(B) < \deg(Q)$ or $B = 0$
\end{theorem}

\begin{definition}
    Let $R$ be a commutative ring and $P\in R[X]$ a polynomial. Then the polynomial $P$ can be evaluated at the element $\lambda \in R$ to produce $P(\lambda)$ by replacing the powers of $X$ in the polynomial $P$ by the corresponding powers of $X$ in the polynomial $P$ by the corresponding powers of $\lambda$. In this way we have a mapping $R[X]\rightarrow\Maps(R,R)$\\This is the percise mathematical descrption of thinking of a polynomial as a function. An element $\lambda \in R$ is a root of $P$ is $P(\lambda) = 0$
\end{definition}

\begin{prop}
    Let $R$ be a commutative ring, let $\lambda \in R$ and $P(X) \in R[X]$. Then $\lambda$ is a root of $P(X)$ i.f.f. $(X - \lambda)$ divides P(X) 
\end{prop}

\begin{theorem}
    Let $R$ be a field, or more generally an integral domain. Then a non-zero polynomial $P \in R[X]\backslash\{0\}$ has at most $\deg(P)$ roots in $R$
\end{theorem}

\begin{definition}
    A field $F$ is algebraically closed of each non-constant polynomial $P \in F[X]\backslash F$ with coefficients in our field has a root in our field $F$
\end{definition}

\begin{theorem}[Fundamental Theorem of Algebra]
    The field of complex numbers $\mathbb{C}$, is algebraically closed.
\end{theorem}

\begin{theorem}
    If $F$ is an algebraically closed field, then every non-zero polynomial $P\in F[X]\backslash\{0\}$ \textbf{decomposes into linear factors}
    $$P = c\prod_{i=1}^n{(X-\lambda_i)}$$
    with $n \geq 0, c\in F^\times$ and $\lambda_1,...,\lambda_n\in F$
\end{theorem}

% 3.4
\subsection{Homomorpgism, Ideals and Subrings}
\begin{definition}
    Let $R,S$ be rings. A mapping $f:R\rightarrow S$ is a \textbf{ring homomorphism} if the following hold for all $x,y \in R$:
    \[\begin{split}
        f(x+y) &= f(x)f(y)\\
        f(xy) &= f(x)f(y)
    \end{split}\]
\end{definition}

\begin{lemma}
    Let $R,S$ be rings and $f:R\rightarrow S$ a ring homomorphism. Then for all $x,y\in R, m\in \mathbb{Z}$:
    \begin{enumerate}
        \item $f(0_r) = 0_s$ Where $O_R,0_S$ are the zeros of the resptive ring
        \item $f(-x) = -f(x)$
        \item $f(x-y) = f(x) - f(y)$
        \item $f(mx) = mf(x)$
    \end{enumerate}
\end{lemma}

\begin{definition}
    A subset $I$ of a ring $R$ is an \textbf{ideal},written $I \trianglelefteq R$, if the following hold:
    \begin{enumerate}
        \item $I \neq 0$
        \item $I$ is closed under subtraction
        \item $\forall i \in I, r\in R: ir,ri\in I$
    \end{enumerate} 
\end{definition}

\begin{definition}
    Let $R$ be a commutative ring and let $T \subset R$. Then the \textbf{ideal of $R$ generated by $T$} is the set
    $$_R\langle T\rangle = \left\{\sum_{i=1}^m{r_it_i}: t_1,...,t_m\in T, r_1,...,r_m \in R\right\}$$
\end{definition}

\begin{example}
    \quad
    \begin{itemize}
        \item Let $m \in \mathbb{Z}$. Then $_\mathbb{Z}\langle m \rangle = m\mathbb{Z}$
        \item Let $P \in \mathbb{R}[X]. Then _{\mathbb{R}[X]}\langle P \rangle = \{AP: A \in \mathbb{R}[X]\} = \{Q:P \text{ divides } Q \text{ in } \mathbb{R}[X]\}$
    \end{itemize}
\end{example}

\begin{prop}
    Let $R$ be a commutative ring and let $T \subseteq R$. Then $_R\langle T \rangle$ is the smallest ideal of $R$ that contains $T$
\end{prop}

\begin{definition}
    Let $R$ be a commutative ring. An ideal $I$ of $R$ is called a \textbf{principal ideal} if $I = \langle t \rangle$ for some $t \in R$
\end{definition}

\begin{definition}
    Let $R,S$ be rings with zero elements $0_R,0_S$ resp and let $f:R\rightarrow S$ be a ring homomorphism. Since $f$ is in particular a group homomorphism from $(R,+)$ to $(S,+)$, ther kernel of $f$ already has a meaning: $\ker(f) = \{r \in R: f(r) = 0_S\}$.
\end{definition}

\begin{prop}
    Let $R$ and $S$ be rings and $f:R \rightarrow S$ a ring homomorphism. Then $\ker(f)$ is an ideal of $R$.
\end{prop}

\begin{lemma}
    $f$ is injective i.f.f. $\ker(f) = \{0\}$
\end{lemma}

\begin{lemma}
    The intersection of any collection of ideals of a ring $R$ is an ideal of $R$
\end{lemma}

\begin{lemma}
    Let $I,J$ be ideals of ring $R$. Then $I+J = \{a+b:a\in I, b\in J\}$ is an ideal of $R$
\end{lemma}

\begin{definition}[subring]
    Let $R$ be a ring. A subset $R'$ of $R$ is a \textbf{subring} of $R$ if $R'$ itself is a ring under the operations of addition and multiplication defined in $R$
\end{definition}

\begin{example}
    $\forall$ ring $R$, $\{0\}, R$ are always the subrings of $R$
\end{example}

\begin{prop}[Test of a subring]
    Let $R'$ be a subset of a ring $R$. Then $R'$ is a subring i.f.f.
    \begin{enumerate}
        \item $R'$ has a multiplicative identity, and
        \item $R'$ is closed under subtraction: $a,b\in R' \rightarrow a-b\in R'$, and
        \item $R'$ is closed under multiplication
    \end{enumerate}
\end{prop}

\begin{prop}
    Let $R$ and $S$ be subrings and $f:R\rightarrow S$ a ring homomorphism
    \begin{enumerate}
        \item If $R'$ is a subring of $R$ then $f(R')$ is a subring of $S$. In particular, $\text{im}(f)$ is a subring of $S$
        \item Assume that $f(1_R) = 1_S$. Then if $x$ is a unit in  $R$, $f(x)$ is a unit in $S$ and $(f(x))^{-1} = f(x^{-1})$. In this case $f$ restricts to a group homomorphism $f|_{R^\times}:R^\times \rightarrow S^\times$
    \end{enumerate}
\end{prop}

% Chap 3.5
\subsection{Equivalece Relations}
\begin{definition}
    A \textbf{relation} $R$ on a set $X$ is a subset $R \subseteq X \times X$. In this context, and only in this context, we write $xRy$ instead of $(x,y) \in R$. $R$ is an \textbf{equivalence relation} on $X$ when for all $x,y,z \in X$ the following holds:
    \begin{enumerate}
        \item \textbf{Reflexivity}: $xRx$
        \item \textbf{Symmetry}: $xRy \Leftrightarrow yRx$
        \item \textbf{Transitivity}: $(xRy \cap yRx) \rightarrow xRz$
    \end{enumerate}
\end{definition}

\begin{definition}
    Suppose that $\sim$ is an equivalence relation on a set $X$. For $x \in X$ the set $E(x) := \{z \in X: z \sim x\}$ is called the \textbf{equivalence class of $x$}. A subset $E \subseteq X$ is called an \textbf{equivalence class} for our equivalence relation if there is an $x \in X$ for which $E = E(x)$. An element of an equivalence relation is called a \textbf{representative} of the class. A subset $Z \subseteq X$ contains percisely one element from each equivalenceclass is called a \textbf{system of representatives} for the equivalence relation.
\end{definition}

\begin{definition}
    Given an equivalence relation $\sim$ on the set $X$ we will denote the set of equivalence classes, which is a subset of the power set $\mathcal{P}(X)$, by $$(X/\sim) := \{E(x):x\in X\}$$
\end{definition}

\begin{example}
    Let $\equiv$ be the equivalence relation of "$a \equiv b (\text{mod}m)$". Then $(\mathbb{Z}/\equiv) = \mathbb{Z}/m\mathbb{Z}$
\end{example}

\begin{definition}
    $g:(X/\sim) \rightarrow Z$ is \textbf{well-defined} if $\exists$ a mapping $f:X \rightarrow Z$ such that $f$ has the peoperty $x \sim y \rightarrow f(x) = f(y) \text{ and } g = \bar{f}$ 
\end{definition}


% Chap 3.6
\subsection{Factor Rings and the First Isomorphism Theorem}
\begin{definition}[Coset]
    Let $R$ be a ring, $I \trianglelefteq R$ be an ideal in ring $R$. The set $$x + I = \{x+i:i\in I\}\subseteq R$$ is a \textbf{coset of $I$ in $R$} or the \textbf{coset of $x$ with respect to $I$ in $R$}
\end{definition}

\begin{definition}
    Let $R$ be a ring, $I \trianglelefteq R$ an ideal, and $\sim$ the equivalence relation defined by $x \sim y \Leftrightarrow x - y \in I$. Then $R/I$, \textbf{the factor ring of $R$ by $I$} or the \textbf{quotient of $R$ by $I$}, is the set $(R/ \sim)$ of cosets of $I$ in $R$.
\end{definition}

\begin{theorem}
    Let $R$ be a ring and $I \trianglelefteq R$ and ideal. Then $R/I$ is a ring, where the operation of additionis defined by
    $$(x+I)+(y+I) = (x+y) + I \quad \forall x, y \in R$$
    and multiplication is defined by
    $$(x + I)\dot(y+I) = xy + I \quad \forall x,y \in R$$
\end{theorem}

\begin{theorem}[The Universal Property of Factor Rings]
    Let $R$ be a ring and $I$ an ideal of $R$:
    \begin{itemize}
        \item The mapping $\mathsf{can}: R \rightarrow R/I$ sending $r$ to $r+I$ for all $r \in R$ is a surjective ring homomorphism with kernel $I$
        \item If $f:R \rightarrow S$ is a ring homomorphism with $f(I) = \{0_S\}$, so that $I \subseteq \ker(f)$, then there is a unique ring homomorphism $\bar{f}:R/I \rightarrow S$ such that $f = \bar{f} \circ \mathsf{can}$
    \end{itemize}
\end{theorem}

\begin{theorem}[First Isomorphism Theorem for Rings]
    Let $R$ and $S$ be rings. Then every ring homomorphism $f:R\rightarrow S$ induces a ring isomorphism
    $$\bar{f}:R/\ker f \stackrel{\sim}{\rightarrow} \text{im} f$$
\end{theorem}


% 3.7
\subsection{Modules and All That}
\begin{definition}
    A \textbf{(left) module $M$ over a ring R} is a pair consisting if an abelian group $M = (M, +)$ and a mapping
    \[\begin{split}
        R\times M &\rightarrow M\\
        (r,a) &\mapsto ra
    \end{split}\]
    such that for all $r,s \in R$ and $a,b \in M$ the following indentities hold:
    \[\begin{split}
        r(a+b) &= ra + rb\\
        (r+s)a &= ra + sa\\
        r(sa) &= (rs)a\\
        1_R a &= a
    \end{split}\]
\end{definition}

\begin{lemma}
    Let $R$ be a ring and $M$ an $R$-module:
    \begin{enumerate}
        \item $0_Ra = 0_M \quad \forall a \in M$
        \item $r0_M = 0_M \quad \forall r \in R$
        \item $(-r)a = r(-a) = -(ra) \quad \forall r\in R, a \in M$
    \end{enumerate}
\end{lemma}

\begin{definition}
    Let $R$ be a ring and let $M,N$ be $R$-modules. A mapping $f:M \rightarrow N$ is an $R$-\textbf{homomorphism} or \textbf{homomorphism} if the following hold for all $a,b \in M$ and $r \in R$
    \[\begin{split}
        f(a+b) &= f(a+b)\\
        f(ra) &= rf(a)
    \end{split}\]
    The kernel of $f$ is $\ker f  =\{a\in M:f(a)=0_N\} \subseteq M$ and the image of $f$ is $\image f = \{f(a):a\in M\}\subseteq N$. If $f$ is a bijection then it is an $R$-\textbf{module isomorphism} or \textbf{isomorphism}($M \simeq N$)
\end{definition}

\begin{definition}
    A non-empty subset $M'$ of an $R$-module $M$ is a submodule if $M'$ is an $R$-module with respect to the operations of the $R$-module $M$ restricted to $M'$
\end{definition}

\begin{prop}[Test for a submodule]
    Let $R$ be a ring and $M$ an $R$-module. A subset $M'$ of $M$ is a submodule i.f.f.
    \begin{enumerate}
        \item $0_M \in M'$
        \item $a,b \in M' \Rightarrow a-b \in M'$
        \item $r \in R, a i\in M' \Rightarrow ra \in M'$
    \end{enumerate}
\end{prop}

\begin{lemma}
    Let $f: M \rightarrow N$ be an $R$-homomorphism. The $\ker f$ is a submodule of $M$ and $\image f$ is a submodule of $N$.
\end{lemma}

\begin{lemma}
    Let $R$ be a ring, $M,N$ be $R$-modules and let $f: M \rightarrow N$ be an $R$-homomorphism. Then $f$ is injective i.f.f. $\ker f = \{0_M\}$
\end{lemma}

\begin{definition}
    Let $R$ be a ring, $M$ and $R$-module and let $T \subseteq M$. The the submodule of $M$ generated by $T$ is the set
    $$_R\langle T \rangle = \{r_1t_1 + ... + r_mt_m:t_1,...,t_m \in T, r_1,..,r_m \in R\}$$
    together with the zero element in the case $T = \emptyset$. If $T = {t_1, ..., t_n}$ a finite set, we can write $_R\langle T \rangle$ as $_R\langle t_1,...,t_n \rangle$. The module $M$ is \textbf{finitely generated} if it is generated bt a finite set: $M = _R\langle t_1,...,t_n \rangle$. It is called \textbf{cyclic} if it is generated by a singleton: $M = _R\langle t \rangle$
\end{definition}

\begin{example}
    \quad
    \begin{itemize}
        \item A cyclic group = a cyclic $\mathbb{Z}$-module
        \item Let $R$ be a commutative ring. The the ideal generated by $T \subseteq R$ = submodule of $R$ generated by $T$.
        \item A principal ideal of $R$ = cyclic submodule of $R$
        \item $\{0_M\}$ is always a cyclic submodule of an $R$-module $M$, generated by the element $0_M$: $_R\langle 0_M \rangle = \{0_M\}$
    \end{itemize}
\end{example}

\begin{lemma}
    Let $T\subseteq M$. The $_R\langle T\rangle$ is the smallest submodule of $M$ that contains $T$
\end{lemma}

\begin{lemma}
    The intersection of any collection of submodules of $M$ is a submodule of $M$.
\end{lemma}

\begin{lemma}
    Let $M_1, M_2$ be submodules of $M$, Then $M_1 + M_2 = \{a+b:a\in M_1, b\in M_2\}$ is a submodule of $M$
\end{lemma}

\begin{definition}
    Let $R$ be a ring, $M$ an $R$-module and $N$ a submodule of $M$. For each $a \in M$ the coset of $a$ with respect to $N$ in $M$ is
    $$a+N = \{a+b:b\in N\}$$
\end{definition}

\begin{theorem}[The Universal Property of Factor Modules]
    Let $R$ be a ring, let $L$ and $M$ be $R$-modules, and $N$ a submodule of $M$.
    \begin{enumerate}
        \item The mapping $g: M \rightarrow M/N$ sending $a$ to $a+N$ for all $a \in M$ is serjective $R$-homomorphism with kernel $N$: $\ker{g} = N$
        \item If $f:M \rightarrow L$ is an $R$-homomorphism with $f(N) = \{0_L\}$, so that $N \subseteq \ker f$, then there is a unique homomorphism $\bar{f}:M/N \rightarrow L$ such that $f = \bar{f}\circ \mathsf{can}$
    \end{enumerate}
\end{theorem}

\begin{theorem}[First Isomorphism Theorem for Modules]
    Let $R$ be a ring and let $M,N$ be $R$-modules. Then every $R$-homomorphism $f:M/\ker f \stackrel{\sim}{\rightarrow} \image f$
\end{theorem}


% Ch 4
\section{Determinants and Eigenvalues Redux}
% 4.1
\subsection{The sign of a permutation}
\begin{definition}
    The group of all permutations of the set $\{1,2,...,n\}$, also known as bijections from $\{1,2,...,n\}$ to itself, is denoted by $\mathfrak{S_n}$ and called the $n$\textbf{-th symmetric group}. It is a group under composition and it has $n!$ elements.\\
    A transposition is a permutation that swaps two elements of the set and leaves all the others unchanged.
\end{definition}

\begin{definition}
    An inversion of a permutation $\sigma \in \mathfrak{S}_n$ is  apair $(i,j)$ such that $1 \leq i < j \leq n$ and $\sigma(i) > \sigma(j)$. The number of inversions of the permutation $\sigma$ is called the length of $\sigma$ ans written $l(\sigma)$. In formulas:
    $$l(\sigma) = |\{(i,j): i < j \text{ but } \sigma(i) > \sigma(j)\}|$$
    The sign of $\sigma$ is defined to be the parity of the permutations of $\sigma$. In formulas:
    $$\text{sgn}(\sigma) = (-1)^{l(\sigma)}$$
\end{definition}

\begin{example}
    consider $(1\,2\,4\,5\,3) = (1\,2\,4\,5\,3)(6) \in \mathfrak{S}_6$, the inversions are $(1,3),(2,3),(2,5),(4,5)$. therefore $l((1\,2\,4\,5\,3)) = 4$
\end{example}

\begin{lemma}[Multiplicity of the sign]
    For each $n \in \mathbb{N}$ the sign of a permutation produces a group homomorphism $\textbf{sgn}: \mathfrak{S}_n \rightarrow \{+1,-1\}$ from the symmetric group ton the two-element group of signs. In formulas:
    $$\sgn(\sigma\tau) = \sgn(\sigma)\sgn(\tau) \quad \forall \sigma,\tau\in \mathfrak{S}_n$$
\end{lemma}

\begin{definition}
    For $n \in \mathbb{N}$, the set of even permutations in $\mathfrak{S}_n$ forms a subgroup of $\mathfrak{S}_n$ because it is the kernel of the group homomorphism $\sgn:\mathfrak{S}_n \rightarrow \{+1,-1\}$. This group is the \textbf{alternating group} and is denoted $A_n$
\end{definition}

% 4.2
\subsection{Determinants and What They Mean}
\begin{definition}
    Let $R$ be a commutative ring and $n \in \mathbb{N}$. The determinant is a mapping $\det : \Mat(n;R) \rightarrow R$ from square matrices with coefficients in $R$ to the ring $R$ from square matrices with coefficient in $R$ ot the ring $R$ that is given by the following formula (\textbf{Leibniz formula}):
    $$A = 
    \begin{pmatrix}
        a_{11} & ... & a_{1n}\\
        \vdots & \ddots & \vdots\\
        a_{n1} & ... & a_{nn}
    \end{pmatrix} \mapsto \det(A) = \sum_{\sigma \in \mathfrak{S}_n} \left(\sgn(\sigma)\prod_{i=1}^n a_{i\sigma (i)}\right)$$
    There is a really good demo in Example 4.2.3 in the Iain's note
\end{definition}

\begin{example}
    \quad
    \begin{itemize}
        \item An $(n \times n)$ matrix $A = (a_{ij})$ is upper triangular if $a_{ij} = 0 \quad \forall i > j$. $\det(A) = \prod_{i=1}^n a_{ii}$
        \item The simmilar holds for the lower triangle
    \end{itemize}
\end{example}


% 4.3
\subsection{Characterising the Determinant}
\begin{definition}
    Let $U,V,W$ be $F$-vector spaces. A \textbf{bilinear form on $U \times V$ with values in $W$} is a mapping $H:U \times V \rightarrow W$ which is a linear mapping in both of its entries. This means that it must satisfy the following properties for all $u_1, u_2 \in U$ and $v_1, v_2 \in V $ and $\lambda \in F$:
    \[\begin{split}
        H(u_1 + u_2, v_1) &= H(u_1, v_1) + H(u_2, v_1)\\
        H(u_1, v_1 + v_2) &= H(u_1, v_1) + H(u_1, v_2)\\
        H(\lambda u_1, v_1) &= \lambda H(u_1, v_1)\\
        H(u_1,\lambda v_1) &= \lambda H(u_1, v_1)
    \end{split}\]
    A bilinear form is \textbf{symmetric} if $U = V$ and $H(u,v) = H(v,u) \quad \forall u,v \in U =V$\\
    A bilinear form is \textbf{altering} or \textbf{antisymmetric} if $U = V$ and $H(u,u) = 0 \quad \forall u \in U =V$
\end{definition}

\begin{definition}
    Let $V_1, ..., V_n, W$ be $F$-vector spaces. A mapping $H:V_1\times v_2\times ... \times V_n \rightarrow W$ is a \textbf{multilinear form} aka. \textbf{multilinear} if for each $j$ the mapping $V_j$ defined by $v_j \mapsto H(v_1,...,v_j,...,v_n)$ with the $v_i\in V_i$ arbitrary fixed vectors of $V_i$ for $i \neq j$, is linear. In the case $n = 2$. this is exactly the definition of a linear mapping.
\end{definition}

\begin{definition}
    Let $V$ and $W$ be $F$-vector spaces. A multilinear form $H:V\times ... \times V \rightarrow W$ is \textbf{alternating} if it vanishes on every $n$-tuple of elements of $V$ that has at least two entries equal, in order words:
    $$(\exists i \neq j \text{ with } v_i = v_j \rightarrow H(v_1,...,v_j,...,v_i,...,v_n) = 0$$
    In the case $n = 2$, this is exactly the definition of an alternating or antisymmetric bilinear mapping.
\end{definition}

\begin{theorem}[Characterisation of the Determinant]
    Let $F$ be a field, The mapping $\mathsf{det}: \Mat(n;F) \rightarrow F$ is the unique alternating multilinear form on $n$-tuples of column vectors with values in $F$ that takes the value $1_F$ on the identity matrix.
\end{theorem}

% 4.4
\subsection{Rules for Caluculating with Determinants}
\begin{theorem}[Multiplicy for Calculating with Determinants]
    Let $R$ be a commutative right and $A,B \in \Mat(n,R)$. Then $\det(AB) = \det(A)\det(B)$
\end{theorem}

\begin{theorem}[Determinantal Criterion for invertibility]
    The determinant of a square matrix with entries in a field $F$ is a non-zero i.f.f. the matrix is invertable.
\end{theorem}

\begin{definition}
    Let $A\in \Mat(n;R)$ fir some commutative ring R and natural number $n$. Let $i$ and $j$ be integer between $1$ and $n$. Then the $(i,j)$ \textbf{cofactor of} $A$ is $C_{ij} = (-1)^{i+j}\det(A\langle i,j\rangle)$ where $A\langle i,j\rangle$ is the matrix $I$ obtain from $A$ deleting the $i$-th row and the $j$-th column.
\end{definition}

\begin{theorem}[Laplace's Expansion of the Determinant]
    Let $A = (a_{ij})$ be an $n\times n$-matrix with entries form a commutative ring $R$. For a fixed $i$ the \textbf{$i$-th row expansion of the determinant is}
    $$\det(A) = \sum_{j=1}^n a_{ij}C_{ij} = \sum_{j=1}^{n} a_{ij}(-1)^{i+j}\det(A\langle i,j\rangle)$$
    and for a fixed $j$ in the \textbf{$j$-th column expansion of the determinant} is
    $$\det(A) = \sum_{i=1}^n a_{ij}C_{ij} = \sum_{j=1}^{n} a_{ij}(-1)^{i+j}\det(A\langle i,j\rangle)$$
\end{theorem}

\begin{definition}[Adjugate Matrix]
    Let $A\in \Mat(n;R)$ where $R$ is a commutative ring. The \textbf{adjugate matrix} $\adj(A)$ is the $(n \times n)$-matrix whose entries are $adj(A)_{ij} = C_{ij}$ where $C_{ij}$ is the $(i,j)$-cofactor.
\end{definition}

\begin{theorem}[Cramer's Rule]
    Let $A \in \Mat(n;R)$ where $R$ is a commutative ring. Then
    $$A\cdot \adj(A) = (\det A)I_n$$
\end{theorem}

\begin{corollary}[invertibility of Matrices]
    A square matrix with entries in a commutative ring $R$ is invertible i.f.f. its determinant is a unit in R. That is, $A \in Mat(n; R)$ is invertible i.f.f. $det(A) \in R^\times$.
\end{corollary}

% 4.5
\subsection{Eigenvalues and Eigenvectors}
\begin{definition}
    Let $f:V \rightarrow V$ an endomorphism of an $F$-vector space $V$. A scalar $\lambda \in F$ is an eigenvalue of $f$ i.f.f. $\exists \vec{v} \in V \quad \vec{v} \neq \vec{0}$ sych that $f(\vec{v}) = \lambda\vec{v}$. Each such vector is called an \textbf{eigenvector of $f$ with eigenvalue $\lambda$}. For any $\lambda \in F$, the \textbf{eigenspace of $f$ with eigenvalue $\lambda$ is}    $$E(\lambda,f) = \{\vec{v}\in V:f(\vec{v}) = \lambda\vec{v}\}$$
\end{definition}

\begin{theorem}[Existance of Eigenvalues]
    Each endomorphism of a non-zero finite dimensional vector space over an algebraically closed field has an eigenvalue.
\end{theorem}

\begin{definition}
    Let $R$ be a commutative ring and let $A \in \Mat(n;R)$ be a square matrix with entries in $R$. The polynomial $\det(A-xI_n) \in R[x]$ is called the \textbf{characteristic polynomial of the matrix $A$}. It is denoted by $\chi_A(x):= \det(A-xI_n)$.\\\colorbox{yellow}{Where $\chi$ stands for $\chi$aracteristic\tiny{(Okay Iain you're a good dad for sure...)}.}
\end{definition}

\begin{theorem}[Eigenvalues and Characteristic Polynomials]
    Let $F$ be a field and $A \in \Mat(n;F)$ with entries in $F$. The eigenvalues of the linear mapping $A:F^n \rightarrow F^n$ are exactly the roots of the characteristic polynomial $\chi_A$.
\end{theorem}


% 4.6
% \newpage
\subsection{Triangularisable, Diagonalisable, and the Cayley-Hamilton Theorem}
Iain (not Ian just to be clear) said he's not examing triangularisable. (Will FC)

\begin{prop}[Triangularisability]
    Let $f:V\rightarrow V$ be an endomorphism of a finite dimensional $F$-vector space $V$. The following two statements are equivalent:
    \begin{enumerate}
        \item The vector space $V$ has an ordered basis $\mathcal{B} = (\vec{v_1},\vec{v_2},...,\vec{v_n})$ such that
                \[\begin{split}
                    f(\vec{v_1}) &= a_{11}\vec{v_1}\\
                    f(\vec{v_2}) &= a_{12}\vec{v_1} + a_{22}\vec{v_2}\\
                    &\vdots\\
                    f(\vec{v_n}) &= \sum_{i=1}^n a_{in}\vec{v_i} \in V
                \end{split}\]
                such that the $n \times n$ matrix $_\mathfrak{B}[f]_\mathfrak{B} = (a_{ij})$ representing $f$ with respect to $\mathfrak{B}$ is upper triangular
                $$A = \begin{pmatrix}
                    a_{11} & a_{12} & \hdots & a_{1n}\\
                    0 & a_{22} & \hdots & a_{2_n}\\
                    \vdots & \vdots & \ddots & \vdots\\
                    0 & 0 & \hdots & a_{nn}
                \end{pmatrix}$$
                When this happens, we say that $f$ is \textbf{triangularisable}.
        \item The characteristic polynomial $\chi_f(x)$ of $f$ decomposes into linear factors in $F[x]$.
    \end{enumerate}
\end{prop}

\begin{definition}[Diagonalisibility]
    Aa endomorphism $f:V \rightarrow V$ of an $F$-vector space $V$ is \textbf{diagonalisable} i.f.f. $\exists$ basis of $V$ consisting of eigenvectors of $f$. If $V$ is finite dimensional then this is the same as saying that $\exists$ an order basis $\mathcal{B} = \{\vec{v_1}, ..., \vec{v_n}\}$ such that corresponding matrix representing $f$ is diagonal, that is $_\mathcal{B}[f]_\mathcal{B} = \text{diag}(\lambda_1,...,\lambda_b)$. In this case, $f(\vec{v_1}) = \lambda_1\vec{v_1}$.\\
    A square matrix is \textbf{diagonalisable} i.f.f. the corresponding linear mapping $F^n \rightarrow F^n$ given by left multiplication by $A$ is diagonalisable. This means $\exists$ invertable matrix $P\in \text{GL}(n;F)$ such that $P^{-1}AP = \text{diag}(\lambda_1, ...,\lambda_n)$
\end{definition}

\begin{lemma}[Linear Independence of Eigenvectors]
    Let $f:V \rightarrow V$ be an endomorphism of a vector space $V$ and let $\vec{v_1}, ...,\vec{v_n}$ be eigenvectors of $f$ with pairwise different eigenvalues $\lambda_1, ..., \lambda_n$. Then the vectors are linearly independent.
\end{lemma}

\begin{theorem}[Cayley-Hamilton Theorem]
    Let $A \in \Mat(n;R)$ be a square matrix with entries in a commutative ring $R$. The evaluating its characteristic polynomial $\chi_{A}(x)\in R[x]$ at the matrix $A$ gives zero.
\end{theorem}


% 4.7
\subsection{Googles PageRank Algorithm (Markov matrix/stochastic matrix)}
\begin{definition}
    A matrix $M$ whose entries are non-zero and such that the sum of the entries of each column equal 1 is a \textbf{Markov matrix} or a \textbf{stochastic matrix}.
\end{definition}

\begin{lemma}
    Suppose $M\in \Mat(n;\mathbb{R})$ is a Markov matrix. Then 1 is an eigenvalue of $M$
\end{lemma}

\begin{theorem}[Perron, 1907]
    If $M \in \Mat(n;\mathbb{R})$ is a Markov matrix all whose entries are positive, then eigenspace $E(1,M)$ is one dimensional. There exists a unique basis vector $\vec{v} \in E(1,M)$ all of whose entries are positive real numbers, $v_i > 0$ for all $i$, and such that the sum of it entries is 1, $\sum_i v_i = 1$
\end{theorem}


%  Chapter 5
% \newpage
\section{Inner Product Spaces}
%  5.1
\subsection{Inner Product Space: Definitions}
\begin{definition}
    Let $V$ be a vector space over $\mathbb{R}$. An \textbf{inner product} on $V$ is a mapping $(-,-): V \times V \rightarrow \mathbb{R}$ that satisfies the following $\forall \vec{x},\vec{y},\vec{z}\in V$ and $\lambda,\mu\in \mathbb{R}$:
    \begin{enumerate}
        \item $(\lambda\vec{x} + \mu\vec{y}, z) = \lambda(\vec{x},\vec{z}) + \mu(\vec{y},\vec{z})$
        \item $(\vec{x}, \vec{y}) = (\vec{y},\vec{x})$
        \item $(\vec{x},\vec{x}) \geq 0$, with $\vec{x} = \vec{0} \Leftrightarrow (\vec{x},\vec{x}) = 0$
    \end{enumerate} 
    A \textbf{real innter space product space} is a real vector space endowed with an inner product.
\end{definition}

\begin{example}
    \quad
    \begin{itemize}
        \item A dot product is a statdard inner product
    \end{itemize}
\end{example}

\begin{definition}
    Let $V$ be a vector space over $\mathbb{C}$. An \textbf{inner product} on $V$ is a mapping $(-,-): V \times V \rightarrow \mathbb{C}$ that satisfies the following $\forall \vec{x},\vec{y},\vec{z}\in V$ amdd $\lambda,\mu\in \mathbb{C}$:
    \begin{enumerate}
        \item $(\lambda\vec{x} + \mu\vec{y}, z) = \lambda(\vec{x},\vec{z}) + \mu(\vec{y},\vec{z})$
        \item $(\vec{x}, \vec{y}) = \overline{(\vec{y},\vec{x})}$
        \item $(\vec{x},\vec{x}) \geq 0$
    \end{enumerate} 
    Where $\overline{z}$ denots the complex conjugate of $z$ A \textbf{complex innter space product space} is a complex vector space endowed with an inner product.
\end{definition}

\begin{example}
    \quad
    \begin{itemize}
        \item \textbf{Standard inner product}: $(\vec{v},\vec{w}) = \sum_{i=1}^n(v_i \overline{w_i})$
    \end{itemize}
\end{example}

\begin{definition}
    In a real or complex innter product space the \textbf{length} or \textbf{inner product norm} or \textbf{norm} $\norm{\vec{v}} = \sqrt{(\vec{v},\vec{v})}$.\\
    Vectors whose length is 1 are called \textbf{units}. Two vectors $\vec{v},\vec{w}$ are \textbf{orthogonal} can be denoted as $\vec{v} \perp \vec{w}$ i.f.f. $\vec{v}, \vec{w} = 0$
\end{definition}

\begin{definition}
    A family $\left(\vec{v_i}\right)_{i\in I}$ for vectors from an inner product space is an \textbf{orthogonal family} if all the vectors $\vec{v_i}$ have length 1 and if they are pairwise orthogonal to each other, which, using the \textit{Kronecker delta symbol} defined in Example 2.1.2(Check full note), means $(\vec{v_i},\vec{v_j}) = \delta_{ij}$\\
    An orthogonal family that is a basis is an \textbf{orthogonal basis}
\end{definition}

\begin{theorem}
    Every finite dimensional inner product space has an orthonormal basis
\end{theorem}

% 5.2
\subsection{Orthogonal Complements and Orthogonal Projections}
\begin{definition}
    Let $V$ be an inner product space and let $T \subseteq V$ be an arbitrary subset. Define $T^\perp = \{\vec{v} \in V: \vec{v} \perp \vec{t} \quad\forall \vec{t} \in T\}$ calling this set the \textbf{orthogonal} to $T$
\end{definition}

\begin{prop}
    Let $V$ be an inner product space and let $U$ be a finite dimensional subspace of $V$. Then $U$ and $Y^\perp$ are complementary in the sense of definition 1.7.6 (Check full note). In other words $V = U \oplus U^\perp$
\end{prop}

\begin{definition}
    Let $U$ be a finite dimensional subspace of an inner product space $V$. The space $U^\perp$ is the \textbf{orthogonal Complement to} $U$. The \textbf{orthogonal projection frome $V$ to $U$} is the mapping $\pi_U:V \rightarrow V$ that sends $\vec{v} = \vec{p}+ \vec{r}$ to $\vec{p}$
\end{definition}

\begin{prop}
    Let $U$ be a finite dimensional subspace of an inner product space $V$ and let $\pi_U$ be the orthogonal projection form $V$ onto $U$.
    \begin{enumerate}
        \item $\pi_U$ is a linear mapping with $\image(\pi_U) = U$ and $\ker(\pi_U) = U^\perp$
        \item If $\{\vec{v_1},..., \vec{v_n}\}$ is an orthogonal basis of $U$, then $\pi_U$ is given by the following formula for all $\vec{v} \in V$ $$\pi_U(\vec{v}) = \sum_{i=1}^n(\vec{v},\vec{v_i})\vec{v_i}$$
        \item $\pi_U^2 = \pi_U$, that is $\pi_U$ is an \textbf{idempotent}
    \end{enumerate}
\end{prop}

\begin{theorem}[Cauchy-Schwarz Inequality]
    Let $\vec{v}, \vec{w}$ be vectors in an inner product space. Then $|(\vec{v}, \vec{w})| \leq \norm{\vec{v}}\norm{\vec{w}}$ with equality i.f.f. $\vec{v}, \vec{w}$ are linearly dependent.
\end{theorem}

\begin{corollary}
    The norm $\norm{\cdot}$ on an inner product space $V$ satisfies, for any $\vec{v}, \vec{w}\in V$ and scalar $\lambda$:
    \begin{enumerate}
        \item $\norm{\vec{v}} \geq 0$ with equality i.f.f. $\vec{v} = \vec{0}$
        \item $\norm{\lambda\vec{v}} = |\lambda|\norm{\vec{v}}$
        \item $\norm{\vec{v}+ \vec{w}} \leq \norm{\vec{v}} + \norm{\vec{w}}$ (triangle inequality)
    \end{enumerate}
\end{corollary}

\begin{theorem}[Gram-Schmit Process]
    Let $\vec{v_1},..., \vec{v_k}$ be a linearly independent vectors in an inner product space $V$. Then there exists an orthonormal family $\vec{w_1},... \vec{w_k}$ with the property for all $1 \leq i \leq k$: $$\vec{w_i} \in \mathbb{R}_{>0}\vec{v_i}+ \langle\vec{v_i-1,...,\vec{v_1}}\rangle$$
    Therefore, $\forall i \in [1,k]\in \mathbb{N} \exists \vec{\lambda} \text{ s.t. } \exists \vec{w_i} = \vec{v_i} - \sum_{j=1}^{i-1}\lambda_j \vec{v_j}$
\end{theorem}

% 5.3
\subsection{Adjoints and Self-Adjoints}
\begin{definition}
    Let $V$ be an inner product space. Then two endomorphism $T,S:V \rightarrow V$ are called \textbf{adjoint} to the other if the following holds for all $\vec{v}, \vec{w} \in V$: $$(T\vec{v}, \vec{w}) = (\vec{v}, S\vec{w})$$
    In this say we can express $S = T^*$ and call $S$ the adjoint of $T$. If $S = T^*$ then $T=S^*$
\end{definition}

\begin{theorem}
    Let $V$ be a finite dimensional inner product space. Let $T:V \rightarrow V$ be an endomorphism. Then $T*$ exists. That is, $\exists$ a unique linear mapping $T^*: V \rightarrow V$ such that for all $\vec{v}, \vec{w} \in V$
    $$(T\vec{v}, \vec{w}) = (\vec{v}, T^*\vec{w})$$
\end{theorem}

\begin{definition}
    An endomorphism of an inner product space $T:V \rightarrow V$ is \textbf{self adjoint} if $T^* = T$
\end{definition}

\begin{theorem}
    Let $T:V \rightarrow V$ be a self-adjoint linear mapping on an inner product space $V$.
    \begin{enumerate}
        \item Every eigenvalue of $T$ is real.
        \item If $\lambda$ and $\mu$ are distinct eigenvalues of $T$ with corresponding eigenvectors $\vec{v}, \vec{w}$, then $(\vec{v}, \vec{w}) = 0$
        \item $T$ has an eigenvalue
    \end{enumerate}
\end{theorem}

\begin{theorem}[The Spectral Theorem for Self-Adjoint Endomorphisms]
    Let $V$ be a finite dimensional inner product space and let $T:V \rightarrow V$ be a self-adjoint linear mapping. Then $V$ has an orthonormal basis consisting of eigenvectors of $T$
\end{theorem}

\begin{definition}[Orthogonal Matrix]
    An \textbf{orthogonal matrix} is a square matrix $P$ with real entries such that $P^{-1} = P^T$ 
\end{definition}

\begin{definition}
    The \textbf{orthogonal group} is defined $O(n) = \{P \in \Mat(n;\mathbb{R}): \transpose{P}P\}$
\end{definition}

\begin{corollary}[The Spectral Theorem for Real Symmetric Matrices]
    Let $A$ be a real $(n \times n)$-symmetrical matrix. Then $\exists$ an $(n \times n)$-orthogonal matrix $P$ such that
    $$\transpose{P}AP = P^{-1}AP = \text{diag}(\lambda_1,..., \lambda_n)$$
    Where $\lambda_1, ..., \lambda_n$ are the (necessarily real) eigenvalues of $A$, repeated according to their multiplicity as roots of the characteristic polynomial of $A$
\end{corollary}

\begin{definition}[Unitary Matrix]
    An \textbf{unitary matrix} is an $(n \times n)$-matrix $P$ with complex entries such that $\overline{P}^TP = I_n$. In other words, $P^{-1} = \overline{P}^T$ 
\end{definition}

\begin{corollary}[The Spectral Theorem for Hermitian Matrices]
    Let $A$ be a $(n\times n)$-hermitian matrix. Then there is an $(n \times n)$-unitary matrix $P$ such that
    $$\overline{P}^TAP = P^{-1}AP = \text{diag}(\lambda_1,..., \lambda_n)$$
    Where $\lambda_1, ..., \lambda_n$ are the (necessarily real) eigenvalues of $A$, repeated according to their multiplicity as roots of the characteristic polynomial of $A$
\end{corollary}


% Chapter 6
\section{Jordan Normal Form}
Will not be incuded in  Exam yay
% % 6.1
% \subsection{Motivation}
% Some intro...

% % 6.2
% \subsection{Statement of the Jordan Normal Form and Strategy of Proof}
% \begin{definition}
%     Given an integer $r \geq 1$ define an $(r \times r)$-matrix $J(r)$, called the \textbf{nilpotent Jordan block of size} $r$, by the rule $J(r)_{ij} = 1$ for $j = i+1$ and $J(r)_{ij} = 0$ otherwise. In particular, $J(1) = \begin{pmatrix}
%         0
%     \end{pmatrix}$\\
%     Given an integer $r \geq 1$ and a scalar $\lambda \in F$ define an $(r \times r)$-matrix $J(r,\lambda)$ called the \textbf{Jordan block of size $r$ and eigenvalue $\lambda$}, by the rule
%     $$J(r,\lambda) = \lambda I_n + J(r) = D + N$$
%     where $D = \lambda I_n,\quad N = J(r)$ such that $DN = ND = \lambda J(r)$
% \end{definition}

% \begin{theorem}[Jordan Normal Form]
%     Let $F$ be an algebraically closed field. Let $V$ be a finite dimensional vector space and let $\phi: V \rightarrow V$ be an endomorphism of $V$ with characteristic polynomial
%     $$\chi_\phi(x) = (\lambda_1 - x)^{a_1}(\lambda_2 - x)^{a_2}...(\lambda_s - x)^{a_s} \in F[x]\left(a_i \geq 1, \sum_{i=1}^s a_i = n\right)$$
%     for distinct $\lambda_1, ..., \lambda_s \in F$. Then $\exists$ order basis $\mathcal{B}$ of V such that the matrix of $\phi$ w.r.t. the basis $\mathcal{B}$ is block diagonal with Jordan blocks on the diagonal
%     $$_\mathcal{B}[\phi]_\mathcal{B} = \diag(J(r_{11},\lambda_1),...,J(r_{1m_1},\lambda_1),J(r_{21},\lambda_2),...,J(r_{sm_s},\lambda_s))$$
%     with $r_{11},...,r_{1m_1},r_{21},...,r_{sm_s} \geq 1$ such that
%     $$a_i = \sum_{j = 1}^{m_i} r_{ij} \quad \forall i: 1 \leq i \leq s $$
% \end{theorem}

% % 6.3
% \subsection{The Proof of Jordan Normal Form}
% I'd suggest reading the full notes from here...

\newpage
\section*{Quick References}
\begin{itemize}
    \item \textbf{Abelian Group}: For group in with operation $(A,*)$ to be abelian group, the following has to be fulfilled:
        \begin{enumerate}
            \item \textbf{Closure}: $\forall a,b \in A, a*b \in A$
            \item \textbf{Associativity}: $\forall a,b,c \in A, (a*b)*c = a*(b*c)$
            \item \textbf{Identity}: $\exists e \in A \text{ s.t. } e*a = a*e = a \forall a \in A$
            \item \textbf{Inverse}: $\forall a \in A \exists a^{-1} \textbf{ s.t. } a^{-1}*a = a*a^{-1} = e$
            \item \textbf{Communtativity}: $\forall a, b \in A, a*b = b*a$
        \end{enumerate}
    \item \textbf{Change of Basis}: for standard basis $A = \{\vec{v_1},...,\vec{v_n}\}$ and basis $B = \{(\sum_{i=1}^n{a_{i1}}\vec{w_i}),...,(\sum_{i=1}^n{a_{in}}\vec{w_i})\}$, $$_A[id]_B = (a_{ij})$$
\end{itemize}

\subsubsection*{The blah-morphism table}
\begin{center}
    \begin{tabular}{| c || c | c | c |}
        \hline
        blah-morphism & Linear? & Bijective? & $f:V \rightarrow V$?\\
        \hline\hline
        Homomorpgism & Yes & Not-required & Not-required\\
        \hline
        Isomorphism & Yes & Yes & Not-required\\
        \hline
        Endomorphism & Yes & Not-required & Yes \\
        \hline
        Automorphism & Yes & Yes & Yes \\
        \hline 
    \end{tabular}
\end{center}

\end{document}