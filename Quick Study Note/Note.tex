\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,scrextend}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{adjustbox}
\usepackage{enumitem}

\setlength{\parskip}{0em}
\setlist[enumerate]{itemsep=0mm}
\setlist[itemize]{itemsep = 0mm}

\theoremstyle{definition}
\newtheorem{definition}{DEFINITION}[subsection]


\pagestyle{fancy}

\let\newproof\proof
\renewenvironment{proof}{\begin{addmargin}[1em]{0em}\begin{newproof}}{\end{newproof}\end{addmargin}\qed}

\newtheorem{theorem}{THEOREM}[subsection]

\newcommand{\uline}[1]{\rule[0pt]{#1}{0.4pt}}
\newcommand{\Hom}{\text{Hom}}
\newcommand{\Maps}{\text{Maps}}
\newcommand{\image}{\text{im}}
\newcommand{\Mat}{\text{Mat}}
\newcommand{\vect}[1]{\overrightarrow{#1}}
\newcommand{\suchthat}{\textit{ s.t. }}

%Set the code name here
\newcommand{\coursename}{Honours Algebra}

\usepackage{tcolorbox}
\tcbuselibrary{theorems}

\newtcbtheorem[number within=section]{mytheo}{Theorem}%
{colback=red!5,colframe=red!35!black,fonttitle=\bfseries}{th}
\newtheorem{lemma}{LEMMA}[subsection]
\newtheorem{prop}{PROPOSITION}[subsection]
\newtheorem{corollary}{COROLLARY}[subsection]
 
\lhead{\coursename}
\rhead{Quick Notes}

\title{\coursename\\Quick Notes}
\author{Ian S.W. Ma}
\date{}

 
 
\begin{document}
\maketitle
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
\pagenumbering{gobble}
\tableofcontents
\newpage

\pagenumbering{arabic}
\setcounter{page}{1}
\section{Vector Spaces}

% Section 1.1
\subsection{Solution of Simultianeous Linear Equations}
Assume \(F = \mathbb{Q},\mathbb{R} \text{ or } \mathbb{C} \), where $a_{ij},b_i \in F$, then
\[
    \sum_{j=1}^m{a_{ij}x_j} = b_i \quad \forall i \in [1,n]:i \in \mathbb{Z}
\]
is a \textbf{system of linear Equations}

\begin{itemize}
    \item if all $b$'s are $0$ then the system is \textbf{homogenous}
    \item $L = \{x_1,...,x_m\}$ is the \textbf{solution set} of Equations
\end{itemize}

% Section 1.2
\subsection{Fields and Vector Spaces}

\begin{definition}
    \quad
    \begin{enumerate}
        \item
              A \textbf{field} $F$ is a set with functions:
              \begin{itemize}
                  \item \textbf{addition} $= +:F\times F \rightarrow F; (\lambda,\mu)\mapsto\lambda+\mu$
                  \item \textbf{multiplication} $= \cdot:F\times F \rightarrow F; (\lambda,\mu)\mapsto\lambda\mu$
              \end{itemize}
              such that $(F,+)$ ans $(F\backslash\{0\},\cdot)$ are abelian groups, with:
              $$\lambda(\mu + \nu) = \lambda\mu + \lambda\nu \in F \quad \forall \lambda,\mu,\nu\in F$$
              The neutral elements are called $0_F,1_F$, in particular for all $\lambda,\mu\in F$
              \begin{itemize}
                  \item $\lambda + \mu = \mu + \lambda \in F$
                  \item $\lambda \cdot \mu = \mu \cdot \lambda \in F$
                  \item $\lambda + 0_F = \lambda \in F$
                  \item $\lambda\cdot 1_F = \lambda \in F$
              \end{itemize}
              For all $\lambda \in F$ there exists $-\lambda\in F$ such that $\lambda + (-\lambda) = 0_F \in F$\\
              For all $\lambda \neq 0 \in F$ tehre exists $\lambda^{-1}\neq 0 \in F$ such that $\lambda(\lambda^{-1}) = 1_F\in F$
              \newpage\item
              A \textbf{vector space} $V$ \textbf{over a field} $F$ is  a pair consisting of an abelian group $V = (V,\dot{+})$ and a mapping
              $$F\times V \rightarrow V; (\lambda,\overrightarrow{v})$$
              such that for all $\lambda,\mu\in F$ and $\overrightarrow{v},\overrightarrow{w}\in V$ the following identities hold:
              \begin{itemize}
                  \item $\lambda(\overrightarrow{v}+\overrightarrow{w}) = \lambda\overrightarrow{v} + \lambda\overrightarrow{w}$ \textbf{Distributive Law}
                  \item $(\lambda+\mu)\overrightarrow{v} = \lambda\overrightarrow{v} + \mu\overrightarrow{v}$ \textbf{Distributive Law}
                  \item $\lambda(\mu\overrightarrow{v}) = (\lambda\mu)\overrightarrow{v}$ \textbf{Associativity Law}
                  \item $1_F\overrightarrow{v} = \overrightarrow{v}$
              \end{itemize}
    \end{enumerate}
\end{definition}
\begin{lemma}
    If $V$ is a vector space and $\overrightarrow{v} \in V$ then $0\overrightarrow{v} = \overrightarrow{0}$
\end{lemma}
\begin{lemma}
    If $V$ is a vector space and $\overrightarrow{v} \in V$ then $(-1)\overrightarrow{v} = -\overrightarrow{v}$
\end{lemma}
\begin{lemma}
    If $V$ is a vector space over a field $F$ then $\lambda\overrightarrow{0} = \overrightarrow{0} \quad \forall \lambda\in F$
\end{lemma}

% Section 1.3
\subsection{Product of Sets and of Vector Spaces}
\begin{itemize}
    \item
          \textbf{Cartesian product} of sets: $X_1 \times ... \times X_n := \{(x_1,...,x_n):x_i\in X_i \text{ for } 1 \leq i \leq n\}$, an element of this product is known as a product n-tuples.\newline
\end{itemize}
There are special mappings called \textbf{projections} for a cartesian product
\[\begin{split}
        \text{pr}_i: X_i\times ... \times X_n &\rightarrow X_i\\
        (x_i,...,x_n) &\mapsto x_i
    \end{split}\]
The cartesian product of $n$ copies of a set $X$ is written in short as $X^n$\\

$\forall n,m \geq 0$, $X^n\times X^m\xrightarrow{\sim} X^{n+m}; ((x_1,...,x_n),(x_{n+1},...,x_{n+m}))\mapsto(x_1,...,x_n,x_{n+1},x_{n+m})$

% Section 1.4
\subsection{Vector Subspaces}
\begin{definition}
    A subset $U$ of a vector space $V$ is called a \textbf{vector subspace} or \textbf{subspace} if $U$ contains the zero vector ($\overrightarrow{0}$) and whenever $\overrightarrow{u},\overrightarrow{v}$ and $\lambda \in F$ we have $\overrightarrow{u}+\overrightarrow{v}\in U$ and $\lambda\overrightarrow{u}\in U$
\end{definition}
\begin{prop}
    Let $T$ be a subset od vector space $V$ over a field $F$. Then amongst all vector subspaces of $V$ that include $T$ there is a smallest vector subspace
    $$\langle T \rangle = \langle T \rangle_F \subseteq V$$
\end{prop}
\begin{definition}
    A subset $S$ of a vector space $V$ is called a \textbf{generating set} of a $V$ if its span is all of the vector space. A vector space that has a finite generating set is \textbf{finitely generated}
\end{definition}
\begin{definition}
    Check Definition 1.4.9 on Iain's(Not me I am Ian) note I dun think it's English
\end{definition}

% Section 1.5
\newpage
\subsection{Linear Independence and Bases}
\begin{definition}
    A subset $L = \{\overrightarrow{v_1},...,\overrightarrow{v_n}\}$ of a vector subspace $V$ is \textbf{linearly independent} if for all arbitrary scalars $\alpha_1, ..., \alpha_n \in F$:
    $$\sum_{i=1}^n{\alpha_i\overrightarrow{v_i}} = 0 \rightarrow \alpha_1 = ... = \alpha_n = 0$$
\end{definition}
\begin{definition}
    A subset $L = \{\overrightarrow{v_1},...,\overrightarrow{v_n}\}$ of a vector subspace $V$ is \textbf{linearly dependent} if it's not \textbf{linearly independent}, whcih mean there exist some $\alpha_j \in \{a_1, ..., a_n\}, \alpha_j \neq 0$ such that  $$\sum_{i=1}^n{\alpha_i\overrightarrow{v_i}} = 0$$
\end{definition}
\begin{definition}
    A \textbf{basis of a vector space} $B$ of a vector space $V$ is a linearly independent generating set in $V$
\end{definition}
\begin{definition}
    Let $F$ be a field, $V$ a vector space over $F$ and $\overrightarrow{v_1},...,\overrightarrow{v_r} \in V$ vectors. The \textit{family} $(\overrightarrow{v_i})_{1\leq i\leq r}$ is a basis of $V$ if and only if the following "\textit{evaluation}"
    \[
        \begin{split}
            \Phi : F^r &\rightarrow V\\
            (\alpha_1,...,\alpha_r) &\mapsto \alpha_1\overrightarrow{v}_1 + ... + \alpha_r\overrightarrow{v}_r
        \end{split}
    \]
    is a bijection
\end{definition}
\begin{definition}
    \begin{itemize}
    The following for a subset $E$ of a vector space $V$ are equivalent:An isomorphism of a vector space to itself is called \textbf{anautomorphism} of our vector space.
        \item Our subset $E$ is a basis, ie. a linearly independent generating set;
        \item Our subset $E$ is a minimal among all generating sets, meaning that $E\backslash\{\overrightarrow{v}\}$ does not generate $V$
        \item Our subset $E$ is maximal among all linearly independent subsets, meaning that $E\cup\{\overrightarrow{v}\}$ is not linearly independent for any $\overrightarrow{v} \in V$
    \end{itemize}
\end{definition}
\begin{corollary}
    Let $V$ be a finitely generated vector space over a vield $F$, then $V$ is a basis
\end{corollary}
\begin{theorem}
    Let $V$ be a vector space.
    \begin{itemize}
        \item If $L \subset V$ is a linearly independent subset and $E$ is a minimal amongst all generating sets of our vector with $L \subseteq E$, then $E$ isn a basis.
        \item If $L \subseteq V$ is a generating set and if $L$ is maximal amongstall linearly independent subsets of vector space with $L \subseteq E$, then $L$ is a basis.
    \end{itemize}
\end{theorem}
\newpage
\begin{theorem}
    Let field $F$, $F$-vector space $V$ and family of vectors $(\overrightarrow{v_i})_{i\in I}$ from $V$, The following are equivalent:
    \begin{itemize}
        \item The family $(\overrightarrow{v_i})_{i\in I}$ is a basis of $V$;
        \item For each vector $\overrightarrow{v} \in V$ there is percisely one family $(a_i)_{i\in I}$ od elements of field $F$, almost all of which are zero and such that: $$\overrightarrow{v} = \sum_{i\in I} {a_i\overrightarrow{v_i}}$$
    \end{itemize}
\end{theorem}

\subsection{Dimension of a vector space}
\begin{theorem}[Fundamental Estimate of Linear Algebra]
    No linearly independent subset of a given vector space has more elements then a generating set. Thus if $V$ is a vector space, $L \subset V$ a linearly independent subset and $E \subseteq V$ a generating set, then $|L| \leq |E|$
\end{theorem}

\begin{theorem}[Steinitz Exchange Theorem]
    Let $V$ be a vector space, $L \subset V$ a finite linearly independent subset and $E \subseteq V$ a generating set. Then there is an injection $\phi: L \hookrightarrow E$ such that $(E \backslash \phi (L)) \cup L$ is also a generating set for $V$
\end{theorem}

\begin{lemma}[Exchange Lemma]
    Let $V$ be a vector space, $M \subseteq V$ a linearly independent subset, adn $E \subseteq V$ a generating subset, such that $M \subseteq E$. if $\overrightarrow{w} \in V\backslash M$ is a vector not belonging to $M$ such that $M \cup \{\overrightarrow{w}\}$ is linearly independent, then there exists $\overrightarrow{e} \in E\backslash M$ such that $\{E\backslash\{\overrightarrow{e}\}\}\cup \{\overrightarrow{w}\}$ is a generating set for $V$
\end{lemma}

\begin{corollary}[Cardinality of Bases]
    Let $V$ be a finitely generated vector space.
    \begin{itemize}
        \item $V$ has a finite basis
        \item $V$ cannot have an infinite basis
        \item Any two bases of $V$ have the same number of elements
    \end{itemize}
\end{corollary}

\begin{definition}
    The cardinality of one (and by Cardinality of Bases each) basis of a finitely generated vector space $V$ is called the \textbf{dimension} of $V$ and will be denoted by $\dim(V)$. If the vector space is not finitely generated, then we write $\dim(V) = \infty$ and call $V$ infinite dimensional. As usual, we will ignore the difference between infinities.
\end{definition}

\begin{corollary}[Cardinality Criterion for Bases]
    Let $V$ be a finitely generated vector space.
    \begin{itemize}
        \item Each linearly independent subset $L \subset V$ has at most $\dim(V)$ elements, and if $|L|=\dim(V)$ then $L$ is actually a basis
        \item Each generating set $E \subseteq V$ has at least $\dim(V)$ elements, and if $|E| = \dim(V)$ the $E$ is actually a basis.
    \end{itemize}
\end{corollary}

\begin{corollary}[Dimension Estimate for Vector Subspaces]
    A proper vector subspace of a finite dimensional vector space has itself a strictly smaller dimension.
\end{corollary}

\begin{theorem}[The Dimension Theorem]
    Let $V$ be vectpr space containing vector subspaces $U,W \subseteq V$. Then
    $$\dim(U + W) + \dim(U\cap W) = \dim(U) + \dim(W)$$
\end{theorem}

\subsection{Linear Mappings}
\begin{definition}
    Let $V,W$ be a vector spaces over a field $F$. A mapping $f: V \rightarrow W$ is called \textbf{linear} or more percisely $\mathbf{F}\textbf{-linearly}$ or even a \textbf{homomorphism of $F$-vector spaces} if for all $\overrightarrow{v}_1, \overrightarrow{v}_2 \in V$ and $\lambda \in F$ we have \[\begin{split}
        f(\overrightarrow{v}_1 + \overrightarrow{v}_2) &= f(\overrightarrow{v}_1) + f(\overrightarrow{v}_2)\\
        f(\lambda\overrightarrow{v}_1) &= \lambda f(\overrightarrow{v}_1) 
    \end{split}\]
\end{definition}

\begin{itemize}
    \item A bijective linear mapping is called as \textbf{isomorphism} of vector spaces. If there is an isomorphism bwetween two vector spaces we say them \textbf{isomorphic}.
    \item A \textbf{homomorphism} from one vector space to itself is called anendomorphismof our vector space.
    \item An isomorphism of a vector space to itself is called \textbf{anautomorphism} of our vector space.
\end{itemize}

\begin{definition}
    A point that is sent to itself br a mapping is called a \textbf{fixed point} of the mapping.
    Given a mapping $f: X \rightarrow X$, we donote the set of fixed points by $$X^f = \{x \in X: f(x) = x\}$$
\end{definition}

\begin{definition}
    Two vector subspaces $V_1,V_2$ of a vector space $V$ are called \textbf{complementary} of addition defines a bijection $V_1 \times V_2 {\stackrel{\sim}{\rightarrow}} V$
\end{definition}

\begin{theorem}[The Classification of Vector Spaces by their Dimension]
    Let $n\in \mathbb{N}$. Then a vector space over a field $F$ is isomorphic to $F^n$ i.f.f it has dimension $n$
\end{theorem}

\begin{lemma}[Linear Mappings and Bases]
    Let $V,W$ be vector spaces over $F$ and let $B \subset V$ be a basis. Then restriction of a mapping gives a bijection
    \[\begin{split}
        \Hom_F(V,W) &\stackrel{\sim}{\rightarrow} \Maps(B,W)\\
        f &\mapsto f|_B
    \end{split}\]
\end{lemma}

\begin{prop}
    \begin{itemize}
        \item Every injective linar mapping $f:V \hookrightarrow W$ has a \textbf{left inverse}, in other words a linear mapping $g: W \rightarrow V$ such that $g \circ f = \text{id}_V$
        \item Every surjective linear mapping $f:V \twoheadrightarrow W$ has a \textbf{right inverse}, in other words a linear mapping $g: W \rightarrow V$ such that $g \circ f = \text{id}_W$
    \end{itemize}
\end{prop}

\subsection{Rank-Nullity Throrem}
\begin{definition}
    The \textbf{image} of a linear mapping $f: V \rightarrow W$ is the subset $\image(f) = f(V) \subseteq W$. The \textbf{preimage} of the zero vector (\textbf{kernel}) of a linear mapping $f: V \rightarrow W$ is denoted by $$\ker(f) := f^{-1}(0) = \{v \in V: f(v) = 0\}$$
    The kernel is a vector subspace if $V$
\end{definition}

\begin{lemma}
    A linear mapping $f: V \rightarrow W$ is injective if an only if it's kernel is zero.
\end{lemma}

\begin{theorem}[Rank-Nullity Theorem]
    Let $f: V \rightarrow W$ be a linear mapping between vector spaces, then $\dim(V) = \dim(\ker{f}) + \dim(\image(f))$
\end{theorem}

% Chapter 2
\section{Linear Mappings and Matrices}
\subsection{Linear Mappings $F^m \rightarrow F^n$ and Matrices}
\begin{theorem}[Linear mappings $F^m \rightarrow F^n$ and Matrices]
    Let $F$ be a field and let $m,n in \mathbb{N}$ be neutral numbers. There is a nijection between the space of linear mappings $F^m \rightarrow F^n$ and the set of matrices woth $n$ rows anf $m$ columns and entries in $F$:
    \[\begin{split}
        \textbf{M}: \Hom_F(F^m,F^n) &\stackrel{\sim}{\rightarrow} \Mat(n \times m; F)\\
        f &\mapsto [f]
    \end{split}\]
    This attaches to each linear mapping$f$ its \textbf{representing matrix} $\textbf{M}(f):=[f]$. The column of this matrux are the images under $f$ of the standard basis elements if $F^m$:
    $$[f] = (f(\overrightarrow{e}_1)|f(\overrightarrow{e}_2)|\dots|f(\overrightarrow{e}_m))$$
\end{theorem}

\begin{definition}
    Let $n,m,l\in \mathbb{N}$, $F$ a field and let $A \in \Mat(n\times m; F)$ and $B \in \Mat(m\times l; F)$ be matrices. The \textbf{product} $A \circ B = AB \in \Mat(n\times l; F)$ is the matrix defined by $$(AB)_{ik} = \sum_{j=1}^m{A_{ij}B_{jk}}$$
\end{definition}

\begin{theorem}[Composition of Linear Mapping and Products of Matrices]
    Let $g:F^l \rightarrow F^m$ and $f:F^m \rightarrow F^n be linear mappings$. Then $[f \circ g] = [f] \circ [g]$
\end{theorem}

\subsection{Basic Properties of Matrices}
\begin{definition}
    A matrix $A$ is called \textbf{invertable} if there exist matrices such tht $BA = I$ and $AC = I$
\end{definition}

\begin{definition}
    will define an \textbf{elementary matrix} to be any square matrix that differs from the identity matrix in at most one entry.
\end{definition}

\begin{theorem}
    Every square matrix with entries in a field can be written as a product if elementary matrices.
\end{theorem}

\begin{definition}
    Any matrix whose only non-zero entries lies on the diagonal, and which has first 1's along the diagonal and then 0's, is said to be in \textbf{Smith Normal Form}:
    \[A_{ij} = \begin{cases}\begin{split}
        0 &\text{ if } i \neq j\\
        1 &\text{ if } A_{(i+1)(j+1)} = 1\\
        0 &\text{ otherwise}
    \end{split}\end{cases}\]
\end{definition}

\begin{theorem}[(Transformation of a Matrix into Smith Normal Form]
    For each matrix $A \in \Mat(n\times m;F)$ there exist invertable matrices $P,Q$ such that $PAQ$ is a matrix in Smith Normal Form.
\end{theorem}

\begin{definition}
    The \textbf{column rank} of a matrix $A \in \Mat(n\times m;F)$ is the dimension of the subsequence of $F^n$ generated by the columns of $A$. Simmilarly, the \textbf{row rank} of $A$ is the dimension of the subspace of $F^m$ generated by the rows of A.
\end{definition}

\begin{theorem}
    The column rank and the row rank of any matrix are equal.
\end{theorem}

Let's now refer the column and row rank as \textbf{rank} for the sake of not losing any generality.

\begin{definition}
    When the rank is as big as possible, meaning that it's equal to either the number of rows or number of columns (whichever is smaller), then the matrix has \textbf{full rank}
\end{definition}


\subsection{Abstract Linear Mappings and Matrices}
\begin{theorem}[Abstract  Linear  Mappings  and  Matrices]
    Let $F$ be a field, $V,W$ vector spaces over $F$ with ordered bases $\mathcal{A} = (\vect{v}_1,...,\vect{v}_m)$ and $\mathcal{B} = (\vect{w}_1,...,\vect{w}_n)$. Then to each linear mapping $f:V\rightarrow W$ we assosiate bases a \textbf{representing matrix} $_\mathcal{B}[f]_\mathcal{A}$ whose entried $a_ij$ are defined by the identity $$f(\vect{v}_j) = \sum_{i=1}^n{a_{ij}\vect{w}_i}\in W$$
    This produces a bijection, which is event an isomorphismof vector spaces:
    \[\begin{split}
        \textbf{M}_\mathcal{B}^\mathcal{A}: \Hom_F(V,W) &\stackrel{\sim}{\rightarrow} \Mat(n \times m; F)\\
        f &\mapsto _\mathcal{B}[f]_\mathcal{A} 
    \end{split}\]
\end{theorem}
We call $\textbf{M}_\mathcal{B}^\mathcal{A}(f) = _\mathcal{B}[f]_\mathcal{A}$ the \textbf{epresenting matrix of the mappingfwith respect to the bases $\mathcal{A}$ and $\mathcal{B}$}

\begin{theorem}[The Representing Matrix of a Composition of Linear Mappings]
    Let $F$ be a field and $U,V,W$ finite dimensional vector spaces over $F$ with ordered bases $\mathcal{A,B,C}$. If $f:U\rightarrow V$ and $g:V \rightarrow W$ are linear mappings, then the representing matrixof the composition $g \circ f: U \rightarrow W$ is the matrix product of the representing matrix of $f$ and $g$: $$_\mathcal{C}[g\circ f]_\mathcal{A} = _\mathcal{C}[g]_\mathcal{B} \circ _\mathcal{B}[f]_\mathcal{A}$$
\end{theorem}

\begin{definition}
    Let $V$ be a finite dimensional vector space with an ordered basis $\mathcal{A} = (\vect{v}_1,...,\vect{v}_m)$. We will denote the invese to the bijection of $\Phi_\mathcal{A} \stackrel{\sim}{\rightarrow} V,(\alpha_a,...,\alpha_m)^T \mapsto \sum_{i=1}^m{\alpha_i\vect{v}_i}$ by
    $$\vect{v} \mapsto _\mathcal{A}[\vect{v}]$$
\end{definition}

\begin{theorem}[Representation of the Image of a Vector]
    Let $V,W$ be finite dimensional vector spaces over $F$ with ordered bases $\mathcal{A,B}$ and let $f:V\rightarrow W$ be a linear mapping. The following holds for $\vect{v} \in V$:
    $$_\mathcal{B}[f(\vect{v})] = _\mathcal{B}[f]_\mathcal{A} \circ _\mathcal{A}[\vect{v}]$$
\end{theorem}

\subsection{Change of Matrix by Change of Basis}
\begin{definition}
    Let $\mathcal{A} = (\vect{v}_1,...,\vect{v}_n), \mathcal{B} = (\vect{w}_1,...,\vect{w}_n)$ be ordered bses of the same $F$-vector space $V$. Then the matrix representing the identity mapping with respect to the bases $_\mathcal{B}[\text{id}_V]_\mathcal{A}$ is called a \textbf{change of basis matrix}. By definition, its entries are given by the equalities $\vect{v}_j = \sum_{i=1}^n{a_{ij}\vect{w}_i}$
\end{definition}

\begin{theorem}[Change of Basis]
    Let $V,W$ be finite dimensional vector spaces over $F$ and let $f:V \rightarrow W$ br a linear mapping. Suppose that $\mathcal{A,A'}$ are order bases of $V$ and $\mathcal{B,B'}$ are ordered bases of $W$. Then:
    $$_\mathcal{B'}[f]_\mathcal{A'} = _\mathcal{B'}[\text{id}_W]_\mathcal{B} \circ _\mathcal{B}[f]_\mathcal{A} \circ _\mathcal{B}[\text{id}_V]_\mathcal{A'}$$
\end{theorem}

\begin{corollary}
    Let $V$ be a finite dimensional vector space and let $f:V\rightarrow V$ be an endomorphism of $V$. Suppose that $\mathcal{A,A'}$ are ordered bases od $V$. Then 
    $$_\mathcal{A'}[f]_\mathcal{A'} = _\mathcal{A}[\text{id}_V]_\mathcal{A'}^{-1} \circ _\mathcal{A}[f]_\mathcal{A} \circ _\mathcal{A}[\text{id}_V]_\mathcal{A'}$$
\end{corollary}

\begin{theorem}[Smith Normal Form]
    Let $f:V\rightarrow W$ be a linear mapping between finite dimensional $F$-vector spaces. There exist an order basis $\mathcal{A}$ of $V$ and an ordered basis $\mathcal{B}$ of $W$, such that the representing matrix $_\mathcal{B}[f]_\mathcal{A}$ has zero entries everywhere except possibly on one diagonal, and along the diagonaltherer are 1's first, followed by 0's  
\end{theorem}

\begin{definition}[Trace]
    The trace of a square matrix is defined to be the sum of its diagonal entries:
    $$\text{tr}(A) = \sum_{i=1}^{n}{a_{ii}}$$
\end{definition}

% Chapter 3
\section{Rings and Modules}
\subsection{Rings}
\begin{definition}
    A \textbf{ring} is a set with two operations $(R,+,\cdot)$ that satisfy:
    \begin{enumerate}
        \item $(R,+)$ is an abelian group
        \item $(R,\cdot)$ is a \textbf{monoid}, meaning that the second operation $\cdot$: $R\times R\rightarrow R$ is assosiative and that there is an \textbf{identity element} $1 = 1_R \in R$, often called just the \textbf{identity}, with the peoperty that $1\cdot a = a \cdot 1 = a \quad\forall a \in R$
        \item The Distributive laws hold, meaning that for all $a,b,c \in R$ \[\begin{split}
            a\cdot (b+c) &= (a\cdot b)+(a \cdot c)\quad \textbf{addition}\\
            (a+b) \cdot c &= (a \cdot c) + (b \cdot c) \quad \textbf{multiplication}
        \end{split}\]
    A ring which element is commutative, that means $a \cdot b = b \cdot a \quad \forall a,b \in R$, is a \textbf{commutative ring}
    \end{enumerate}
\end{definition}

\begin{prop}[Divisbility by Sum]
    A natural number is divisible by 3(respectively by 9) percisely when the sum of its digits is divisible by 3 (or 9)
\end{prop}

\begin{definition}
    A \textbf{field} is a non-zero commutative ring $F$ in which every non-zero element $a\in F$ has an inverse $a^{-1} \in F$, that is an element $a^{-1}$ with the property that $a\cdot a^{-1} = a^{-1}\cdot a = 1$
\end{definition}

\begin{prop}
    Let $m$ be a positive integer. The commutative ring $\mathbb{Z}/m\mathbb{Z}$ is a field i.f.f. $m$ is prime.
\end{prop}

\subsection{Peoperties of Rings}
\begin{lemma}[Mutiplying by zero and negatives]
    Let $R$ be a ring and let $a,b \in R$. Then:
    \begin{enumerate}
        \item $0a = 0 = a0$
        \item $(-a)b = -(ab) = a(-b)$
        \item $(-a)(-b) = ab$
    \end{enumerate}
\end{lemma}

\begin{lemma}[Rules of multiples]
    Let $R$ be a ring, let $a,b\in R$ and $m.n \in \mathbb{Z}$. Then:
    \begin{enumerate}
        \item $m(a+b) = ma + mb$
        \item $(m+n)a = ma + na$
        \item $m(na) = (mn)a$
        \item $m(ab) = (ma)b = a(mb)$
        \item $(ma)(nb) = (mn)(ab)$
    \end{enumerate}
\end{lemma}

\begin{definition}
    Let $R$ be a ring. An element $a \in R$ is called a \textbf{unit} if it is \textbf{invertable} in $R$ or in other words \textbf{has a multiplicative inverse in} $R$, meaning that $\exists a^{-1} \in R$ such that $$aa^{-1} = 1 = a^{-1}a$$
\end{definition}

\begin{prop}
    The set $R^{\times}$ of units in a ring $R$ forms a group under multiplication.
\end{prop}

\begin{definition}[Zero-divisor]
    In a ring $R$ a non-zero element $a$ is called a \textbf{zero-divisor} or \textbf{divisor of zero} if there exists a non-zero element $b$ such that either $ab = 0$ or $ba = 0$
    $${a \neq 0 \in R, \exists b \neq 0 \in R \suchthat ab = 0 \cup ba = 0} \Rightarrow {a\text{ is a zero divisor}}$$
\end{definition}

\begin{definition}[Integral domain]
    An \textbf{integral domain} is a non-zero commutative ring that has no zero-divisors, therefore the if $D$ is an integral domain then:
    \begin{enumerate}
        \item $ab = 0 \Rightarrow  a = 0$ or $b = 0$, and
        \item $a,b\neq 0 \Rightarrow ab \neq 0$
    \end{enumerate}
\end{definition}

\begin{prop}
    Let $m$ be a natural number. The $\mathbb{Z}/m\mathbb{Z}$ is an integral domain i.f.f. $m$ is prime
\end{prop}

\begin{theorem}
    Every \textbf{fnite} integral domain is a field
\end{theorem}

\subsection{Polynomials}
\begin{definition}
    Let $R$ be a ring. A \textbf{polnomial} over $R$ is an expression of the form
    $$P = a_0 + a_1X + a_2X^2 + ... + a_mX^m$$
    for some $m \in \mathbb{N}\backslash 0$ and elements $a_i \in R$ for $0 \leq i \leq m$. The set of all Polynomials over $R$ is denoted by $R[X]$. Incase $a_m$ is not zero, the polynomial $P$ has a degree of $m$, written $\text{deg}(P) = m$, where $a_m$ is the leading coefficient.\\
    When the leading coefficient is 1 the polynomial is a \textbf{monic} polynomial, linear for $a_1$, quardratic for $a_2$, then cubic for $a_3$.
\end{definition}

\begin{definition}
    Whith the definition in the set $R[X]$ becomes a ring called the \textbf{ring of polynomials with coefficients in $R$, or over $R$}. The zero and the indentity of $R[X]$ are the zero and identity of $R$ resp.
\end{definition}

\begin{lemma}
    \quad
    \begin{enumerate}
        \item If a ring $R$ with no zero-divisors, then $R[X]$ has no zero-divisors and $\deg(PQ) = \deg(P) + \deg(Q)$ for all non-zero $P,Q\in R[X]$
        \item If $R$ is an integral domain then so is $R[X]$
    \end{enumerate}
\end{lemma}

\begin{theorem}[Division and Remainder]
    Let $R$ be an integral domain and let $P,Q\in R[X]$ with $Q$. Then there exists unique $A,B\in R[X]$ such that $P = AQ + B$ and $\deg(B) < \deg(Q)$ or $B = 0$
\end{theorem}

\begin{definition}
    Let $R$ be a commutative ring and $P\in R[X]$ a polynomial. Then the polynomial $P$ can be evaluated at the element $\lambda \in R$ to produce $P(\lambda)$ by replacing the powers of $X$ in the polynomial $P$ by the corresponding powers of $X$ in the polynomial $P$ by the corresponding powers of $\lambda$. In this way we have a mapping $R[X]\rightarrow\Maps(R,R)$\\This is the percise mathematical descrption of thinking of a polynomial as a function. An element $\lambda \in R$ is a root of $P$ is $P(\lambda) = 0$
\end{definition}

\begin{definition}
    A field $F$ is algebraically closed of each non-constant polynomial $P \in F[X]\backslash F$ with coefficients in our field has a root in our field $F$
\end{definition}

\begin{theorem}[Fundamental Theorem of Algebra]
    The field of complex numbers $\mathbb{C}$, is algebraically closed.
\end{theorem}

\begin{theorem}
    If $F$ is an algebraically closed field, then every non-zero polynomial $P\in F[X]\backslash\{0\}$ \textbf{decomposes into linear factors}
    $$P = c\prod_{i=1}^n{(X-\lambda_i)}$$
    with $n \geq 0, c\in F^\times$ and $\lambda_1,...,\lambda_n\in F$
\end{theorem}

\subsection{Homomorpgism, Ideals and Subrings}
\begin{definition}
    Let $R,S$ be rings. A mapping $f:R\rightarrow S$ is a \textbf{ring homomorphism} if the following hold for all $x,y \in R$:
    \[\begin{split}
        f(x+y) &= f(x)f(y)\\
        f(xy) &= f(x)f(y)
    \end{split}\]
\end{definition}

\begin{lemma}
    Let $R,S$ be rings and $f:R\rightarrow S$ a ring homomorphism. Then for all $x,y\in R, m\in \mathbb{Z}$:
    \begin{enumerate}
        \item $f(0_r) = )_s$ Where $O_R,0_S$ are the zeros of the resptive ring
        \item $f(-x) = -f(x)$
        \item $f(x-y) = f(x) - f(y)$
        \item $f(mx) = mf(x)$
    \end{enumerate}
\end{lemma}

\begin{definition}
    A subset $I$ of a ring $R$ is an \textbf{ideal},written $I \trianglelefteq R$, if the following hold:
    \begin{enumerate}
        \item $I \neq 0$
        \item $I$ is closed under subtraction
        \item $\forall i \in I, r\in R: ir,ri\in I$
    \end{enumerate} 
\end{definition}

\begin{definition}
    Let $R$ be a commutative ring and let $T \subset R$. The the \textbf{ideal of $R$ generated by $T$} is the set
    $$_R\langle T\rangle = \left\{\sum_{i=1}^m{r_it_i}: t_1,...t_m\in T, r_1,...,r_m \in R\right\}$$
\end{definition}

\begin{prop}
    Let $R$ be a commutative ring and let $T \subseteq R$. The $_R\langle T \rangle$ is the smallest ideal of $R$ that contains $T$
\end{prop}

\begin{definition}
    Let $R$ be a commutative ring. AN ideal $I$ of $R$ is called a \textbf{principal ideal} if $I = \langle t \rangle$ for some $t \in R$
\end{definition}

\begin{definition}
    Let $R,S$ be rings with zero elements $0_R,0_S$ resp and let $f:R\rightarrow S$ be a ring homomorphism. Since $f$ is in particular a group homomorphism. Then $\ker(f)$ is an ideal of $R$
\end{definition}

\begin{lemma}
    $f$ us injective i.f.f. $\ker(f) = \{0\}$
\end{lemma}

\begin{lemma}
    The intersection of any collection of ideals of a ring $R$ is an ideal of $R$
\end{lemma}

\begin{lemma}
    Let $I,J$ be ideals of ring $R$. Then $I+J = \{a+b:a\in I, b\in J\}$ is an ideal of $R$
\end{lemma}

\begin{definition}[subring]
    Let $R$ be a ring. A subset $R'$ of $R$ is a \textbf{subring} of $R$ if $R'$ itself is a ring under the operations of addition and multiplication defined in $R$
\end{definition}

\begin{prop}[Test of a subring]
    Let $R'$ be a subset of a ring $R$. Then $R'$ is a subring i.f.f.
    \begin{enumerate}
        \item $R'$ has a multiplicative identity, and
        \item $R'$ is closed under subtraction: $a,b\in R' \rightarrow a-b\in R'$, and
        \item $R'$ is closed under multiplication
    \end{enumerate}
\end{prop}

\begin{prop}
    Let $R$ and $S$ be subrings and $f:R\rightarrow S a ring homomorphism$
    \begin{enumerate}
        \item If $R'$ is a subring of $R$ then $f(R')$ is a subring of $S$. In particular, $\text{im}(f)$ is a subring of $S$
        \item Assume that $f(1_R) = 1_S$. Then if $x$ is a unit in  $R$, $f(x)$ is a unit in $S$ and $(f(x))^{-1} = f(x^{-1})$. In this case $f$ restricts to a group homomorphism $f|_{R^\times}:R^\times \rightarrow S^\times$
    \end{enumerate}
\end{prop}

% Chap 3.5
\subsection{Equivalece Relations}
\begin{definition}
    A \textbf{relation} $R$ on a set $X$ is a subset $R \subseteq X \times X$. In this context, adn only in theis context, we write $xRy$ instead of $(x,y) \in R$. $R$ is an \textbf{equivalence relation} on $X$ when for all $x,y,z \in X$ the following holds:
    \begin{enumerate}
        \item \textbf{Reflexivity}: $xRx$
        \item \textbf{Symmetry}: $xRy \Leftrightarrow yRx$
        \item \textbf{Transitivity}: $(xRy \cap yRx) \rightarrow xRz$
    \end{enumerate}
\end{definition}

\begin{definition}
    Suppose that $\sim$ is an equivalence relation on a set $X$. For $x \in X$ the set $E(x) := \{z \in X: z \sim x\}$ is called the \textbf{equivalence class of $x$}. A subset $E \subseteq X$ is called an \textbf{equivalence class} for our equivalence relation if there is an $x \in X$ for which $E = E(x)$. An element of an equivalence relation is called a \textbf{representative} of the class. A subset $Z \subseteq X$ contains percisely one element from each equivalenceclass is called a \textbf{system of representatives} for the equivalence relation.
\end{definition}

\begin{definition}
    Given an equivalence relation $\sim$ on the set $X$ we will denote the set of equivalence classes, which is a subset of the power set $\mathcal{P}(X)$, by $$(X/\sim) := \{E(x):x\in X\}$$
\end{definition}

\begin{definition}
    $g:(X/\sim) \rightarrow Z$ is \textbf{well-defined} if $\exists$ a mapping $f:X \rightarrow Z$ such that $f$ has the peoperty $x \sim y \rightarrow f(x) = f(y) \cap g = \bar{f}$ 
\end{definition}


% Chap 3.6
\subsection{Factor Rings and the First Isomorphism Theorem}
\begin{definition}
    Let $R$ be a ring, $I \trianglelefteq R$ be an ideal in ring $R$. The set $$x + I = \{x+i:i\in I\}\subseteq R$$ is a coset of $I$ in $R$ or the coset of $x$ with respect to $I$ in $R$
\end{definition}

\begin{definition}
    Let $R$ be a ring, $I \trianglelefteq R$ an ideal, and $\sim$ the equivalence relation defined by $x \sim y \Leftrightarrow x - y \in I$. Then $R/I$, the factor ring of $R$ by $I$ or the quotient of $R$ by $I$, is the set $(R/ \sim)$ of cosets of $I$ in $R$.
\end{definition}

\begin{theorem}
    Let $R$ be a ring and $I \trianglelefteq R$ and ideal. Then $R/I$ is a ring, where the operation of additionis defined by
    $$(x+I)+(y+I) = (x+y) + I \quad \forall x, y \in R$$
    and multiplication is defined by
    $$(x + I)\dot(y+I) = xy + I \quad \forall x,y \in R$$
\end{theorem}

\begin{theorem}[First Isomorphism Theorem for Rings]
    Let $R$ and $S$ be rings. Then every ring homomorphism $f:R\longrightarrow S$ induces a ring isomorphism
    $$\bar{f}:R/\ker f \stackrel{\sim}{\rightarrow} \text{im} f$$
\end{theorem}

\end{document}