\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,scrextend}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{adjustbox}

\setlength{\parskip}{0em}

\theoremstyle{definition}
\newtheorem{definition}{DEFINITION}[subsection]


\pagestyle{fancy}

\let\newproof\proof
\renewenvironment{proof}{\begin{addmargin}[1em]{0em}\begin{newproof}}{\end{newproof}\end{addmargin}\qed}

\newtheorem{theorem}{THEOREM}[subsection]

\newcommand{\uline}[1]{\rule[0pt]{#1}{0.4pt}}
\newcommand{\Hom}{\text{Hom}}
\newcommand{\Maps}{\text{Maps}}
\newcommand{\image}{\text{im}}
\newcommand{\Mat}{\text{Mat}}
\newcommand{\vect}[1]{\overrightarrow{#1}}
% \newcommand{\vector}[1]{\ensuremath{\overrightarrow{#1}}}
\usepackage{tcolorbox}
\tcbuselibrary{theorems}

\newtcbtheorem[number within=section]{mytheo}{Theorem}%
{colback=red!5,colframe=red!35!black,fonttitle=\bfseries}{th}
\newtheorem{lemma}{LEMMA}[subsection]
\newtheorem{prop}{PROPOSITION}[subsection]
\newtheorem{corollary}{COROLLARY}[subsection]
 
\lhead{Honours Differential Equations}
\rhead{Quick Notes}

\title{Honours Algebra \\ Quick Notes}
\author{Ian S.W. Ma}
\date{}

 
 
\begin{document}
\maketitle
% --------------------------------------------------------------
%                         Start here
% --------------------------------------------------------------
\pagenumbering{gobble}
\tableofcontents
\newpage

\pagenumbering{arabic}
\setcounter{page}{1}
\section{Vector Spaces}

% Section 1.1
\subsection{Solution of Simultianeous Linear Equations}
Assume \(F = \mathbb{Q},\mathbb{R} \text{ or } \mathbb{C} \), where $a_{ij},b_i \in F$, then
\[
    \sum_{j=1}^m{a_{ij}x_j} = b_i \quad \forall i \in [1,n]:i \in \mathbb{Z}
\]
is a \textbf{system of linear Equations}

\begin{itemize}
    \item if all $b$'s are $0$ then the system is \textbf{homogenous}
    \item $L = \{x_1,...,x_m\}$ is the \textbf{solution set} of Equations
\end{itemize}

% Section 1.2
\subsection{Fields and Vector Spaces}

\begin{definition}
    \(\)\newline
    \begin{enumerate}
        \item
              A \textbf{field} $F$ is a set with functions:
              \begin{itemize}
                  \item \textbf{addition} $= +:F\times F \rightarrow F; (\lambda,\mu)\mapsto\lambda+\mu$
                  \item \textbf{multiplication} $= \cdot:F\times F \rightarrow F; (\lambda,\mu)\mapsto\lambda\mu$
              \end{itemize}
              such that $(F,+)$ ans $(F\backslash\{0\},\cdot)$ are abelian groups, with:
              $$\lambda(\mu + \nu) = \lambda\mu + \lambda\nu \in F \quad \forall \lambda,\mu,\nu\in F$$
              The neutral elements are called $0_F,1_F$, in particular for all $\lambda,\mu\in F$
              \begin{itemize}
                  \item $\lambda + \mu = \mu + \lambda \in F$
                  \item $\lambda \cdot \mu = \mu \cdot \lambda \in F$
                  \item $\lambda + 0_F = \lambda \in F$
                  \item $\lambda\cdot 1_F = \lambda \in F$
              \end{itemize}
              For all $\lambda \in F$ there exists $-\lambda\in F$ such that $\lambda + (-\lambda) = 0_F \in F$\\
              For all $\lambda \neq 0 \in F$ tehre exists $\lambda^{-1}\neq 0 \in F$ such that $\lambda(\lambda^{-1}) = 1_F\in F$
              \newpage\item
              A \textbf{vector space} $V$ \textbf{over a field} $F$ is  a pair consisting of an abelian group $V = (V,\dot{+})$ and a mapping
              $$F\times V \rightarrow V; (\lambda,\overrightarrow{v})$$
              such that for all $\lambda,\mu\in F$ and $\overrightarrow{v},\overrightarrow{w}\in V$ the following identities hold:
              \begin{itemize}
                  \item $\lambda(\overrightarrow{v}+\overrightarrow{w}) = \lambda\overrightarrow{v} + \lambda\overrightarrow{w}$ \textbf{Distributive Law}
                  \item $(\lambda+\mu)\overrightarrow{v} = \lambda\overrightarrow{v} + \mu\overrightarrow{v}$ \textbf{Distributive Law}
                  \item $\lambda(\mu\overrightarrow{v}) = (\lambda\mu)\overrightarrow{v}$ \textbf{Associativity Law}
                  \item $1_F\overrightarrow{v} = \overrightarrow{v}$
              \end{itemize}
    \end{enumerate}
\end{definition}
\begin{lemma}
    If $V$ is a vector space and $\overrightarrow{v} \in V$ then $0\overrightarrow{v} = \overrightarrow{0}$
\end{lemma}
\begin{lemma}
    If $V$ is a vector space and $\overrightarrow{v} \in V$ then $(-1)\overrightarrow{v} = -\overrightarrow{v}$
\end{lemma}
\begin{lemma}
    If $V$ is a vector space over a field $F$ then $\lambda\overrightarrow{0} = \overrightarrow{0} \quad \forall \lambda\in F$
\end{lemma}

% Section 1.3
\subsection{Product of Sets and of Vector Spaces}
\begin{itemize}
    \item
          \textbf{Cartesian product} of sets: $X_1 \times ... \times X_n := \{(x_1,...,x_n):x_i\in X_i \text{ for } 1 \leq i \leq n\}$, an element of this product is known as a product n-tuples.\newline
\end{itemize}
There are special mappings called \textbf{projections} for a cartesian product
\[\begin{split}
        \text{pr}_i: X_i\times ... \times X_n &\rightarrow X_i\\
        (x_i,...,x_n) &\mapsto x_i
    \end{split}\]
The cartesian product of $n$ copies of a set $X$ is written in short as $X^n$\\

$\forall n,m \geq 0$, $X^n\times X^m\xrightarrow{\sim} X^{n+m}; ((x_1,...,x_n),(x_{n+1},...,x_{n+m}))\mapsto(x_1,...,x_n,x_{n+1},x_{n+m})$

% Section 1.4
\subsection{Vector Subspaces}
\begin{definition}
    A subset $U$ of a vector space $V$ is called a \textbf{vector subspace} or \textbf{subspace} if $U$ contains the zero vector ($\overrightarrow{0}$) and whenever $\overrightarrow{u},\overrightarrow{v}$ and $\lambda \in F$ we have $\overrightarrow{u}+\overrightarrow{v}\in U$ and $\lambda\overrightarrow{u}\in U$
\end{definition}
\begin{prop}
    Let $T$ be a subset od vector space $V$ over a field $F$. Then amongst all vector subspaces of $V$ that include $T$ there is a smallest vector subspace
    $$\langle T \rangle = \langle T \rangle_F \subseteq V$$
\end{prop}
\begin{definition}
    A subset $S$ of a vector space $V$ is called a \textbf{generating set} of a $V$ if its span is all of the vector space. A vector space that has a finite generating set is \textbf{finitely generated}
\end{definition}
\begin{definition}
    Check Definition 1.4.9 on Iain's(Not me I am Ian) note I dun think it's English
\end{definition}

% Section 1.5
\newpage
\subsection{Linear Independence and Bases}
\begin{definition}
    A subset $L = \{\overrightarrow{v_1},...,\overrightarrow{v_n}\}$ of a vector subspace $V$ is \textbf{linearly independent} if for all arbitrary scalars $\alpha_1, ..., \alpha_n \in F$:
    $$\sum_{i=1}^n{\alpha_i\overrightarrow{v_i}} = 0 \rightarrow \alpha_1 = ... = \alpha_n = 0$$
\end{definition}
\begin{definition}
    A subset $L = \{\overrightarrow{v_1},...,\overrightarrow{v_n}\}$ of a vector subspace $V$ is \textbf{linearly dependent} if it's not \textbf{linearly independent}, whcih mean there exist some $\alpha_j \in \{a_1, ..., a_n\}, \alpha_j \neq 0$ such that  $$\sum_{i=1}^n{\alpha_i\overrightarrow{v_i}} = 0$$
\end{definition}
\begin{definition}
    A \textbf{basis of a vector space} $B$ of a vector space $V$ is a linearly independent generating set in $V$
\end{definition}
\begin{definition}
    Let $F$ be a field, $V$ a vector space over $F$ and $\overrightarrow{v_1},...,\overrightarrow{v_r} \in V$ vectors. The \textit{family} $(\overrightarrow{v_i})_{1\leq i\leq r}$ is a basis of $V$ if and only if the following "\textit{evaluation}"
    \[
        \begin{split}
            \Phi : F^r &\rightarrow V\\
            (\alpha_1,...,\alpha_r) &\mapsto \alpha_1\overrightarrow{v}_1 + ... + \alpha_r\overrightarrow{v}_r
        \end{split}
    \]
    is a bijection
\end{definition}
\begin{definition}
    \begin{itemize}
    The following for a subset $E$ of a vector space $V$ are equivalent:An isomorphism of a vector space to itself is called \textbf{anautomorphism} of our vector space.
        \item Our subset $E$ is a basis, ie. a linearly independent generating set;
        \item Our subset $E$ is a minimal among all generating sets, meaning that $E\backslash\{\overrightarrow{v}\}$ does not generate $V$
        \item Our subset $E$ is maximal among all linearly independent subsets, meaning that $E\cup\{\overrightarrow{v}\}$ is not linearly independent for any $\overrightarrow{v} \in V$
    \end{itemize}
\end{definition}
\begin{corollary}
    Let $V$ be a finitely generated vector space over a vield $F$, then $V$ is a basis
\end{corollary}
\begin{theorem}
    Let $V$ be a vector space.
    \begin{itemize}
        \item If $L \subset V$ is a linearly independent subset and $E$ is a minimal amongst all generating sets of our vector with $L \subseteq E$, then $E$ isn a basis.
        \item If $L \subseteq V$ is a generating set and if $L$ is maximal amongstall linearly independent subsets of vector space with $L \subseteq E$, then $L$ is a basis.
    \end{itemize}
\end{theorem}
\newpage
\begin{theorem}
    Let field $F$, $F$-vector space $V$ and family of vectors $(\overrightarrow{v_i})_{i\in I}$ from $V$, The following are equivalent:
    \begin{itemize}
        \item The family $(\overrightarrow{v_i})_{i\in I}$ is a basis of $V$;
        \item For each vector $\overrightarrow{v} \in V$ there is percisely one family $(a_i)_{i\in I}$ od elements of field $F$, almost all of which are zero and such that: $$\overrightarrow{v} = \sum_{i\in I} {a_i\overrightarrow{v_i}}$$
    \end{itemize}
\end{theorem}

\subsection{Dimension of a vector space}
\begin{theorem}[Fundamental Estimate of Linear Algebra]
    No linearly independent subset of a given vector space has more elements then a generating set. Thus if $V$ is a vector space, $L \subset V$ a linearly independent subset and $E \subseteq V$ a generating set, then $|L| \leq |E|$
\end{theorem}

\begin{theorem}[Steinitz Exchange Theorem]
    Let $V$ be a vector space, $L \subset V$ a finite linearly independent subset and $E \subseteq V$ a generating set. Then there is an injection $\phi: L \hookrightarrow E$ such that $(E \backslash \phi (L)) \cup L$ is also a generating set for $V$
\end{theorem}

\begin{lemma}[Exchange Lemma]
    Let $V$ be a vector space, $M \subseteq V$ a linearly independent subset, adn $E \subseteq V$ a generating subset, such that $M \subseteq E$. if $\overrightarrow{w} \in V\backslash M$ is a vector not belonging to $M$ such that $M \cup \{\overrightarrow{w}\}$ is linearly independent, then there exists $\overrightarrow{e} \in E\backslash M$ such that $\{E\backslash\{\overrightarrow{e}\}\}\cup \{\overrightarrow{w}\}$ is a generating set for $V$
\end{lemma}

\begin{corollary}[Cardinality of Bases]
    Let $V$ be a finitely generated vector space.
    \begin{itemize}
        \item $V$ has a finite basis
        \item $V$ cannot have an infinite basis
        \item Any two bases of $V$ have the same number of elements
    \end{itemize}
\end{corollary}

\begin{definition}
    The cardinality of one (and by Cardinality of Bases each) basis of a finitely generated vector space $V$ is called the \textbf{dimension} of $V$ and will be denoted by $\dim(V)$. If the vector space is not finitely generated, then we write $\dim(V) = \infty$ and call $V$ infinite dimensional. As usual, we will ignore the difference between infinities.
\end{definition}

\begin{corollary}[Cardinality Criterion for Bases]
    Let $V$ be a finitely generated vector space.
    \begin{itemize}
        \item Each linearly independent subset $L \subset V$ has at most $\dim(V)$ elements, and if $|L|=\dim(V)$ then $L$ is actually a basis
        \item Each generating set $E \subseteq V$ has at least $\dim(V)$ elements, and if $|E| = \dim(V)$ the $E$ is actually a basis.
    \end{itemize}
\end{corollary}

\begin{corollary}[Dimension Estimate for Vector Subspaces]
    A proper vector subspace of a finite dimensional vector space has itself a strictly smaller dimension.
\end{corollary}

\begin{theorem}[The Dimension Theorem]
    Let $V$ be vectpr space containing vector subspaces $U,W \subseteq V$. Then
    $$\dim(U + W) + \dim(U\cap W) = \dim(U) + \dim(W)$$
\end{theorem}

\subsection{Linear Mappings}
\begin{definition}
    Let $V,W$ be a vector spaces over a field $F$. A mapping $f: V \rightarrow W$ is called \textbf{linear} or more percisely $\mathbf{F}\textbf{-linearly}$ or even a \textbf{homomorphism of $F$-vector spaces} if for all $\overrightarrow{v}_1, \overrightarrow{v}_2 \in V$ and $\lambda \in F$ we have \[\begin{split}
        f(\overrightarrow{v}_1 + \overrightarrow{v}_2) &= f(\overrightarrow{v}_1) + f(\overrightarrow{v}_2)\\
        f(\lambda\overrightarrow{v}_1) &= \lambda f(\overrightarrow{v}_1) 
    \end{split}\]
\end{definition}

\begin{itemize}
    \item A bijective linear mapping is called as \textbf{isomorphism} of vector spaces. If there is an isomorphism bwetween two vector spaces we say them \textbf{isomorphic}.
    \item A \textbf{homomorphism} from one vector space to itself is called anendomorphismof our vector space.
    \item An isomorphism of a vector space to itself is called \textbf{anautomorphism} of our vector space.
\end{itemize}

\begin{definition}
    A point that is sent to itself br a mapping is called a \textbf{fixed point} of the mapping.
    Given a mapping $f: X \rightarrow X$, we donote the set of fixed points by $$X^f = \{x \in X: f(x) = x\}$$
\end{definition}

\begin{definition}
    Two vector subspaces $V_1,V_2$ of a vector space $V$ are called \textbf{complementary} of addition defines a bijection $V_1 \times V_2 {\stackrel{\sim}{\rightarrow}} V$
\end{definition}

\begin{theorem}[The Classification of Vector Spaces by their Dimension]
    Let $n\in \mathbb{N}$. Then a vector space over a field $F$ is isomorphic to $F^n$ i.f.f it has dimension $n$
\end{theorem}

\begin{lemma}[Linear Mappings and Bases]
    Let $V,W$ be vector spaces over $F$ and let $B \subset V$ be a basis. Then restriction of a mapping gives a bijection
    \[\begin{split}
        \Hom_F(V,W) &\stackrel{\sim}{\rightarrow} \Maps(B,W)\\
        f &\mapsto f|_B
    \end{split}\]
\end{lemma}

\begin{prop}
    \begin{itemize}
        \item Every injective linar mapping $f:V \hookrightarrow W$ has a \textbf{left inverse}, in other words a linear mapping $g: W \rightarrow V$ such that $g \circ f = \text{id}_V$
        \item Every surjective linear mapping $f:V \twoheadrightarrow W$ has a \textbf{right inverse}, in other words a linear mapping $g: W \rightarrow V$ such that $g \circ f = \text{id}_W$
    \end{itemize}
\end{prop}

\subsection{Rank-Nullity Throrem}
\begin{definition}
    The \textbf{image} of a linear mapping $f: V \rightarrow W$ is the subset $\image(f) = f(V) \subseteq W$. The \textbf{preimage} of the zero vector (\textbf{kernel}) of a linear mapping $f: V \rightarrow W$ is denoted by $$\ker(f) := f^{-1}(0) = \{v \in V: f(v) = 0\}$$
    The kernel is a vector subspace if $V$
\end{definition}

\begin{lemma}
    A linear mapping $f: V \rightarrow W$ is injective if an only if it's kernel is zero.
\end{lemma}

\begin{theorem}[Rank-Nullity Theorem]
    Let $f: V \rightarrow W$ be a linear mapping between vector spaces, then $\dim(V) = \dim(\ker{f}) + \dim(\image(f))$
\end{theorem}

% Chapter 2
\section{Linear Mappings and Matrices}
\subsection{Linear Mappings $F^m \rightarrow F^n$ and Matrices}
\begin{theorem}[Linear mappings $F^m \rightarrow F^n$ and Matrices]
    Let $F$ be a field and let $m,n in \mathbb{N}$ be neutral numbers. There is a nijection between the space of linear mappings $F^m \rightarrow F^n$ and the set of matrices woth $n$ rows anf $m$ columns and entries in $F$:
    \[\begin{split}
        \textbf{M}: \Hom_F(F^m,F^n) &\stackrel{\sim}{\rightarrow} \Mat(n \times m; F)\\
        f &\mapsto [f]
    \end{split}\]
    This attaches to each linear mapping$f$ its \textbf{representing matrix} $\textbf{M}(f):=[f]$. The column of this matrux are the images under $f$ of the standard basis elements if $F^m$:
    $$[f] = (f(\overrightarrow{e}_1)|f(\overrightarrow{e}_2)|\dots|f(\overrightarrow{e}_m))$$
\end{theorem}

\begin{definition}
    Let $n,m,l\in \mathbb{N}$, $F$ a field and let $A \in \Mat(n\times m; F)$ and $B \in \Mat(m\times l; F)$ be matrices. The \textbf{product} $A \circ B = AB \in \Mat(n\times l; F)$ is the matrix defined by $$(AB)_{ik} = \sum_{j=1}^m{A_{ij}B_{jk}}$$
\end{definition}

\begin{theorem}[Composition of Linear Mapping and Products of Matrices]
    Let $g:F^l \rightarrow F^m$ and $f:F^m \rightarrow F^n be linear mappings$. Then $[f \circ g] = [f] \circ [g]$
\end{theorem}

\subsection{Basic Properties of Matrices}
\begin{definition}
    A matrix $A$ is called \textbf{invertable} if there exist matrices such tht $BA = I$ and $AC = I$
\end{definition}

\begin{definition}
    will define an \textbf{elementary matrix} to be any square matrix that differs from the identity matrix in at most one entry.
\end{definition}

\begin{theorem}
    Every square matrix with entries in a field can be written as a product if elementary matrices.
\end{theorem}

\begin{definition}
    Any matrix whose only non-zero entries lies on the diagonal, and which has first 1's along the diagonal and then 0's, is said to be in \textbf{Smith Normal Form}:
    \[A_{ij} = \begin{cases}\begin{split}
        0 &\text{ if } i \neq j\\
        1 &\text{ if } A_{(i+1)(j+1)} = 1\\
        0 &\text{ otherwise}
    \end{split}\end{cases}\]
\end{definition}

\begin{theorem}[(Transformation of a Matrix into Smith Normal Form]
    For each matrix $A \in \Mat(n\times m;F)$ there exist invertable matrices $P,Q$ such that $PAQ$ is a matrix in Smith Normal Form.
\end{theorem}

\begin{definition}
    The \textbf{column rank} of a matrix $A \in \Mat(n\times m;F)$ is the dimension of the subsequence of $F^n$ generated by the columns of $A$. Simmilarly, the \textbf{row rank} of $A$ is the dimension of the subspace of $F^m$ generated by the rows of A.
\end{definition}

\begin{theorem}
    The column rank and the row rank of any matrix are equal.
\end{theorem}

Let's now refer the column and row rank as \textbf{rank} for the sake of not losing any generality.

\begin{definition}
    When the rank is as big as possible, meaning that it's equal to either the number of rows or number of columns (whichever is smaller), then the matrix has \textbf{full rank}
\end{definition}


\subsection{Abstract Linear Mappings and Matrices}
\begin{theorem}[Abstract  Linear  Mappings  and  Matrices]
    Let $F$ be a field, $V,W$ vector spaces over $F$ with ordered bases $\mathcal{A} = (\vect{v}_1,...,\vect{v}_m)$ and $\mathcal{B} = (\vect{w}_1,...,\vect{w}_n)$. Then to each linear mapping $f:V\rightarrow W$ we assosiate bases a \textbf{representing matrix} $_\mathcal{B}[f]_\mathcal{A}$ whose entried $a_ij$ are defined by the identity $$f(\vect{v}_j) = \sum_{i=1}^n{a_{ij}\vect{w}_i}\in W$$
    This produces a bijection, which is event an isomorphismof vector spaces:
    \[\begin{split}
        \textbf{M}_\mathcal{B}^\mathcal{A}: \Hom_F(V,W) &\stackrel{\sim}{\rightarrow} \Mat(n \times m; F)\\
        f &\mapsto _\mathcal{B}[f]_\mathcal{A} 
    \end{split}\]
\end{theorem}
We call $\textbf{M}_\mathcal{B}^\mathcal{A}(f) = _\mathcal{B}[f]_\mathcal{A}$ the \textbf{epresenting matrix of the mappingfwith respect to the bases $\mathcal{A}$ and $\mathcal{B}$}

\begin{theorem}[The Representing Matrix of a Composition of Linear Mappings]
    Let $F$ be a field and $U,V,W$ finite dimensional vector spaces over $F$ with ordered bases $\mathcal{A,B,C}$. If $f:U\rightarrow V$ and $g:V \rightarrow W$ are linear mappings, then the representing matrixof the composition $g \circ f: U \rightarrow W$ is the matrix product of the representing matrix of $f$ and $g$: $$_\mathcal{C}[g\circ f]_\mathcal{A} = _\mathcal{C}[g]_\mathcal{B} \circ _\mathcal{B}[f]_\mathcal{A}$$
\end{theorem}

\begin{definition}
    Let $V$ be a finite dimensional vector space with an ordered basis $\mathcal{A} = (\vect{v}_1,...,\vect{v}_m)$. We will denote the invese to the bijection of $\Phi_\mathcal{A} \stackrel{\sim}{\rightarrow} V,(\alpha_a,...,\alpha_m)^T \mapsto \sum_{i=1}^m{\alpha_i\vect{v}_i}$ by
    $$\vect{v} \mapsto _\mathcal{A}[\vect{v}]$$
\end{definition}

\begin{theorem}[Representation of the Image of a Vector]
    Let $V,W$ be finite dimensional vector spaces over $F$ with ordered bases $\mathcal{A,B}$ and let $f:V\rightarrow W$ be a linear mapping. The following holds for $\vect{v} \in V$:
    $$_\mathcal{B}[f(\vect{v})] = _\mathcal{B}[f]_\mathcal{A} \circ _\mathcal{A}[\vect{v}]$$
\end{theorem}

\subsection{Change of Matrix by Change of Basis}
\begin{definition}
    Let $\mathcal{A} = (\vect{v}_1,...,\vect{v}_n), \mathcal{B} = (\vect{w}_1,...,\vect{w}_n)$ be ordered bses of the same $F$-vector space $V$. Then the matrix representing the identity mapping with respect to the bases $_\mathcal{B}[\text{id}_V]_\mathcal{A}$ is called a \textbf{change of basis matrix}. By definition, its entries are given by the equalities $\vect{v}_j = \sum_{i=1}^n{a_{ij}\vect{w}_i}$
\end{definition}

\begin{theorem}[Change of Basis]
    Let $V,W$ be finite dimensional vector spaces over $F$ and let $f:V \rightarrow W$ br a linear mapping. Suppose that $\mathcal{A,A'}$ are order bases of $V$ and $\mathcal{B,B'}$ are ordered bases of $W$. Then:
    $$_\mathcal{B'}[f]_\mathcal{A'} = _\mathcal{B'}[\text{id}_W]_\mathcal{B} \circ _\mathcal{B}[f]_\mathcal{A} \circ _\mathcal{B}[\text{id}_V]_\mathcal{A'}$$
\end{theorem}

\begin{corollary}
    Let $V$ be a finite dimensional vector space and let $f:V\rightarrow V$ be an endomorphism of $V$. Suppose that $\mathcal{A,A'}$ are ordered bases od $V$. Then 
    $$_\mathcal{A'}[f]_\mathcal{A'} = _\mathcal{A}[\text{id}_V]_\mathcal{A'}^{-1} \circ _\mathcal{A}[f]_\mathcal{A} \circ _\mathcal{A}[\text{id}_V]_\mathcal{A'}$$
\end{corollary}

\begin{theorem}[Smith Normal Form]
    Let $f:V\rightarrow W$ be a linear mapping between finite dimensional $F$-vector spaces. There exist an order basis $\mathcal{A}$ of $V$ and an ordered basis $\mathcal{B}$ of $W$, such that the representing matrix $_\mathcal{B}[f]_\mathcal{A}$ has zero entries everywhere except possibly on one diagonal, and along the diagonaltherer are 1's first, followed by 0's  
\end{theorem}

\begin{definition}[Trace]
    The trace of a square matrix is defined to be the sum of its diagonal entries:
    $$\text{tr}(A) = \sum_{i=1}^{n}{a_{ii}}$$
\end{definition}

% Chapter 3
\section{Rings and Modules}
\subsection{Rings}
\begin{definition}
    A \textbf{ring} is a set with two operations $(R,+,\cdot)$ that satisfy:
    \begin{enumerate}
        \item $(R,+)$ is an abelian group
        \item $(R,\cdot)$ is a \textbf{monoid}, meaning that the second operation $\cdot$: $R\times R\rightarrow R$ is assosiative and that there is an \textbf{identity element} $1 = 1_R \in R$, often called just the \textbf{identity}, with the peoperty that $1\cdot a = a \cdot 1 = a \quad\forall a \in R$
        \item The Distributive laws hold, meaning that for all $a,b,c \in R$ \[\begin{split}
            a\cdot (b+c) &= (a\cdot b)+(a \cdot c)\quad \textbf{addition}\\
            (a+b) \cdot c &= (a \cdot c) + (b \cdot c) \quad \textbf{multiplication}
        \end{split}\]
    A ring which element is commutative, that means $a \cdot b = b \cdot a \quad \forall a,b \in R$, is a \textbf{commutative ring}
    \end{enumerate}
\end{definition}

\begin{prop}[Divisbility by Sum]
    A natural number is divisible by 3(respectively by 9) percisely when the sum of its digits is divisible by 3 (or 9)
\end{prop}

\begin{definition}
    A \textbf{field} is a non-zero commutative ring $F$ in which every non-zero element $a\in F$ has an inverse $a^{-1} \in F$, that is an element $a^{-1}$ with the property that $a\cdot a^{-1} = a^{-1}\cdot a = 1$
\end{definition}

\begin{prop}
    Let $m$ be a positive integer. The commutative ring $\mathbb{Z}/m\mathbb{Z}$ is a field i.f.f. $m$ is prime.
\end{prop}

\subsection{Peoperties of Rings}
\begin{lemma}[Mutiplying by zero and negatives]
    Let $R$ be a ring and let $a,b \in R$. Then:
    \begin{enumerate}
        \item $0a = 0 = a0$
        \item $(-a)b = -(ab) = a(-b)$
        \item $(-a)(-b) = ab$
    \end{enumerate}
\end{lemma}

\begin{lemma}[Rules of multiples]
    Let $R$ be a ring, let $a,b\in R$ and $m.n \in \mathbb{Z}$. Then:
    \begin{enumerate}
        \item $m(a+b) = ma + mb$
        \item $(m+n)a = ma + na$
        \item $m(na) = (mn)a$
        \item $m(ab) = (ma)b = a(mb)$
        \item $(ma)(nb) = (mn)(ab)$
    \end{enumerate}
\end{lemma}

\begin{definition}
    Let $R$ be a ring. An element $a \in R$ is called a \textbf{unit} if it is \textbf{invertable} in $R$ or in other words \textbf{has a multiplicative inverse in} $R$, meaning that $\exists a^{-1} \in R$ such that $$aa^{-1} = 1 = a^{-1}a$$
\end{definition}

\begin{prop}
    The set $R^{\times}$ of units in a ring $R$ forms a group under multiplication.
\end{prop}

% 3.2.10 done, Iain started 3.2 today [FRI 7 Feb]

\end{document}