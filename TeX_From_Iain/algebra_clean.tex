\documentclass[11pt]{amsbook}
\usepackage{graphicx} 

\usepackage{palatino, amsfonts, amscd, amssymb, hyperref}
\usepackage{fullpage}
\usepackage{exercise}
\usepackage{graphicx}
\usepackage{lgreek}


\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{decorations.pathreplacing,calc}
\usetikzlibrary{decorations.pathmorphing}
\tikzset{help lines/.style=very thin}

\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}
\usepackage[latin1]{inputenc}

\usetikzlibrary{fit}

\usepackage[greek,english]{babel}

\usetikzlibrary{matrix,arrows}

\renewcommand{\thesection}{\thechapter.\arabic{section}}


\setcounter{tocdepth}{1}

\newcommand\hlight[1]{\tikz[overlay, remember picture,baseline=-\the\dimexpr\fontdimen22\textfont2\relax]\node[rectangle,fill=red!50,rounded corners,fill opacity = 0.2,draw,thick,text opacity =1] {$#1$};}

\newcommand{\hc}{H_{\mathbf{c}}}
\newcommand{\xc}{X_{\mathbf{c}}}
\newcommand{\zc}{Z_{\mathbf{c}}}
\newcommand{\yc}{Y_{\mathbf{c}}}
\newcommand{\irr}[1]{\textsf{Irrep}(#1)}
\newcommand{\triv}{\textsf{triv}}
\newcommand{\sign}{\textsf{sign}}

\newcommand{\psiol}{{\overline \psi}}
\newcommand{\Rol}{{\overline R}}


\def\CC{{\mathbb C}}

\def\PP{{\mathbb P}}
\def\RR{{\mathbb R}}

\def\fm{{\mathfrak{m}}}
\def\fp{{\mathfrak{p}}}
\def\fq{{\mathfrak{ q}}}
\def\fh{{\mathfrak{h}}}


\DeclareMathOperator{\id}{\mathrm{id}}
\DeclareMathOperator{\im}{\mathrm{im}}

\usepackage[mathscr,mathcal]{eucal}

\usepackage{colortbl}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{enumerate}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{ypoth}[theorem]{Assumptions}
\newtheorem{ypothesh}[theorem]{Assumption}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{thedef}[theorem]{Theorem-Definition}
\newcommand{\remarks}{\it Remarks:\/}
\newcommand{\remark}{\it Remark:\/}
\newtheorem{px}[theorem]{Example}
\newcommand{\apod}[1]{\textbf{Proof:}\qquad#1\hspace{\stretch{1}}$\blacksquare$\\}
\newcommand{\apodl}[1]{Proof:\qquad#1\hspace{\stretch{1}}$\square$\\}
\theoremstyle{definition}
\newtheorem{rem}[theorem]{Remark}
\newtheorem{limma}{Lemma}
\newtheorem{orismos}{Definition}
\newtheorem{ex}[theorem]{Example}
\newtheorem{exas}[theorem]{Examples}
\newtheorem*{etym}{Etymology}
\newtheorem{exercise}{Exercise}

\hypersetup{ colorlinks=true, linkcolor=red, urlcolor=red }

\begin{document}

\title{HONOURS ALGEBRA 2019-20}
\author{Notes by Iain Gordon \\[1ex]
used and developed by Iain Gordon, Richard Gratwick, Milena Hering, Gergo Nemes and Andrew Ranicki}


\maketitle

\frontmatter
\tableofcontents

\chapter*{Preface}
These are the notes for the Y3 Honours Algebra course. I recommend these ahead of any book, although in due course you may find some books mentioned on the LEARN page for this course. The notes are often quite discursive and they will contain all the content and more of what we cover in during lectures. They also overlap with Workshops. It is these notes that form the basis of the examinable material for the course.

\chapter*{Acknowledgments} I have been combining already existing lecture notes from a number of sources to produce: Professors Brown, Gillespie, Mason, Smith, Smyth, Soergel, Webber, Whitelaw, Wemyss. I found the point of view of Wolfgang Soergel particularly to the point and satisfying, and so followed his lead quite often.

\mainmatter


\chapter{Vector Spaces}
\label{ch:vectorspaces}
In this chapter I will discuss ``real space" and simultaneous linear equations, and how the theory of abstract vector spaces provides a bridge between the two.
\section{Solutions of Simultaneous Linear Equations}
\label{slas}

Let $F$ be a field. I will discuss precisely what this means later; in what we are going to do for the moment you can just as well think of the field $F = \mathbb{Q}$ of rational numbers, or the field $F=\mathbb{R}$ of real numbers, or the field $F = \mathbb{C}$ of complex numbers.

Suppose we have been given $n$ equations in $m$ unknowns:
\begin{eqnarray*}
      a_{11} x_1 + a_{12}x_2 +  \cdots  + a_{1m}x_m & =  & b_1 \\
      a_{21} x_1 + a_{22}x_2 +  \cdots  + a_{2m}x_m & = & b_2 \\
      \vdots  \hspace{13mm} \vdots \hspace{23mm} \vdots \hspace{5mm} &&  \, \vdots \\
      a_{n1} x_1 + a_{n2}x_2 + \cdots  + a_{nm}x_m & =  & b_n
\end{eqnarray*}
I intend that $a_{ij}, b_i \in F$ are fixed, and that we are supposed to solve the equations by finding the $x_j$'s. You, as a mathematically mature reader, know the general convention in Mathematics that we use letters from the beginning of the alphabet for ``known indeterminates" and letters from the end of the alphabet for ``unknown indeterminates"; don't get confused with the convention in \href{http://en.wikipedia.org/wiki/There_are_known_knowns}{Politics}.

We call this a {\bf system of linear equations}. Linear means that there are no complicated terms in the unknowns such as $x_1^2$ or $x_1x_2^7$. If all the $b_i$ are zero in the right hand side of our equations, then we call our system {\bf homogeneous}. We can construct a new system of linear equations from the old one by setting all the $b_i$ to zero: this is called the associated {\bf homogenised} system of equations.

What I want us to do is give a description of all $m$-tuples $(x_1, \ldots , x_m)$ of elements of $F$ such each of the $n$ equations is satisfied. A better way to express this is for me to ask you to give the most explicit description you can of the subset $L \subseteq F^m$ consisting of all $m$-tuples that satisfy the $n$ equations. The set $L$ is called {\bf solution set} of our system of equations.

\begin{exas} \label{exsla} Here are some examples of such systems.
\begin{enumerate}
\item Here is a system of linear equations with three equations and three unknowns:
\begin{eqnarray*}
x_1 + 3x_2 \hspace{8.5mm} & = & 1\\
2x_1 + 2x_2 + x_3 & = & 2 \\
4x_1 + 6x_2 + x_3 & = & 8
\end{eqnarray*}
\item Here is a homogeneous system of linear equations with two equations and three unknowns:
\begin{eqnarray*}
2y - 17z & = & 0 \\
4x + 22y +z & = & 0
\end{eqnarray*}
I've written $x,y,z$ instead of $x_1, x_2, x_3$: it's more sensible to use a notation that needs as few indices as possible.
\item Here is an inhomogeneous system of linear equations with one equation and one unknown:
$$ 0x = 1 $$  It has an empty solution set.
\end{enumerate}
\end{exas}

It saves a lot of writing to omit the symbols $x_i$ as well as the plus signs and the equality signs from the system of linear equations, and instead describe it in shorthand by its {\bf extended coefficient matrix}
$$
\left(
\begin{array}{cccc|c}
a_{11} & a_{12} & \ldots & a_{1m} & b_1 \\
a_ {21} & a_{22} & \ldots & a_{2m} & b_2 \\
\vdots & & & \vdots & \\
a_{n1} & a_{n2} & \ldots & a_{nm} & b_n \\
\end{array}
\right)
$$
The specification ``extended" refers to the last column containing the $b_i$. The family of $a_{ij}$ alone is called the {\bf coefficient matrix} of our system of equations. For instance, \ref{exsla}(1) is written as $$\left( \begin{array}{ccc|c} 1&3&0&1 \\ 2&2&1&2 \\ 4&6&1&8\end{array} \right)$$

\begin{rem} When you look at our a system of linear equations, don't read $a_{12}$ as $a$--twelve, rather as $a$--one--two. It would of course be more precise to separate the two indices by a comma, writing $a_{1,2}$ and so on, but this makes the system of equations harder to read. When writing Mathematics there is often a choice between a precise hard-to-read statement and an easy-to-read imprecise statement: it seems better to me to be legible. In Physics, sadly, it is common to put indices as superscripts, writing $a^2_1$ and so on, but that can also cause confusion with squares $(a_1)^2$.
\end{rem}

\phantomsection
\label{opGE}
To calculate the solution set to a system of linear equations we can apply {\bf Gaussian elimination}.  This is based on the elementary observation that the solution set does not change if we apply either of the following operations to produce a new system of linear equations:
\smallskip

\begin{tabular}{l}
{[}{\it Row Addition}] Replace an equation by its sum with a multiple of another of our equations;\\
{[}{\it Row Swap}] Swap two of our equations.\\
\end{tabular}

\vspace{2mm}
\noindent
By applying these operators, Gaussian elimination transforms a system of linear equations to {\bf echelon form} without changing the solution set. To describe this formally, we say that a system of linear equations is ``in echelon form" precisely when we can find $r\geqslant 0$ and indices $1 \leqslant s(1) < s(2) < \ldots < s(r) \leqslant m$ so that in our system of equations we have $a_{i,s(i)} \neq 0$ for $1\leqslant i \leqslant r$ and that $a_{\nu \mu} \neq 0$ occurs only when there is an $i$ with $\nu \leqslant i$ and $\mu \geqslant s(i)$.

In an example, this looks so:
$$
\begin{tikzpicture}
\matrix (echelon) [matrix of math nodes]
{
0 & 0 & \neq 0 & * & * & * & * & * & * & * & {} \\
0 & 0 & 0 & 0 & \neq 0 & * & * & * & * & * & \\
0 & 0 & 0 & 0 & 0 & \neq 0 & * & * & *  & * & \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & \neq 0 & *& * & {} \\
0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & * &  \\
0 & 0 & 0  & 0 &0 & 0& 0& 0 &0  & * & \\
& & s(1) & & s(2) & s(3) & & s(4) & & & \\
 };
 \draw[very thick,red] (echelon-1-1.north west) -- (echelon-1-2.north east) -- (echelon-2-2.north east) -- (echelon-2-4.north east) --  (echelon-3-4.north east) -- (echelon-3-5.north east) --  (echelon-4-5.north east) -- (echelon-4-7.north east) -- (echelon-5-7.north east) -- (echelon-5-9.north east);
 \draw[thick,black] (echelon-1-9.north east) -- (echelon-6-9.south east);
 \draw[very thick] (-3.4,2.1) .. controls (-3.6,1.9) and (-3.6, -1.3) .. (-3.4,-1.5);
 \draw[very thick] (3.2,2.1) .. controls (3.4,1.9) and (3.4, -1.3) .. (3.2,-1.5);
  \draw [decoration={brace,amplitude=0.5em},decorate,very thick,black]
        (echelon-1-11.north -| echelon.east) -- node[black, right=0.7em] {$r(=4)$} (echelon-4-11.south -| echelon.east);
  \end{tikzpicture} $$

\begin{etym}``Echelon" is French for ``step": here it is the entries that are zero in the coefficient matrix that make ``steps of height one but with variable breadth". The symbol $*$ that we see throughout the top-right indicates that it is irrelevant what these entries are.
\end{etym}

\noindent
{Gaussian ELIMINATION}
\label{gaussalg} solves a system of linear equations functions as follows:
\smallskip

$\bullet$ If all the coefficients in the first column are zero, we ignore this column and continue with the algorithm on the remainder of the coefficient matrix. If there is a coefficient in the first column that is not zero, we bring it to the first row by applying a [{\it Row Swap}] with the first row. Then apply [{\it Row Addition}] to add suitable multiples of the first row to the other rows so that we arrive at a system in which the entries below the top of the first column are zero. We then continue the algorithm by ignoring the first row and repeating the above process.
Obviously we can use this to bring any system of linear equations into echelon form, without changing its solution set.
\smallskip

$\bullet$ Having carried out the above procedure, the solution set of a linear system of equations in echelon form is quick to calculate: If any of the numbers $b_{r+1} , \ldots , b_n$ are not zero there are no solutions at all. Assuming that $b_{r+1} = \cdots = b_n = 0$, we then can choose arbitrary values for $x_{\mu}$ with $\mu > s(r)$ and then calculate the unique value for $x_{s(r)}$ in terms of $b_r$ and the $x_{\mu}$ with $\mu > s(r)$ by using the equation in the $r$-th row, then $x_{s(r-1)}$ in terms of $b_{r-1}$ and $x_{\mu}$ with $\mu > s(r-1)$ by using the equation in the $r-1$-st row, and so on. This provides us with the general $m$-tuple $(x_1, \ldots , x_m)$ that is a solution of the system of equations.
\medskip

\begin{ex} A linear system of equations with three equations and three unknowns and the derivation of its solution with Gaussian elimination.
\begin{eqnarray*}
x_1 + 2x_2 \hspace{0.85cm} &=& -1 \\
2x_1 + 7 x_2 + x_3 &=& -2 \\
5x_1 + 8x_2 + x_3 &=& -3
\end{eqnarray*}
$$
\begin{tikzpicture}
  \draw [->,decorate,
      decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}]
    (0,0) -- (0,-1)
node[left, align=right,midway] {\hspace{2.5cm}}
    node [right,align=left, midway]
    { transform to \\ coefficient matrix
         };
\end{tikzpicture}
$$
$$\left(
\begin{array}{ccc|c}
1 & 2 & 0 & -1 \\
2 & 7 & 1 & -2 \\
5 & 8 & 1 & -3
\end{array}
\right)
$$
$$
\begin{tikzpicture}
  \draw [->,decorate,
      decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}]
    (0,0) -- (0,-1)
node[left, align=right,midway] {\hspace{2.4cm}}
    node [right,align=left,midway]
    { $R2 \mapsto R2 -  2R1$ \\ $R3 \mapsto R3 - 5R1$
         };
\end{tikzpicture}
$$
$$\left(
\begin{array}{ccc|c}
1 & 2 & 0 & -1 \\ 0 & 3 & 1 & 0 \\ 0 & -2 & 1 & 2
\end{array}
\right)
$$
$$
\begin{tikzpicture}
  \draw [->,decorate,
      decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}]
    (0,0) -- (0,-1)
node[left, align=right,midway] {\hspace{2.5cm}}
    node [right,align=left,midway]
    { $R3 \mapsto R3 + \frac{2}{3}R2$
         };
\end{tikzpicture}
$$
$$\left(
\begin{array}{ccc|c}
1 & 2 & 0 & -1 \\ 0 & 3 & 1 & 0 \\ 0 & 0 & \frac{5}{3} & 2
\end{array}
\right)
$$
$$
\begin{tikzpicture}
  \draw [->,decorate,
      decoration={snake,amplitude=.4mm,segment length=2mm,post length=1mm}]
    (0,0) -- (0,-1)
node[left, align=right,midway] {\hspace{2.8cm}}
    node [right,align=left,midway]
    { solve for \\ $x_3$, then $x_2$, then $x_1$
         };
\end{tikzpicture}
$$
$$ \begin{cases}
x_3 = \frac{6}{5}, \\
x_2 = -\frac{2}{5}, \\
x_1 = -\frac{1}{5}.
\end{cases}
$$
Often in the examples we study it won't be necessary to swap rows. If there are exactly as many equations as unknowns we usually expect exactly one solution, just as above.
\end{ex}
\bigskip

A mapping from the product of sets $\{1 , \ldots , n\} \times \{ 1, \ldots , m\}$ to a set $Z$ is called an $(n\times m)$-{\bf matrix with coefficients in $Z$}. Given such a matrix $A$ we will write $A_{ij}$ or $a_{ij}$ instead of $A(i,j)$ and visualise this data as a rectangular arrangement of elements from $Z$, just as in the case when $Z$ is the field $F$. We will call the $i$ the {\bf row index} since it indicates in which row our entry $a_{ij}$ stands, and the $j$ will be called the {\bf column index} of our matrix entry. The set of all $(n\times m)$-matrices with coefficients in $Z$ will be denoted \begin{equation}\label{matrixdef} {\sf Mat}(n\times m; Z) := {\rm Maps}(\{1, \ldots , n\}\times \{ 1, \ldots , m\} , Z).\end{equation}  In the case when $n=m$ we will speak of a {\bf square matrix} and shorten our notation from ${\sf Mat}(n\times n;Z)$ to ${\sf Mat}(n;Z)$.

\begin{etym} The terminology ``Matrix" was introduced by the English mathematician Arthur Cayley in an article ``Remarques sur la notation des functions alg\'ebriques" in Crelle's Journal in 1855, Volume 50, page 282. The terminology seems to originate from the Latin word ``mater" for the English ``mother": at least Cayley writes on page 284 from loc. cit. : ``Il y aurait bien des choses \`a dire sur cette th\'eorie des matrices, laquelle doit, il me semble, pr\'ec\'eder la th\'eorie des D\'eterminants". On page 313 he introduces the so-called ``minors", so one supposes that he wanted the role of the matrix to the fore, with the image of the ``mother of the determinant and her general minors". We'll discuss the determinant later, but probably not the minors.
\end{etym}

\begin{theorem}[Solution sets of inhomogeneous systems of linear equations] If the solution set of a linear system of equations is non-empty, then we obtain all solutions by adding componentwise an arbitrary solution of the associated homogenised system to a fixed solution of the system.
\end{theorem}
\begin{proof}If $c = (c_1, \ldots , c_m)$ is a solution of our system of linear equations and $h = (h_1, \ldots , h_m)$ is a solution of the homogenised system, then it is obvious that the componentwise sum $c \dotplus h = (c_1+ h_1, \ldots , c_m+h_m)$ is a solution of the original system. On the other hand if $c' = (c_1', \ldots , c_m')$ is another solution of our system of linear equations then it is clear that the componentwise difference $h = (c_1' - c_1, \ldots , c_m'-c_m)$ is a solution of the homogenised system which furthermore satisfies $c' = c \dotplus h$.
\end{proof}

\noindent
HERE IS WHERE THE THINKING STARTS!
\medskip
\label{theshotthatshooktheworld}

These considerations show how to determine the solution set of any system of linear equations. If the solution set is non-empty, we use \hyperref[gaussalg]{Gaussian elimination} to bring the system in to echelon form. This then produces a bijection between $t$-tuples of elements of $F$ and particular solutions, where $t = m -r$ is the number of variables less the ``number of stairs", corresponding to values of $x_j$ where $j$ is different from the ``column indices of stairs" $j\neq s(1), \ldots , s(r)$.
\medskip

Now observe that when we run the Gaussian algorithm we get to choose which [{\it Row Swaps}] we make as it runs. This leads us immediately to ask: do we always arrive at the same echelon form for our matrix, independent of the choice of row swaps? The answer is ``no", but nonetheless the ``widths of the individual stairs", that is the column indices of the stairs $s(1), \ldots , s(r)$ are independent of all choices. In fact these can be directly described if we consider the associated homogenised system of equations and run through the variables from last to first. We ask if for each $(x_{j+1}, \ldots , x_m)$ which can be extended to a solution $(x_1, x_2, \ldots , x_m)$, there exists a unique $x_j$ such that $(x_j, x_{j+1}, x_{j+2}, \ldots , x_m)$ extends to a solution $(x_1, x_2, \ldots , x_m)$? The answer is ``yes" precisely when a new step begins at the $j$-th column.

It is also obvious that if we relabel the variables in our system of linear equations, nothing should really change. Another way to think about this is that if we swap the columns of our coefficient matrix, nothing should really change in the solution. But if we now run Gaussian elimination we will get, just as above,  a bijection between $u$-tuples of elements of $F$ and the solution set. The question then staring us in the face is: is it true that $u=t$? In other words, do we get an echelon form with the same number of stairs if we arbitrarily swap the columns of our matrix before we run Gaussian elimination?

The answer is of course ``yes", but I don't know an elementary argument. In fact, I'm really quite happy that I don't! We can take this question as a motivation to develop the abstract theory of vector spaces, and of course, that is what we are about to do. In doing this, we will introduce the notion of ``dimension" of a ``vector space", and show that the number of stairs is independent of all choices because it can be interpreted in terms of the ``dimension of the solution set" of the associated homogenised system of equations.

\section{Fields and Vector Spaces}
\begin{definition} \label{vsdef}
{\rm (i)} A {\bf field} $F$  is a set with functions
$$\begin{array}{l}
\text{addition}~=~+:~ F \times F \to F~;~(\lambda,\mu) \mapsto \lambda+\mu\\[1ex]
\text{multiplication}~=~.~:~F \times F \to F~;~~(\lambda,\mu) \mapsto \lambda\mu
\end{array}$$
such that $(F,+)$ and $(F \backslash \{0\},.)$ are abelian groups, with
$$\lambda(\mu+\nu)~=~\lambda \mu +\lambda\nu \in F$$
for any $\lambda,\mu,\nu \in F$. The neutral elements are called $0_F$,
$1_F$. In particular, for all $\lambda,\mu \in F$
$$\lambda + \mu ~=~\mu+\lambda~,~\lambda. \mu~=~\mu. \lambda~,~
\lambda+0_F~=~\lambda~,~\lambda.1_F~=~\lambda \in F~.$$
For every $\lambda \in F$ there  exists $-\lambda \in F$ such that
$$\lambda+(-\lambda)~=~0_F \in F~.$$
For every $\lambda\neq 0 \in F$ there  exists $\lambda^{-1}\neq 0 \in F$ such that
$$\lambda(\lambda^{-1})~=~1_F \in F~.$$
{\rm (ii)}   A {\bf vector space $V$ over a field $F$} is a pair consisting of an abelian group $V = (V,\dotplus)$ and a mapping
$$F\times V  \to V \colon  (\lambda , \vec{v})\mapsto \lambda \vec{v}$$
such that for all $\lambda, \mu \in F$ and $\vec{v}, \vec{w} \in V$ the following identities hold:
\begin{eqnarray*}
\lambda (\vec{v}\dotplus \vec{w}) &=& (\lambda \vec{v}) \dotplus (\lambda \vec{w}) \\ (\lambda + \mu) \vec{v} &=& (\lambda \vec{v}) \dotplus (\mu \vec{v}) \\ \lambda( \mu \vec{v}) &=& (\lambda \mu) \vec{v} \\ 1_F \vec{v} &=& \vec{v}
\end{eqnarray*}
The first two laws are the {\bf Distributive Laws}; the third law is called the {\bf Associativity Law}. I will often also call a vector space $V$ over a field $F$ an {\bf $F$-vector space}.
\end{definition}
\medskip

I will typically call the elements of a vector space {\bf vectors} and the elements of the field $F$ {\bf scalars}. I will call the field itself the {\bf ground field}. The mapping $(\lambda, \vec{v}) \mapsto \lambda \vec{v}$ is called {\bf multiplication by scalars} or alternatively the {\bf action of the field $F$ on $V$}. You absolutely must not confuse this with the ``scalar product" that you have met in other courses: that produces a scalar from two vectors. For didactic reasons, I have written the addition of vectors by $\dotplus$. This distinguishes it from the addition of elements of the field. But, since it needs extra typing, I will not do it for much longer. Typically I'll use the usual convention of ``order of operations", the simpler notation for addition of vectors $+$, and the shorthand $1_F = 1$, in order to get a formulation that seems much easier-on-the-eye: for all scalars $\lambda, \mu$ and all vectors $\vec{v}, \vec{w}$ the following hold:
\begin{eqnarray*}
\lambda (\vec{v}\dotplus \vec{w}) &=& \lambda \vec{v} \dotplus\lambda \vec{w} \\ (\lambda + \mu) \vec{v} &=& \lambda \vec{v} \dotplus\mu \vec{v} \\ \lambda( \mu \vec{v}) &=& (\lambda \mu) \vec{v} \\ 1 \vec{v} &=& \vec{v}
\end{eqnarray*}
For more serious didactic reasons, I've written vectors with a little arrow, but sometimes I lapse; you should certainly remember to imagine the arrow there. I will write $\vec{0}$ for the zero element of the  abelian group $V$ and call it the {\bf zero vector}. Observe that the last law $1 \vec{v} =  \vec{v}$ excludes the possibility that we take $V$ to be some non-zero abelian group and then set $\lambda  \vec{v} = \vec{0}$ for all $\lambda \in F$ and $ \vec{v} \in V$.

\begin{etym} \label{pusher} The terminology ``vector" comes from the Latin ``vehere" which means ``to transport" or ``to carry". This comes from thinking of a vector in the plane as transporting all points in the plane in the direction and length of the vector. So in English an intuitive translation would be ``pusher". But since I'd then be giving a course on ``Pusher Algebra", I'm much happier with the traditional terminology and I thank the Romans, once again. The use of the word ``scalar" for the elements of the ground field comes from the Latin word ``scala" for ``ladder". From here the meaning developed into the name for a tape measure and then into what one can read off of a tape measure, and hence the real numbers. In Mathematics and Physics we don't use only real vector spaces, and so we adapt this word for general use as the terminology for an element of ground field.
\end{etym}
\medskip

We now need to make a few deductions to check vector spaces' hygiene.

\begin{lemma}[Product with the scalar zero] \label{zerovs} If $V$ is a vector space and $\vec{v} \in V$, then $0\vec{v} = \vec{0}$.
\end{lemma}
In words ``zero times a vector is the zero vector".
\begin{proof}To prove this, we use the second distributive law $$0 \vec{v} = (0 + 0)\vec{v} = 0\vec{v} + 0\vec{v}$$ and then subtract $0\vec{v}$ from both sides to leave $\vec{0} = 0\vec{v}$.
\end{proof}

\begin{lemma}[Product with the scalar $(-1)$]  \label{-1vs} If $V$ is a vector space and $\vec{v}\in V$, then $(-1) \vec{v} = -\vec{v}.$
\end{lemma}
\begin{proof} To prove this we find with the last and the second laws from the definition of a vector space that $$\vec{v} \dotplus (-1)\vec{v} = 1\vec{v} \dotplus (-1)\vec{v} = (1 + (-1))\vec{v} = 0 \vec{v} = \vec{0}$$ and thus $(-1)\vec{v}$ is the additive inverse of $\vec{v}$.
\end{proof}

\begin{lemma}[Product with the zero vector] \label{prodzero} If $V$ is a vector space over a field $F$, then  $\lambda \vec{0} = \vec{0}$ for all $\lambda \in F$. Furthermore, if $\lambda \vec{v} = \vec{0}$ then either $\lambda = 0$ or $\vec{v} = \vec{0}.$
\end{lemma}
\begin{proof}
Exercise.
\end{proof}

\begin{ex} \label{zerovs1} Given a field $F$, the abelian group $V = F$ is an $F$-vector space with multiplication by scalars given by the usual multiplication in $F$. The one-element abelian group $V = 0$ equipped with the obvious operation is a vector space over every field, and called the {\bf trivial vector space} or the {\bf zero vector space}.
\end{ex}

\begin{exercise} \label{matvs}Given a set $X$ and a vector space $V$ over $F$, show that: the set ${\rm Maps}(X,V)$ of all mappings from $X \to V$ is an $F$-vector space, if we define addition by $(f+g)(x) = f(x) + g(x)$ and multiplication by scalars by $(\lambda f)(x) = \lambda (f(x))$.
\end{exercise}

In particular, it follows from this exercise that the set ${\sf Mat} (n\times m ; F)$ of all $(n\times m)$-matrices with entries from the field $F$ from \eqref{matrixdef} has the structure of an $F$-vector space.



\section{Product of Sets and of Vector Spaces}
\label{cartesian}

So far you will be used to the {\bf cartesian product} $X\times Y$ of two sets $X$ and $Y$. But of course we can also introduce the cartesian product of the sets $X_1, \ldots , X_n$ $$X_1\times \cdots \times X_n := \{ (x_1, \ldots , x_n): x_i \in X_i \text{ for } 1\leqslant i \leqslant n\}.$$
We call the elements of such a product {\bf $n$-tuples}. An individual entry $x_i$ of an $n$-tuple $(x_1, \ldots , x_n)$ is called a {\bf component}.

There are special mappings called {\bf projections} for a cartesian product \begin{eqnarray*} {\rm pr}_i: X_1\times \cdots \times X_n &\to & X_i \\ (x_1, \ldots , x_n) & \mapsto & x_i.\end{eqnarray*}

The cartesian product of $n$ copies of a set $X$ is written in short as $$X^n$$ The elements of $X^n$ are $n$-tuples of elements from $X$. In the special case $n=0$ we use the general convention that $X^0$ is ``the" one element set, so that for all $n,m \geqslant 0$ we then have the canonical bijection 
$$X^n\times X^m \stackrel{\sim}{\to} X^{n+m}~;~((x_1,x_2,\dots,x_n),(
x_{n+1},x_{n+2},\dots,x_{n+m})) \mapsto (x_1,x_2,\dots,x_n,
x_{n+1},x_{n+2},\dots,x_{n+m})~.$$

\begin{ex}[The vector space of $n$-tuples] \label{ntups} A particularly important example is the vector space $$V = F^n$$ over the field $F$. Here I am using the notation from above, so that elements of $V$ are $n$-tuples of elements from the field $F$. The vector space operations are as follows:
$$
\begin{pmatrix}
v_1 \\ \vdots \\ v_n
\end{pmatrix}
\dotplus
\begin{pmatrix}
w_1 \\ \vdots \\ w_n
\end{pmatrix}
:=
\begin{pmatrix}
v_1+w_1 \\ \vdots \\ v_n+w_n
\end{pmatrix}
\qquad \text{and} \qquad
\lambda
\begin{pmatrix}
v_1 \\ \vdots \\ v_n
\end{pmatrix}
 :=
\begin{pmatrix}
\lambda v_1 \\ \vdots \\ \lambda v_n
\end{pmatrix}
$$
for $\lambda, v_1, \ldots , v_n, w_1, \ldots , w_n\in F$.
\end{ex}
I have written the components of our $n$-tuples vertically here because it's easier to read, rather than putting them side-by-side separated by commas. The first of our equations defines the sum of two $n$-tuples, and thus addition in our vector space $V=F^n$, and it does so using the addition in the field $F$. The second equation provides multiplication by scalars. I've continued with the didactic notation $\dotplus$ instead of $+$, but from now on I'm giving up and I will write $+$ instead of $\dotplus$. You need to know that if $\vec{v}\in F^n$, I will write $v_1, v_2, \ldots , v_n$ for its components and since these are not furnished with arrows, they are elements of the ground field $F$. If ever I write $\vec{v}_1, \vec{v}_2, \ldots ,\vec{v}_n$, then these are not the components of an $n$-tuple $\vec{v}$, but rather $n$ vectors from a vector space. Eventually, however, I'm probably going to drop the arrows, and then it is up to you, dear Reader, to decide from the context what is intended.


\begin{ex}[Vector space as {\textcolor{black}{SPACE AROUND US}}] \label{realspace} Do not misunderstand the following waffle as the basis for a philosophical theory of anything! I think of a real vector space mostly as the space all around us -- whatever that is and whatever that means --, together with a predetermined fixed point of origin. More exactly, I think of vectors as arrows attached to the fixed point and ending at some other point. To add two vectors I then \hyperref[pusher]{push} one in a parallel direction until it reaches the endpoint of the first, and then I take the point where it ends to determine a new arrow that ends there. I take the product of a vector with a scalar to mean change the length of the arrow by the amount given by the scalar. The negative of an arrow just an arrow of the same length pointing in the opposite direction. I think of the zero vector as the distinguished fixed point itself.
\end{ex}

$$
\begin{tikzpicture}
\draw [->] (0,0) -- (2,1);
\draw [->] (0,0) --  (6,3);
\draw [->] (0,0) -- (-0.5,2);
\draw [->] (0,0) -- (-2, -1);
\draw [ dotted ] (-0.5,2) -- (1.5, 3);
\draw [dotted] (2, 1) -- (1.5, 3);
\draw [->] (0,0) -- (1.5,3);
\draw (2,0.75) node {$\vec{v}$};
\draw (6,2.75) node {$3\vec{v}$};
\draw (-0.5,2.25) node {$\vec{w}$};
\draw (1.5, 3.25) node {$\vec{v}+\vec{w}$};
\draw (-2,-1.25) node {$-\vec{v}$};
\draw (0,-0.25) node {$\vec{0}$};
\end{tikzpicture}
$$

\begin{exercise} \label{prodvs} Given a field $F$ and $F$-vector spaces $V_1, \ldots , V_n$, show that: the cartesian product $V_1\times \cdots \times V_n$ is an $F$-vector space if we define addition and multiplication by scalars componentwise.

In formulas, these look just like the formulas in \hyperref[ntups]{Example \ref{ntups}} except that wherever we see $v_i$ and $w_i$ there we put arrows, and instead of writing $v_i, w_i \in F$ we write $\vec{v}_i, \vec{w}_i \in V_i$. We have a special notation for this new vector space $$V_1\oplus \cdots \oplus V_n$$ and we name it the {\bf product} or the {\bf direct sum} or most-precisely-of-all the {\bf external direct sum} (because we will discuss the distinct notion of ``internal direct sum of subspaces" in \hyperref[internaldirsum]{Definition \ref{internaldirsum}}). In particular $F^n$ is the external direct sum $F\oplus \cdots \oplus F$ of $n$ copies of the $F$-vector space $F$.
\end{exercise}

\section{Vector Subspaces}
\begin{definition}
A subset $U$ of a vector space $V$ is called a {\bf vector subspace} or {\bf subspace} if $U$ contains the zero vector and whenever $\vec{u}, \vec{v} \in U$ and $\lambda \in F$ we have $\vec{u} + \vec{v} \in U$ and $\lambda \vec{u} \in U$.
\end{definition}

\begin{rem} \label{highersubdef} From a higher standpoint the ``correct" definition of a vector subspace looks a bit different and is as follows: let $F$ be a field. A subset of an $F$-vector space is called a vector subspace if it can be given the structure of an $F$-vector space such that the embedding is a ``homomorphism of $F$-vector spaces". I can't give you this better definition because we haven't learnt about homomorphisms of $F$-vector spaces. So in fact this definition is more complicated! But I think it is better because it also applies to subgroups, subfields, sub-almost-any-other-structure-you-want, as you will learn later.
\end{rem}

\begin{ex}[Solution sets as vector subspaces] \label{solsaresubs}
As we discussed in \hyperref[slas]{the first section of this chapter}, by a homogeneous system of linear equations over a field $F$, we mean a system of equations of the form
\begin{eqnarray*}
      a_{11} x_1 + a_{12}x_2 +  \cdots  + a_{1m}x_m & =  & 0 \\
      a_{21} x_1 + a_{22}x_2 +  \cdots  + a_{2m}x_m & = & 0 \\
      \vdots  \hspace{13mm} \vdots \hspace{23mm} \vdots \hspace{5mm} &&  \, \vdots \\
      a_{n1} x_1 + a_{n2}x_2 + \cdots  + a_{nm}x_m & =  & 0
\end{eqnarray*}
where there are only zeros on the right. The solution set of such a homogeneous system of linear equations is obviously a vector subspace $L \subseteq F^m$.
\end{ex}

\begin{ex}[Vector subspaces of {\textcolor{black}{SPACE AROUND US}}] Do not misunderstand the following waffle as the basis for a philosophical theory of anything! In \hyperref[realspace]{Example \ref{realspace}} of a vector space as the {\textcolor{black}{space around us}} with a predetermined fixed point of origin, a subspace would be one of the following: (1) the one-element subset consisting only of the fixed point of origin, (2) all straight lines that pass through the fixed point of origin, (3) all planes that pass through the fixed point of origin, and (4) the whole of {\textcolor{black}{space around us}}.
\end{ex}

\begin{proposition}[Generating a vector subspace from a subset] \label{generatingsubsp} Let $T$ be a subset of a vector space $V$ over a field $F$. Then amongst all vector subspaces of $V$ that include $T$ there is a smallest vector subspace $$\langle T \rangle = \langle T \rangle_F \subseteq V.$$ It can be described as the set of all vectors $\alpha_1 \vec{v}_1+ \cdots + \alpha_r \vec{v}_r$ with $\alpha_1 , \ldots , \alpha_r \in F$ and $\vec{v}_1, \ldots,
 \vec{v}_r\in T$, together with the zero vector in the case $T = \emptyset$.
\end{proposition}

An expression of the form $\alpha_1 \vec{v}_1+ \cdots + \alpha_r \vec{v}_r$ is called a {\bf linear combination} of vectors $\vec{v}_1, \ldots , \vec{v}_r$. We only allow sums with a finite number of terms. The smallest vector subspace $\langle T \rangle \subseteq V$ containing $T$ is called the {\bf vector subspace generated by $T$} or the vector subspace {\bf spanned by $T$} or even the {\bf span of $T$}. If we allow the zero vector to be the ``empty linear combination of $r=0$ vectors", which is what we will mean from hereon, then the span of $T$ is exactly the set of all linear combinations of vectors from $T$.

\begin{proof} It is clear that all linear combinations of vectors from $T$ form a vector subspace of $V$ which contains $T$. It is just as clear that every vector subspace of $V$ that contains $T$ must contain all linear combinations of vectors from $T$. \end{proof}

Other possible notation for the subspace generated by a subset $T$ includes ${\sf span}(T)$ and ${\sf lin}(T)$.

\begin{ex} We will use the following fact a lot: If $v\in \langle T \rangle$, then $\langle T \cup \{v\}\rangle = \langle T \rangle$. 
\end{ex}

\begin{definition}
A subset of a vector space is called a {\bf generating set} of our vector space if its span is all of the vector space. A vector space that has a finite generating set is said to be {\bf finitely generated}.
\end{definition}

\begin{ex}[Finite generation in {\textcolor{black}{SPACE AROUND US}}]
 Do not misunderstand the following waffle as the basis for a philosophical theory of anything! In \hyperref[realspace]{Example \ref{realspace}} of a vector space as the {\textcolor{black}{space around us}} with a predetermined fixed point of origin: the vector subspace generated by the zero vector consists only of the zero vector and so is just our fixed point of origin; the vector subspace generated by one non-zero vector can be thought of as the infinite straight line that passes through the fixed point of origin and the endpoint of our arrow; the vector subspace generated by two vectors, neither of which is a multiple of the other, is the infinite plane in which our fixed point of origin and the endpoints of both arrows lie.
 \end{ex}

\begin{exercise} \label{hyperplane} A subset of a vector subspace is called a {\bf hyperplane} or more precisely a {\bf linear hyperplane} if it is a proper subspace and such that it, together with one single further vector, generates our whole vector space. Show that: a hyperplane together with {\it any} vector not belonging to the given hyperplane generates all of our original vector space.
\end{exercise}

\begin{definition}
Let me recall that if $X$ is a set, then the set of all subsets $\mathcal{P}(X) = \{ U : U\subseteq X\}$ of $X$ is the so-called {\bf power set} of X. I get bewildered talking about sets of sets, so I will say that a subset of $\mathcal{P}(X)$ is a {\bf system of subsets of $X$}.  Given such a system $\mathcal{U}\subseteq \mathcal{P}(X)$ we can create two new subsets of $X$, the {\bf union} and the  {\bf intersection} of the sets of our system $\mathcal{U}$, as follows:
\begin{eqnarray*}
\bigcup_{U\in \mathcal{U}} U &=& \{ x\in X: \text{there is $U\in \mathcal{U}$ with $x\in U$}\} \\
\bigcap_{U\in \mathcal{U}} U &=& \{ x\in X: \text{$x\in U$ for all $U\in \mathcal{U}$} \}
\end{eqnarray*}
In particular the intersection of the empty system of subsets of $X$ is $X$, and the union of the empty system of subsets of $X$ is the empty set.
\end{definition}

\begin{exercise} \label{intvssub} Show that: each intersection of vector subspaces of a vector space is again a vector subspace. Note that this has the following consequence: for a subset $T$ of a vector space $V$ over $F$ the intersection of all vector subspaces of $V$ that contain $T$ is obviously the smallest vector subspace of $V$ that contains $T$. This provides us with a new proof of \hyperref[generatingsubsp]{Proposition \ref{generatingsubsp}} on the existence of such a smallest subspace. This proof has the advantage that it is easier to generalise.
\end{exercise}

\section{Linear Independence and Bases}

\begin{definition} A subset $L$ of a vector space $V$ is called {\bf linearly independent} if for all pairwise different vectors $\vec{v}_1, \ldots , \vec{v}_r \in L$ and arbitrary scalars $\alpha_1, \ldots , \alpha_r\in F$, $$\alpha_1  \vec{v}_1 + \cdots + \alpha_r \vec{v}_r = \vec{0} \, \to \, \alpha_1 = \cdots = \alpha_r = 0.$$
\end{definition}

\begin{definition} A subset $L$ of a vector space $V$ is called {\bf linearly dependent} if it is not linear independent. This means there exist pairwise different vectors $\vec{v}_1, \ldots , \vec{v}_r \in L$ and scalars $\alpha_1, \ldots , \alpha_r\in F$, not all zero, such that $\alpha_1  \vec{v}_1 + \cdots \alpha_r \vec{v}_r = \vec{0}$.
\end{definition}

\begin{ex}[Don't fall asleep!] The empty set is linearly independent in every vector space.\end{ex}

\begin{ex}[Keep your eyes open!] A one-element subset is linearly independent precisely when it does not comprise the zero vector. There is trouble for the zero vector because of multiplication by the scalar 1, namely $1\cdot  \vec{0} =  \vec{0}$, and since $1\neq 0$ in a field we see that a set consisting only of the zero vector is not linearly independent. That every other one-element subset is linear independent follows from \hyperref[prodzero]{Lemma \ref{prodzero}}. \end{ex}

\begin{ex}[Now it's interesting] A two-element subset of a vector space is linearly independent if neither of its vectors is a multiple of the other.
\end{ex}

\begin{ex}[Well, it was] A subset of a vector space is linearly dependent if at least one of its vectors can be written as a linear combination of others remaining. I want you to check you can prove this. \end{ex}

\begin{ex} If we think about \hyperref[realspace]{Example \ref{realspace}} where we had the example of {\textcolor{black}{space around us}} together with a fixed point of origin, we say sloppily that three vectors are linearly independent precisely when ``together with the fixed point of origin they do not lie on a plane".
\end{ex}


\begin{definition} \label{basis}A {\bf basis of a vector space} of a vector space $V$ is a linearly independent generating set in $V$.
\end{definition}

\begin{ex}
If we think about \hyperref[realspace]{Example \ref{realspace}} where we had the example of {\textcolor{black}{space around us}} together with a fixed point of origin, then a basis is a set with three vectors that, together with the fixed point of origin, do not lie on a plane.
\end{ex}

Let $A$ and $I$ be sets. Then I will refer to a mapping $I\to A$ as a {\bf family of elements of $A$ indexed by $I$} and use the notation $$(a_i)_{i\in I}.$$ I will use this mainly when the set $I$ plays a secondary role to $A$. In the case $I = \emptyset$, I will talk about the {\bf empty family} of elements of $A$.

I have introduced this notation because it is sometimes practical and it sometimes makes for more elegant reading to use families of vectors $(\vec{v}_i)_{i\in I}$ instead of subsets of our vector space. For instance the family $(\vec{v}_i)_{i\in I}$ would be called a generating set if the set $\{ \vec{v}_i : i \in I\}$ is a generating set. Similarly it would be called {\bf linearly independent} or pedantically a {\bf linearly independent family} if, for pairwise distinct indices $i(1), \ldots , i(r) \in I$ and arbitrary scalars $\alpha_1, \ldots , \alpha_r \in F$, $$\alpha_{1}\vec{v}_{i(1)} + \cdots + \alpha_{r}\vec{v}_{i(r)} = \vec{0} \, \to \, \alpha_1 = \cdots = \alpha_r = 0.$$ An essential but wee-bit-subtle difference between families and subsets is that the same vector may be represented by different indices in a family, in which case linear independence as a family is not possible. A family of vectors that is not linearly independent is called a {\bf linearly dependent family}. A family of vectors that is a generating set and linearly independent is called either a {\bf basis} or a {\bf basis indexed by $i\in I$}.

We will often later use bases that are indexed by a set $\{1 , \ldots , n\}$ where $n\in \mathbb{N}$. The essential difference between this and \hyperref[basis]{Definition \ref{basis}} is that we can then say which basis vector is first, which is second, and so on. In this way, we speak of an {\bf ordered basis}.

\begin{ex}\label{standard} Let $F$ be a field and $n\in \mathbb{N}$. We consider the following vectors in $F^n$ $$\vec{e}_i = ( 0, \ldots , 0, 1, 0, \ldots , 0)$$ with one $1$ in the $i$-th place and zero everywhere else. Then $\vec{e}_1, \ldots , \vec{e}_n$ form an ordered basis of $F^n$, the so-called {\bf standard basis} of $F^n$.
\end{ex}

\begin{theorem}[Linear Combinations of Basis Elements]  \label{lincombiso} Let $F$ be a field, $V$ a vector space over $F$ and $\vec{v}_1, \ldots , \vec{v}_r\in V$ vectors. The family $(\vec{v}_i)_{1\leqslant i \leqslant r}$ is a basis of $V$ if and only if the following ``evaluation" mapping \begin{eqnarray*} \Phi : F^r &\to&  V \\ (\alpha_1, \ldots , \alpha_r) & \mapsto & \alpha_1 \vec{v}_1 + \cdots + \alpha_r \vec{v}_r \end{eqnarray*} is a bijection.
\end{theorem}

If we label our ordered family by $\mathcal{A} = ( \vec{v}_1, \ldots , \vec{v}_r)$, then we denote the above mapping by $\Phi = \Phi_{\mathcal{A}}: F^r \to V.$

\begin{proof}
In gory detail, this goes as follows:
\begin{align*}
(\vec{v}_i)_{1\leqslant i \leqslant r} \text{ is a generating set} & \Leftrightarrow   \Phi \text{ is a surjection }  F^r \twoheadrightarrow V \\
(\vec{v}_i)_{1\leqslant i \leqslant r} \text{ is linearly independent} & \Leftrightarrow   \Phi \text{ is a injection }  F^r \hookrightarrow V \\
(\vec{v}_i)_{1\leqslant i \leqslant r} \text{ is a basis} & \Leftrightarrow   \Phi \text{ is a bijection }  F^r\stackrel{\sim}{\to} V
\end{align*}
The first equivalence comes directly from the definition of a generating set. To see the implication $\Leftarrow$ of the second equivalence, I observe that $\Phi$ sends $(0, \ldots , 0)$ to the zero vector and so there cannot be any other vector in $F^r$ that $\Phi$ sends to the zero vector. To see the implication $\Rightarrow$ we argue by contradiction: suppose that $\Phi$ were not injective, so that there exists $(\alpha_1, \ldots , \alpha_r)\neq (\beta_1, \ldots , \beta_r)$ with the same image $\alpha_1 \vec{v}_1 + \cdots + \alpha_r \vec{v}_r = \beta_1 \vec{v}_1 + \cdots + \beta_r \vec{v}_r$ under $\Phi$. Then we would have $$(\alpha_1-\beta_1) \vec{v}_1 + \cdots + (\alpha_r -\beta_r) \vec{v}_r = \vec{0}$$ as a non-trivial representation of the zero vector. But this is a linear combination of the $\vec{v}_i$ and so our vectors could not have been linearly independent. The last equivalence is a direct consequence of the first two.
\end{proof}

\begin{theorem}[Characterisation of Bases] \label{charbasis}
The following are equivalent for a subset $E$ of a vector space $V$:
\begin{enumerate}
\item Our subset $E$ is a basis, i.e. a linearly independent generating set;
\item Our subset $E$ is minimal among all generating sets, meaning that
$E \backslash \{\vec{v}\}$ does not generate $V$, for any $\vec{v} \in E$;
\item Our subset $E$ is maximal among all linearly independent subsets, meaning
that $E \cup \{\vec{v}\}$ is not linearly independent for any $\vec{v} \in V$.
\end{enumerate}
\end{theorem}

You must understand the terms minimal and maximal here as referring to inclusion between subsets, not as something to do with the number of elements. In order to emphasise that, although I don't like it, I will also speak of a minimal generating set as an {\bf unshortenable} generating set.

\begin{proof} $(1) \Leftrightarrow (2)$. We have to show: a generating set is linearly independent if and only if it is minimal. So it is also enough to show: a generating set is linearly dependent if and only if it is not minimal. Now let $E\subseteq V$ be a generating set which is linearly dependent, so that there is a relation $\lambda_1\vec{v}_1 + \cdots +  \lambda_r\vec{v}_r = \vec{0}$ with $r\geqslant 1$, the $\vec{v}_i\in E$ pairwise distinct, and some $\lambda_i \neq 0$. Without loss of generality we may take $i=1$. It follows that $$\vec{v}_1 = -\lambda_1^{-1}\lambda_2\vec{v}_2 - \ldots - \lambda_1^{-1}\lambda_r\vec{v}_r\in \langle E\setminus \{\vec{v}_1 \}\rangle$$ and so $E\setminus \{\vec{v}_1\}$ is also a generating set for $V$, and hence $E$ is not minimal. Conversely, if $E$ is not minimal, there exists $\vec{v}\in E$ such that $E\setminus \{\vec{v}\}$ is still a generating set. In particular there exists a representation $$\vec{v} = \lambda_1 \vec{v}_1 + \cdots + \lambda_n \vec{v}_n$$ with $n\geqslant 0$ and $\vec{v}_i\in E\setminus \vec{v}$ pairwise distinct. It thus follows that $\vec{v} - \lambda_1 \vec{v}_1 - \cdots - \lambda_n \vec{v}_n =0$ and $E$ is not linearly independent.

$(1) \Leftrightarrow (3)$. We have to show: a linearly independent subset is a generating set if and only if it is maximal. We argue again by contradiction. If $L\subset V$ is linearly independent but not a generating set, then for each $\vec{v} \in V \setminus \langle L \rangle$ we also have that $L\cup \{ \vec{v} \}$ is linearly independent and so $L$ was not maximal. Conversely, if $L$ is not maximal then there is a vector $\vec{v}$ such that $L\cup \{ \vec{v}\}$ is linearly independent and therefore $L$ cannot be a generating set since this vector $\vec{v}$ can not have been generated. \end{proof}

If you look carefully, our \hyperref[charbasis]{Theorem \ref{charbasis}} implies in particular that every finitely generated vector space has a finite basis: simply start with a finite generating set, then take vectors away until we have an unshortenable generating set.
\begin{corollary}[The existence of a basis] \label{existencebasis} Let $V$ be a finitely generated vector space over a field $F$. Then $V$ has a basis.
\end{corollary}
\begin{proof}
Let $E=\{\vec{e}_1,\vec{e}_2,\dots,\vec{e}_r\} \subset V$ be a finite generating set. If it is not linearly independent then
$$ \lambda_1 \vec{e}_1 + \cdots + \lambda_r \vec{e}_r~=~\vec{0} \in V$$
for some $\lambda_i\in F$ which are not all $0 \in F$. Without loss of generality we may assume that $\lambda_1 \neq 0 \in F$, and then the
subset $E \setminus \{\vec{e}_1\}=\{\vec{e}_2,\vec{e}_2,\dots,\vec{e}_r\} \subset V$
is a smaller finite generating set. Proceed in this manner, until you end up with
a linearly independent subset of $E$ which generates $V$ -- a basis!
\end{proof}
With more sophisticated set theory we can even show the general {\bf Existence-of-a-Basis Theorem}, which states that every vector space has a basis. We'll discuss that in a workshop.

\begin{theorem}[A useful variant on the Characterisation of Bases] Let $V$ be a vector space.
\begin{enumerate}
\item If $L\subset V$ is a linearly independent subset and $E$ is minimal amongst all generating sets of our vector space with the property that $L\subseteq E$, then $E$ is a basis.
\item If $E\subseteq V$ is a generating set and if $L$ is maximal amongst all linearly independent subsets of vector space with the property $L\subseteq E$, then $L$ is a basis.
\end{enumerate}
\end{theorem}

\begin{proof} (1) If $E$ were not a basis, then there would be a non-trivial relation between its vectors $\lambda_1\vec{v}_1 + \cdots +  \lambda_r\vec{v}_r = \vec{0}$ with $r\geqslant 1$, the $\vec{v}_i\in E$ pairwise distinct, and all $\lambda_i \neq 0$. Not all the vectors $\vec{v}_i$ could belong to $L$ because $L$ is linearly independent. Thus there is a $\vec{v}_i$ belonging to $E\setminus L$ and it can be written as a linear combination of the other elements of $E$. But then $E\setminus \{ \vec{v}_i \}$ is also a generating set that contains $L$ and so $E$ was not minimal.

(2) If $L$ were not a basis, then $L$ could not be a generating set and so there would necessarily be a vector $\vec{v}\in E$ that didn't lie in the subspace generated by $L$. If we add that vector to $L$ then we obtain a bigger linearly independent subset contained in $E$ and so $L$ was not maximal. \end{proof}

\begin{definition}[To infinity, but not beyond] \label{mapsvs} Let $X$ be a set and $F$ a field. The set ${\rm Maps}(X,F)$ of all mappings $f:X \to F$ becomes an $F$-vector space with the operations of pointwise addition and multiplication by a scalar. The subset of all mappings which send almost all elements of $X$ to zero is a vector subspace $$F\langle X \rangle \subseteq {\rm Maps}(X,F).$$ This vector subspace is called the {\bf free vector space on the set $X$} or pedantically {\bf the free vector space over $F$ on the set $X$}.
\end{definition}

I want to remind you that ``almost all" is a technical mathematical abbreviation meaning ``all but finitely many". So when I write that almost all elements of $X$ are sent to zero by $f$, this means that only finitely many elements of $X$ take non-zero values by the mapping $f$. Of course, this is no condition at all if $X$ itself is a finite set, but if $X$ is infinite then it is a severe restriction.
\medskip

We call $a\in F\langle X\rangle$ a ``formal linear combination of elements from $X$" and instead of writing it as $(a_x)_{x\in X}$ I will use the more suggestive $\sum_{x\in X}a_x x$. For instance if $X = \{ \heartsuit, \sharp, \bigstar \}$ then a typical element of $\mathbb{Q}\langle X \rangle$ is $$\frac{14}{23}\heartsuit + 7\sharp - \frac{66}{5}\bigstar.$$

\begin{theorem}[A useful variant on Linear Combinations of Basis Elements]\label{lincombbasvar}
Let $F$ be a field, $V$ an $F$-vector space and $(\vec{v}_i)_{i\in I}$ a family of vectors from the vector space $V$. The following are equivalent:
\begin{enumerate}
\item The family $(\vec{v}_i)_{i\in I}$ is a basis for $V$;
\item For each vector $\vec{v}\in V$ there is precisely one family $(a_i)_{i\in I}$ of elements of our field $F$, almost all of which are zero and such that $$\vec{v} = \sum_{i\in I} a_i \vec{v}_i.$$
\end{enumerate}
\end{theorem}
\begin{proof} I use the notation $F\langle X \rangle$ to reformulate the theorem as follows: given an $F$-vector space $V$, a family $(\vec{v}_i)_{i\in I}$ of vectors is a basis if and only if the ``evaluation" mapping \begin{eqnarray*} \Phi : F\langle I \rangle & \to & V \\ (a_i)_{i\in I} & \mapsto & \sum_{i\in I}a_i \vec{v}_i\end{eqnarray*} is a bijection between the free vector space $F\langle I \rangle$ on the set $I$ and the given vector space $V$.

In gory detail, this goes as follows:
\begin{align*}
(\vec{v}_i)_{i\in I} \text{ is a generating set}  & \Leftrightarrow  \Phi \text{ is a surjection }  F\langle I \rangle \twoheadrightarrow V \\
(\vec{v}_i)_{i\in I} \text{ is linearly independent}  & \Leftrightarrow \Phi \text{ is a injection }  F\langle I \rangle \hookrightarrow V \\
(\vec{v}_i)_{i\in I} \text{ is a basis}  & \Leftrightarrow \Phi \text{ is a bijection }  F\langle I \rangle \stackrel{\sim}{\to} V
\end{align*}
The proof of these statements follows the proof of \hyperref[lincombiso]{Theorem \ref{lincombiso}} word-for-word.
\end{proof}

\section{Dimension of a Vector Space}


\begin{theorem}[Fundamental Estimate of Linear Algebra] \label{estimate}
No linearly independent subset of a given vector space has more elements than a generating set. Thus if $V$ is a vector space, $L\subset V$ a linearly independent subset and $E\subseteq V$ a generating set, then: $$|L|\leqslant |E|$$
\end{theorem}

The ``Fundamental Estimate of Linear Algebra" is a thunderous title for such an obviously obvious theorem. In my statement of it, I am using the convention that for any infinite set $X$ I simply set $|X| = \infty$. Then the theorem has content for finitely generated vector spaces only. But, with a more refined interpretation of $|X|$ as the cardinality of a set, it is still true.

\begin{proof} By contradiction. Let $F$ be our ground field. Let's assume that we have a generating set $E = \{ \vec{w}_1, \ldots , \vec{w}_m\}$ and a linearly independent subset $L = \{ \vec{v}_1, \ldots , \vec{v}_n\}$ with $n>m$.  By definition we can find $a_{ij} \in F$ with
\begin{eqnarray*}
\vec{v}_1 & = &  a_{11}\vec{w}_1 +  a_{21}\vec{w}_2 + \cdots +  a_{m1} \vec{w}_m \\
\vdots \hspace{0.2cm} &  & \hspace{0.3cm} \vdots \hspace{1.5cm} \vdots \hspace{2.5cm} \vdots  \\
\vec{v}_n & = &  a_{1n}\vec{w}_1 +  a_{2n}\vec{w}_2 + \cdots +  a_{mn} \vec{w}_m
\end{eqnarray*}
Now take from this the ``vertically written" homogeneous system of linear equations
\begin{eqnarray*}
x_1a_{11} \hspace{1cm} x_1a_{21} & \hspace{1cm} \cdots & \hspace{1cm} x_1a_{m1} \\
 + \hspace{1.6cm} + \quad &  & \hspace{1.2cm} + \\
\vdots \hspace{1.8cm}  \vdots \quad \, & \hspace{1cm} \cdots &\hspace{1.3cm} \vdots \\
+  \hspace{1.6cm} + \quad  & & \hspace{1.2cm} + \\
x_na_{1n} \hspace{1cm}  x_na_{2n} & \hspace{1cm} \cdots & \hspace{1cm} x_na_{mn} \\
= \hspace{1.6cm} = \quad & &\hspace{1.2cm} = \\
0 \hspace{1.75cm} 0 \quad & \hspace{1cm} \cdots & \hspace{1.3cm} 0
\end{eqnarray*}
In its usual form this looks like:
\begin{eqnarray*}
      a_{11} x_1 + a_{12}x_2 +  \cdots  + a_{1n}x_n & =  & 0 \\
      a_{21} x_1 + a_{22}x_2 +  \cdots  + a_{2n}x_n & = & 0 \\
      \vdots  \hspace{13mm} \vdots \hspace{23mm} \vdots \hspace{5mm} &&  \, \vdots \\
      a_{m1} x_1 + a_{m2}x_2 + \cdots  + a_{mn}x_n & =  & 0
\end{eqnarray*}
This system of linear equations has fewer equations than unknowns, so \hyperref[gaussalg]{Gaussian elimination} shows that there is at least one non-zero solution $(x_1, \ldots , x_n) \neq (0, \ldots , 0)$. Then, for each such solution $$x_1 \vec{v}_1 + \cdots + x_n \vec{v}_n = \vec{0},$$ which contradicts the linear independence of the $ \vec{v}_i$.
\end{proof}

A consequence of the Fundamental Estimate is the following theorem, which develops the statement of the Fundamental Estimate.

\begin{theorem}[Steinitz Exchange Theorem]  \label{steinitz} Let $V$ be a vector space, $L\subset V$ a finite linearly independent subset and $E\subseteq V$ a generating set. Then there is an injection $\phi: L \hookrightarrow E$ such that $(E\setminus \phi(L)) \cup L$ is also a generating set for $V$.
\end{theorem}

In other words, we can swap some elements of a generating set by the elements of our linearly independent set, and still keep a generating set. With more sophisticated methods from set theory, this theorem holds without the finiteness restriction on $L$.

\begin{proof} This follows by induction from the \hyperref[exchange]{Exchange Lemma}  below: it lets us swap elements from $E$ with elements from $L$, one-by-one.
\end{proof}

\begin{lemma}[Exchange Lemma] \label{exchange}Let $V$ be a vector space, $M \subseteq V$ a linearly independent subset, and $E\subseteq V$ a generating subset, such that $M \subseteq E$.
If $\vec{w}\in V\setminus M$ is a vector not belonging to $M$ such that $M\cup \{ \vec{w} \}$ is linearly independent, then there exists $\vec{e}\in E\setminus M$ such that $\left\{E\setminus \left\{\vec{e}\right\}\right\}\cup \{ \vec{w}\}$ is a generating set for $V$.
\end{lemma}

\begin{proof}
Since $E$ is a generating set for $V$, we can write $\vec{w}$ as a linear combination of vectors from $E$,
$$\vec{w} = \lambda_1 \vec{e}_1 + \cdots + \lambda_r \vec{e}_r~~(\lambda_i\in F)$$
with pairwise distinct $\vec{e}_i\in E$ and all $\lambda_i \neq 0$. (The subset $\{\vec{e}_1,\dots,\vec{e}_r\} \subseteq E$ depends on $\vec{w}$). Since $M\cup \{ \vec{w} \}$ is linearly independent, it cannot be that all $\vec{e}_i$ belong to $M$. Without loss of generality we may then assume that $\vec{e}_1\notin M$. Now we write the above identity as $$\vec{e}_1 = \lambda_1^{-1}(\vec{w} - \lambda_2 \vec{e}_2 - \cdots - \lambda_r\vec{e}_r)$$ from which we see that $(E\setminus \{ \vec{e}_1\}) \cup \{ \vec{w} \}$ is a generating set.
\end{proof}


\begin{corollary}[Cardinality of Bases] \label{card}
Let $V$ be a finitely generated vector space.
\begin{enumerate}
\item $V$ has a finite basis.
\item $V$ cannot have an infinite basis.
\item Any two bases of $V$ have the same number of elements.
\end{enumerate}
\end{corollary}
\begin{proof} 
(1) This is Corollary~\ref{existencebasis}.\\
(2) If $V$ has a finite basis with $r$ elements and an infinite basis then a subset of the infinite basis with $r+1$ elements would be linearly independent
and $r+1 \leqslant r$ by the Fundamental Estimate of Linear Algebra  \ref{estimate} -- a contradiction! \\
(3) 
Suppose that $B_1$ and $B_2$ are two finite bases for $V$.  Since $B_1$ is linearly independent and $B_2$ generates  
$|B_1|\leqslant |B_2|$
by the Fundamental Estimate of Linear Algebra  \ref{estimate}.
Similarly, since $B_2$ is linearly independent and $B_1$ generates, $|B_2|\leqslant |B_1|$. 
Hence $|B_1|=|B_2|$.
\end{proof}

With more sophisticated methods from set theory, we can remove the restriction to finitely generated vector spaces: for any two bases of a vector space there exists a bijection between one and the other.

\begin{definition} The cardinality of one (and by \hyperref[card]{Corollary \ref{card}} each) basis of a finitely generated vector space $V$ is called the {\bf dimension} of $V$ and will be denoted by $\dim V$. If $F$ is a field, and we want to denote that we mean dimension as an $F$-vector space, then we will write $ \dim_F V.$ If the vector space is not finitely generated, then we write $\dim V =\infty$ and call $V$ {\bf infinite dimensional}. As usual, we will ignore the difference between different infinities.
\end{definition}

\begin{rem}\label{physdim} In Physics, sadly, the term ``Dimension" has a completely different use: physical dimensions are things like length, time, mass, frequency, and so on. In our use these all model ``one-dimensional real vector spaces". I hope that you, dear Reader, use context to determine which notion of dimension is meant at any one time.
\end{rem}

\begin{ex} The zero vector space has as its basis the empty set. Its dimension therefore is zero. In general, thanks to \hyperref[standard]{Example \ref{standard}}, we have $$\dim_F F^n = n.$$\end{ex}

\begin{corollary}[Cardinality Criterion for Bases] \label{cardbasis}  Let $V$ be a finitely generated vector space.
\begin{enumerate}
\item Each linearly independent subset $L\subset V$ has at most $\dim V$ elements, and if $|L| = \dim V$ then $L$ is actually a basis;
\item Each generating set $E\subseteq V$ has at least $\dim V$ elements, and if $|E| = \dim V$ then $E$ is actually a basis.
\end{enumerate}
\end{corollary}
\begin{proof} Following \hyperref[estimate]{Theorem \ref{estimate}} for $L$ a linearly independent subset, $B$ a basis and $E$ a generating set, we have $$|L| \leqslant |B| \leqslant |E|.$$ Since there is a finite generating set, if $|L|= |B|$ then it must be that $|L|$ is a maximal linearly independent subset, and so a basis by \hyperref[charbasis]{Theorem \ref{charbasis}}. If $|B| = |E|$ then $E$ must be a minimal generating set and so by \hyperref[charbasis]{Theorem \ref{charbasis}} a basis.
\end{proof}

\begin{corollary}[Dimension Estimate for Vector Subspaces] \label{subspacesmall} A proper vector subspace of a finite dimensional vector space has itself a strictly smaller dimension.
\end{corollary}

\begin{rem}\label{dim&contain} If $U\subseteq V$ is a vector subspace of an arbitrary vector space, then we have $\dim U \leqslant  \dim V$ and if we have $\dim U = \dim V < \infty$ then it follows that $U=V$.
\end{rem}

\begin{proof}
 Since $V$ is finite dimensional, it is finitely generated and so, thanks to \hyperref[cardbasis]{Corollary \ref{cardbasis}}, $U$ contains a maximal linearly independent subset and each such subset must have at most $\dim V$ elements. By \hyperref[charbasis]{Theorem \ref{charbasis}} each such subset is a basis of $U$ and so this shows that $\dim U \leqslant \dim V$. If this were an equality, then since $V$ is finite dimensional it would mean by \hyperref[cardbasis]{Corollary \ref{cardbasis}} that each basis of $U$ was also a basis of $B$, showing that $U=V$.
\end{proof}

\begin{exercise} Show that each one dimensional vector space has exactly two vector subspaces.
\end{exercise}

If $V$ is a vector space and $U,W$ are subspaces of $V$, then we define $U+W$ to be the subspace $\langle U\cup W \rangle$ of $V$ generated by $U$ and $W$ together.

\begin{theorem}[The Dimension Theorem] \label{dimthm} Let $V$ be a vector space containing vector subspaces $U,W \subseteq V$. Then $$\dim (U+W) + \dim (U\cap W) = \dim U + \dim W.$$
\end{theorem}

In \ref{dimagain} we'll prove this theorem again as a corollary of the Rank-Nullity Theorem for linear mappings.

\begin{ex}
If we think about \hyperref[realspace]{Example \ref{realspace}} of {\textcolor{black}{space around us}} with a fixed point of origin, then the two-dimensional vector subspaces are the planes that pass through our fixed point of origin. Given a two distinct two-dimensional vector spaces  $U,W$, their sum spans the whole space so that $\dim (U+W) = 3$. Now two distinct planes passing through the same fixed point of origin obviously meet in a straight line, and that confirms the statement of the above theorem, which in this case then states $3+1 = 2+2$.
\end{ex}
% $$
% \includegraphics[scale=0.5]{IntersectingPlanes.pdf}
% $$
\begin{proof}
If either of $U$ or $W$ are infinite dimensional, then the statement is clear. Otherwise, choose a basis $s_1, \ldots , s_d$ of $U\cap W$ and extend it by the elements $u_1, \ldots, u_r\in U$ to a basis of $U$ and then by the elements $w_1, \ldots , w_t \in W$ to a basis of $U+W$. We'll have won if we can show that $s_1, \ldots , s_d, w_1, \ldots , w_t$ is a basis of $W$. Since this subset is linearly independent by construction, we only need to show that it generates $W$. We can definitely write $w\in W$ as a linear combination
\begin{align*}
w = & \lambda_1 u_1 + \cdots + \lambda_r u_r \\ & \hspace{0.25cm}+ \mu_1s_1 + \cdots + \mu_d s_d \\ & \hspace{0.6cm} + \nu_1 w_1 + \cdots + \nu_t w_t
\end{align*}
From this it is clear that $\lambda_1 u_1 + \cdots + \lambda_r u_r \in W\cap U$ and so we can write this element as a linear combination of the $s_i$. It follows then that $w$ is itself a linear combination of the $s_i$ and the $w_j$, which is what we wanted to show.
\end{proof}

\begin{exercise} \label{dimofdirsum} Given $F$-vector spaces $V_1, \ldots , V_n$ show that: the dimension of their cartesian product, in the sense of \hyperref[cartesian]{Section \ref{cartesian}}, is given by $$\dim (V_1\oplus \cdots \oplus V_n) = \dim(V_1) + \cdots + \dim(V_n).$$\end{exercise}

\begin{exercise} \label{complextoreal} Recall that $\mathbb{R}\subset \mathbb{C}$. Thus we can think of any $\mathbb{C}$-vector space as a vector space over $\mathbb{R}$. Show that: $\dim_{\mathbb{R}}V = 2 \dim_{\mathbb{C}} V$\end{exercise}

\section{Linear Mappings}

\begin{definition} \label{deflinmap} Let $V,W$ be vector spaces over a field $F$. A mapping $f: V\to W$ is called {\bf linear} or more precisely {\bf $F$-linear} or even a {\bf homomorphism of $F$-vector spaces} if for all $\vec{v}_1, \vec{v}_2\in V$ and $\lambda\in F$ we have
\begin{align*}
f(\vec{v}_1+\vec{v}_2) = & f(\vec{v}_1) + f(\vec{v}_2) \\
f(\lambda \vec{v}_1) = & \lambda f(\vec{v}_1)
\end{align*}
A bijective linear mapping is called an {\bf isomorphism} of vector spaces. If there is an isomorphism between two vector spaces, we call them {\bf isomorphic}. A homomorphism from one vector space to itself is called an {\bf endomorphism} of our vector space. An isomorphism of a vector space to itself is called an {\bf automorphism} of our vector space.
\end{definition}

\begin{etym} This terminology ``homomorphism" comes from the Greek ``\greektext morfh\latintext" for ``form, shape" and the Greek ``\begin{greek} <omo \end{greek}" \latintext for ``same, similar".  Then ``isomorphism" arises from the Greek ``\greektext >'iso\latintext" for ``equal". The word ``endomorphism" comes from the Greek ``\greektext >'endon\latintext" for ``within, inside", while ``automorphism" comes from ``\greektext a>ut'os\latintext" for ``self". The word ``linear" for a mapping seems to come from the case of an $\mathbb{R}$-linear mapping $f: \mathbb{R}\to \mathbb{R}$ whose graph must be a straight line. But it's a little annoying, because the functions $f: \mathbb{R}\to \mathbb{R}, x\mapsto ax+b$ are all straight lines, but they are only linear in our sense in the case $b=0$. Maybe in your past you called them linear when $b\neq 0$, but now you will not; those ones are ``affine".
\end{etym}

\begin{ex} (i) The projections  ${\sf pr}_i : F^n \to F;(\lambda_1,\lambda_2,\dots,\lambda_n) \mapsto \lambda_i$ are linear. \\
(ii) The squaring
function  $F\to F;\lambda \mapsto \lambda^2$ is not linear, unless $2=0 \in F$, e.g. if $F={\mathbb F}_2=\{0,1\}$ is the field with two elements.
\end{ex}

\begin{ex} Given vector spaces $V,W$, the projection mappings ${\sf pr}_V : (V\oplus W) \to V$ and ${\sf pr}_W : (V\oplus W) \to W$ are linear. The same holds for more general projections ${\sf pr_i} : V_1 \oplus \cdots \oplus V_n \to V_i$. Furthermore the {\bf canonical injections} ${\sf in}_V : V \to (V\oplus W), v\mapsto (v,0)$ and ${\sf in}_W : W \to (V\oplus W), w\mapsto (0,w)$ are linear and again the same holds with more summands for the analogously defined ${\sf in}_i: V_i \to V_1 \oplus \cdots \oplus V_n.$
\end{ex}

\begin{ex}
The map $\Phi$ from \hyperref[lincombiso]{Theorem \ref{lincombiso}} is an isomorphism. 
\end{ex}

\begin{exercise} \label{linmapcomp} Every composition of a vector space homomorphisms is again a vector space homomorphism. In other words if $f: V\to W$ and $g: U\to V$ are linear mappings, then so too is $f\circ g: U \to W$.
\end{exercise}

\begin{exercise} \label{linmapiso} Show that: If $f: V \to W$ is a vector space isomorphism, then the inverse mapping $f^{-1}: W\to V$ is also a vector space isomorphism. In particular, the automorphisms of a vector space $V$ form a subgroup of its permutation group. This group is called the {\bf general linear group} or the {\bf automorphism group} of our vector space $V$ and is denoted by $${\rm GL}(V) = {\rm Aut}(V) \subseteq {\rm Maps}^{\times}(V,V).$$ If I want to draw attention to the ground field, I will write ${\rm Aut}_k(V)$. These groups are amongst my all-time personal favourites.
\end{exercise}

\begin{exercise}\label{imageissub} Show that: the image of a vector subspace under a linear mapping is again a vector subspace, and that the preimage of a vector subspace under a linear mapping is again a vector subspace.
\end{exercise}

\begin{definition}\label{fixer} A point that is sent to itself by a mapping is called a {\bf fixed point} of the mapping. Given a mapping $f:X \to X$, we denote the set of fixed points by $$X^f = \{ x\in X : f(x) = x \}.$$
\end{definition}

\begin{exercise} \label{fixerex} Let $V$ be a vector space and $f\in {\rm End}(V)$ an endomorphism. Show that: the fixed point set $V^f \subseteq V$ is a vector subspace.\end{exercise}

\begin{exercise}[Homomorphisms from direct sums] \label{dirsumhom} Show that: given vector spaces $V_1, \ldots ,V_n, W$ and linear mappings $f_i : V_i \to W$ then we can form a new linear mapping $f: V_1 \oplus \cdots \oplus V_n \to W$ by the recipe $f(v_1, \ldots , v_n)  = f_1(v_1) + \cdots + f_n(v_n)$. In this way we even get a bijection $${\rm Hom}(V_1, W) \times \cdots \times {\rm Hom}(V_n, W) \stackrel{\sim}{\to} {\rm Hom}(V_1 \oplus \cdots \oplus V_n, W)$$ whose inverse can be written as $f\mapsto (f\circ {\sf in}_i)_i$.
\end{exercise}

\begin{exercise}[Homomorphisms for products] Show that: given vector spaces $V, W_1, \ldots , W_n$ and linear mappings $g_i: V\to W_i$ then we can form a new linear mapping $g: V\to W_1 \oplus \cdots \oplus W_n$ by the recipe $g(v) = (g_1(v), \ldots , g_n(v))$. In this way we even get a bijection $${\rm Hom}(V, W_1) \times \cdots \times {\rm Hom}(V, W_n) \stackrel{\sim}{\to}  {\rm Hom}(V, W_1 \oplus \cdots \oplus W_n) $$ whose inverse can be written as $g\mapsto ({\sf pr}_i \circ g)_i$.
\end{exercise}

\begin{definition}\label{internaldirsum} Two vector subspaces $V_1, V_2$ of a vector space $V$ are called {\bf complementary} if addition defines a bijection $$V_1\times V_2 \stackrel{\sim}{\to} V.$$ Taking $W = V$ in \hyperref[dirsumhom]{Exercise \ref{dirsumhom}}, we then produce an vector space isomorphism $V_1\oplus V_2 \stackrel{\sim}{\to} V$. We abuse notation a little by writing $V = V_1 \oplus V_2$ and say that the vector space $V$ is the {\bf direct sum}, or more precisely the {\bf internal direct sum} of the vector subspaces $V_1$ and $V_2$.
\end{definition}

Let $V$ be a vector space with vector subspaces $V_1, \ldots , V_n$. Then the vector subspace of $V$ they generate is called the {\bf sum} of our vector subspaces and denoted by $V_1 + \cdots + V_n$. This generalises the sum defined just before \hyperref[dimthm]{Theorem \ref{dimthm}}. If the natural homomorphism given by addition $V_1 + \cdots + V_n \to V$ is an injection then we say the {\bf the sum of the vector subspaces $V_i$ is direct} and we write their sum also as $V_1\oplus \cdots \oplus V_n$.

\begin{exercise}\label{reflections} How many vector subspaces are there in $\mathbb{R}^2$ that are sent to themselves under the reflection $(x,y) \mapsto (x,-y)$? Which vector subspaces in $\mathbb{R}^3$ are sent to themselves by the reflection $(x,y,z) \mapsto (x,y,-z)$?
\end{exercise}
\begin{center}
\begin{minipage}{.3\textwidth}
$$
\begin{tikzpicture}
\draw[->] (-1.5,0) -- (1.5,0) node[below]{$x$};
\draw[->] (0,-1.5) -- (0,1.5) node[right]{$y$};
 \draw[ultra thick, draw = red, <->] (-0.5,0.8) .. controls (-0.2, 1.2) and (0.2, 1.2)  .. (0.5,0.8);
\end{tikzpicture}
$$
\end{minipage}
and
\begin{minipage}{.3\textwidth}
$$
\begin{tikzpicture}
\draw[->] (-1.5,0) -- (1.5,0) node[below]{$x$};
\draw[->] (0,-1.5) -- (0,1.5) node[right]{$y$};
\draw[->] (-1,-1) -- (1,1) node[right]{$z$};
 \draw[ultra thick, draw = red, <->] (0.2,0.6) .. controls (0.4, 1.0) and (1.0, 0.9)  .. (0.9,0.3);
\end{tikzpicture}
$$
\end{minipage}
\end{center}

\begin{theorem}[The Classification of Vector Spaces by their Dimension] \label{dimclass} Let $n$ be a natural number. Then a vector space over a field $F$ is isomorphic to $F^n$ if and only if it has dimension $n$.
\end{theorem}

\begin{proof}
It's easy to check that a surjective homomorphism sends a generating set to a generating set and that an injective homomorphism sends a linearly independent subset to a linearly independent subset. So a vector space isomorphism sends a basis to a basis. Thus if two vector spaces are isomorphic then they have the same dimension. Conversely, if a vector space $V$ has an ordered basis $B = (\vec{v}_1, \ldots , \vec{v}_n)$ consisting of $n$ vectors, then the mapping $(\lambda_1, \ldots , \lambda_n) \mapsto \lambda_1 \vec{v}_1 + \cdots + \lambda_n \vec{v}_n$ of \hyperref[lincombiso]{Theorem \ref{lincombiso}} produces a vector space isomorphism $F^n \stackrel{\sim}{\to} V$.
\end{proof}

Now we are in a position to solve the problem raised at the end \hyperref[theshotthatshooktheworld]{Section \ref{slas}}: is the ``number of free parameters" in our representation of the solution set of a system of linear equations well-defined, or more precisely whether the number of stairs appearing in Gaussian elimination does not depend on the enumeration of the variables, i.e. on the ordering of the columns. Now if we can show this for homogeneous systems then it obviously follows for arbitrary systems. As we already pointed out in \hyperref[solsaresubs]{Example \ref{solsaresubs}} the solution set $L\subseteq F^m$ of a homogeneous system is a vector subspace of $F^m$. We obtain a vector space isomorphism $L\stackrel{\sim}{\to} F^{m-r}$ by ``removing all entries where a stair begins", in other words by omitting $x_{s(1)}, x_{s(2)}, \ldots , x_{s(r)}$ from our $m$-tuple $(x_1, \ldots , x_m)\in L$. Here $r$ is the number of stairs. Hence we see that it has an independent description via the dimension of the solution set, namely $r = m - \dim_F L.$
\medskip

Let $V,W$ be vector spaces over a field $F$. The set of all homomorphisms from $V$ to $W$ is denoted by $${\rm Hom}_F (V,W) = {\rm Hom}(V,W) \subseteq {\rm Maps}(V,W).$$

\begin{lemma}[Linear mappings and bases] \label{linmapbas} Let $V,W$ be vector spaces over $F$ and let $B\subset V$ be a basis. Then restriction of a mapping gives a bijection \begin{eqnarray*} {\rm Hom}_F(V,W) &\stackrel{\sim}{\to}& {\rm Maps}(B, W)\\ f&\mapsto & f|_B.\end{eqnarray*}
\end{lemma}
In other words each linear mapping determines and is completely determined by the values it takes on a basis.
\begin{proof}
Let $f,g: V\to W$ be linear. If $f(\vec{v}) = g(\vec{v})$ for all $\vec{v}\in B$ then it follows by linearity that $f(\lambda_1\vec{v}_1 + \cdots + \lambda_r \vec{v}_r) = g(\lambda_1 \vec{v}_1 + \cdots + \lambda_r \vec{v}_r)$ for all $\lambda_1, \ldots , \lambda_r\in F$ and $\vec{v}_1, \ldots , \vec{v}_r \in B$. Thus $f(\vec{v}) = g(\vec{v})$ for all $\vec{v}$ in the linear span of $B$, in other words for all $\vec{v}\in V$. This proves the injectivity of the restriction mapping ${\rm Hom}_k(V,W) {\to} {\rm Maps}(B, W)$ when $B$ is any generating set for $V$. Now suppose conversely that we have a mapping of sets $g:B \to W$. We extend it to a linear mapping $\tilde{g}:V \to W$ as follows: each vector $\vec{v}\in V$ may be written thanks to \hyperref[lincombbasvar]{Theorem \ref{lincombbasvar}} uniquely as a linear combination of basis vectors, $\vec{v} = \lambda_1 \vec{v}_1 + \cdots + \lambda_r \vec{v}_r$ with pairwise different $\vec{v}_i\in B$. Now simply set $$\tilde{g}(\vec{v}) = \lambda_1 g(\vec{v}_1) + \cdots + \lambda_r g (\vec{v}_r).$$ This produces the linear extension of $g$ we were seeking.\end{proof}

\begin{exercise} \label{linmapvs} Let $V,W$ be vector spaces over a field $F$. Show that ${\rm Hom}_F(V,W)$ is a vector subspace of the set of all mappings ${\rm Maps}(V,W)$ from $V$ to $W$ with its vector space structure given similarly to \hyperref[mapsvs]{Definition \ref{mapsvs}}. Show that: $$\dim {\rm Hom}_F(V,W) = (\dim V)(\dim W),$$ where I am using the convention $0\cdot \infty = \infty \cdot 0 = 0.$ In fact, it is possible to make a more sophisticated version of this equality using the relationship with cardinality given by \hyperref[linmapbas]{Lemma \ref{linmapbas}}.\end{exercise}

\begin{exercise} \label{existenceofcomplements} Let $V$ be a finite dimensional vector space, and let $U$ be a proper vector subspace. Show that: there exists at least one (and in fact many different) vector subspace(s) of $V$ complementary to $U$. If you're brave, try to do this also for not necessarily finite dimensional vector spaces. \end{exercise}

\begin{proposition} \begin{enumerate} \item Every injective linear mapping $f: V\hookrightarrow W$ has a {\bf left inverse}, in other words a linear mapping $g:W \to V$ such that $g\circ f = \id_V$.
\item Every surjective linear mapping $f:V \twoheadrightarrow W$ has a {\bf right inverse}, in other words a linear mapping $g:W \to V$ such that $f\circ g = \id_W$.
\end{enumerate}
\end{proposition}
 I will give the proof for finite dimensional vector spaces $V,W$. The general case will follow by the same argument, but as usual you need a little more from set theory.
\begin{proof}
(1) By \hyperref[existenceofcomplements]{Exercise \ref{existenceofcomplements}}, we may choose a complement $U\subseteq W$ to the subspace $f(V)$ of $W$. We then define $g:W\to V$ by the rule $g(u+f(v)) = v$ for all $u\in U, v\in V$. This is well-defined, since $U$ and $f(V)$ are complementary and so the mapping $U\times V \to W, (u,v) \mapsto u+f(v)$ is an isomorphism.

(2) Choose a basis $B \subset W$. Then since $f$ is surjective we can define a mapping of sets $\tilde{g}: B\to V$ such that $f(\tilde{g}(b)) = b$ for all $b\in B$. Now let $g: W\to V$ be the unique linear mapping which satisfies $g(b) = \tilde{g}(b)$, whose existence follows from \hyperref[linmapbas]{Lemma \ref{linmapbas}}. It follows that $f(g(b)) = b$ for all $b\in B$ and therefore $f(g(w)) = w$ for all $w\in W$.
\end{proof}

\begin{exercise} Show that: each linear mapping from a vector subspace $U$ of a vector space $V$ to another vector space $W$, $f:U \to W$, can be extended to a linear mapping $\tilde{f} : V\to W$ on the whole vector space $V$.\end{exercise}

\section{Rank-Nullity Theorem}
\begin{definition}The {\bf image} of a linear mapping $f:V \to W$ is the subset $\im (f) = f(V) \subseteq W$. It is a vector subspace of $W$ by \hyperref[imageissub]{Exercise \ref{imageissub}}. The preimage of the zero vector of a linear mapping $f: V\to W$ is denoted by $${\rm ker}(f):= f^{-1}(0) = \{ v\in V: f(v) = 0 \}$$ and is called the {\bf kernel} of the linear mapping $f$. The kernel is a vector subspace of $V$ by \hyperref[imageissub]{Exercise \ref{imageissub}}.
\end{definition}

\begin{lemma} \label{ker=0} A linear mapping $f:V \to W$ is injective if and only if its kernel is zero.
\end{lemma}

\begin{rem} Of course, you already know this result from Y2 Fundamentals of Pure Mathematics.\end{rem}

\begin{proof}
If any element other than the zero vector of $V$ is in the kernel of $f$, then $f$ would send it to the zero vector of $W$ and hence $f$ would not be injective. Conversely if the mapping $f$ is not injective, then there exist $v_1\neq v_2$ in $V$ with $f(v_1) = f(v_2)$. It follows that $v_1-v_2\neq 0$ but $f(v_1-v_2) = 0$. Thus $v_1-v_2 \in {\rm ker} (f)$ is different from zero, so the kernel is not zero.
\end{proof}

\begin{theorem}[{Rank-Nullity Theorem}] \label{rnthm} Let $f: V\to W$ be a linear mapping between vector spaces. Then:
$$\dim V = \dim ({\rm ker} f) + \dim (\im f).$$
\end{theorem}

This is called the ``Rank-Nullity Theorem" because it is common to call the dimension of $\im f$ the {\bf rank} of $f$, and the dimension of $\ker f$ the {\bf nullity} of $f$.

\begin{proof}
If $V$ is finitely generated then so too is $\im f$, since the image $f(E)$ of a generating set $E\subset V$ is a generating set for $f(V) = \im f$. Furthermore $\ker f$ is also finitely generated thanks to \hyperref[subspacesmall]{Corollary \ref{subspacesmall}}, since it is a subspace of $V$. Thus if $\dim (\ker f) = \infty$ or $\dim (\im f) = \infty$ then we deduce that $\dim V = \infty$ and the theorem is proved in those two cases. Thus we may assume that both $\ker f$ and $\im f$ are finite dimensional.

\phantomsection
\label{rnthm2ndpara}Let $A$ be a basis of the kernel of $f$, $B$ a basis of the image of $f$, and $g: B \hookrightarrow V$ a choice of preimage of our basis of the image, so that $(f\circ g)(b) = b$ for all $b\in B$. I will show that $g(B) \cup A$ is a basis of $V$: For $\vec{v}\in V$ we can write $f(\vec{v}) = \lambda_1 \vec{w}_1 + \cdots + \lambda_r\vec{w}_r$ with $\vec{w}_i \in B$. Obviously, the element $\vec{v}-  \lambda_1 g(\vec{w}_1) - \cdots - \lambda_rg(\vec{w}_r)$ belongs to the kernel of $f$, and so it follows that $g(B) \cup A$ generates all of $V$. In order to show that $g(B) \cup A$
is linearly independent, assume that $$ \lambda_1 g(\vec{w}_1) + \cdots + \lambda_rg(\vec{w}_r) + \mu_1 \vec{v}_1 + \cdots + \mu_s \vec{v}_s  =  \vec{0}$$ where the $\vec{v}_i\in A$ and $\vec{w}_j\in B$ are pairwise distinct. On applying $f$ to this we deduce that $\lambda_1 \vec{w}_1 + \cdots + \lambda_r \vec{w}_r =  \vec{0}$ and so $\lambda_1 = \cdots = \lambda_r = 0$ since the $\vec{w}_j$ are linearly independent. Putting this information back in to the displayed equation shows $\mu_1 = \cdots = \mu_s = 0$ because of the linear independence of the vectors $\vec{v}_i$.

The theorem follows since $|g(B) \cup A| = |B| + |A|$.

\end{proof}

\begin{corollary}[The Dimension Theorem, again] \label{dimagain} Let $V$ be a vector space, and $U,W\subseteq V$ vector subspaces. Then $$\dim (U+W) + \dim(U\cap W) = \dim U + \dim W.$$
\end{corollary}
I already proved this on-the-hoof in \hyperref[dimthm]{Theorem \ref{dimthm}}; with the Rank-Nullity Theorem \hyperref[rnthm]{Theorem \ref{rnthm}} I can give an alternative proof.
\begin{proof}Consider the linear mapping $$f: U \oplus W \to V$$ given by $f(u,w) = u+w$, so that $(\im f) = U+W$ and so that the mapping $k\mapsto (k,-k)$ defines an isomorphism $(U\cap W) \stackrel{\sim}{\to} \ker f$. Then the formula in \hyperref[dimofdirsum]{Exercise \ref{dimofdirsum}} for the dimension of a direct sum, together with the Rank-Nullity Theorem give $$\dim U + \dim W = \dim (U\oplus W) = \dim(U\cap W) + \dim (U+W).$$
\end{proof}

\begin{exercise} Let $f: V\to W$ be a linear mapping. Show that: if $ \vec{v}_1, \ldots , \vec{v}_s$ is a basis of the kernel $\ker f$ and $\vec{v}_{s+1}, \ldots , \vec{v}_n$ an extension to a linearly independent subset of $V$, then the family $f(\vec{v}_{s+1}), \ldots , f(\vec{v}_n)$ is linearly independent in $W$. If the extension is moreover a basis of $V$, then show that $(f(\vec{v}_i))_{s+1\leqslant i \leqslant n}$ is a basis of the image of $f$. Use this to deduce another proof of the Rank-Nullity Theorem in the finite dimensional case.
\end{exercise}

\begin{exercise} \label{compex} Show that: two subspaces $U,W$ of a  vector space $V$ are complementary if and only if $V = U+W$ and $U\cap W =0$.
\end{exercise}

\begin{exercise} Show that: two subspaces $U,W$ of a finite dimensional vector space $V$ are complementary if and only if $V = U+W$ and $\dim U + \dim W \leqslant \dim V$.
\end{exercise}

\begin{exercise} Show that: the kernel of a non-zero linear mapping $V\to F$ is a hyperplane, in the sense of \hyperref[hyperplane]{Exercise \ref{hyperplane}}.
\end{exercise}

\begin{exercise} \label{split} Let $\phi: V\to V$ be an endomorphism of a finite dimensional vector space $V$. Show that $\ker (\phi\circ \phi) = \ker \phi$ is equivalent to $V = \ker \phi \oplus \im \phi.$
\end{exercise}

\begin{exercise}\label{projalong} An element $f$ of a set with composition or product is called an {\bf idempotent} if $f^2 = f$. (Here I've written $f^2$ for $f\circ f$.) An example of a set with composition is ${\rm End}(V)$, the set of endomorphisms of a finite dimensional vector space $V$. By \hyperref[split]{Exercise \ref{split}} the idempotent endomorphisms of $V$ correspond uniquely to decompositions of $V$ into a direct sum of two complementary subspaces. Show that: the mapping $f\mapsto (\im f, \ker f)$ affords a bijection $$\{ f\in {\rm End} V: f^2 = f\} \stackrel{\sim}{\to} \left\{ (I,K) \in \mathcal{P}(V)^2 : I,K\subseteq V \text{ are vector subspaces with }I\oplus K = V\right\}.$$
The inverse of this bijection associates to the decomposition $V = I\oplus K$ the {\bf projection of $V$ onto $I$ along $K$}.
\end{exercise}

\chapter{Linear Mappings and Matrices} \label{chap2}

\section{Linear Mappings $F^m\to F^n$ and Matrices} \label{linmat}
\begin{theorem}[Linear mappings $F^m\to F^n$ and Matrices] \label{linmapmat} Let $F$ be a field and let $m,n\in \mathbb{N}$ be natural numbers. There is a bijection between the space of linear mappings $F^m \to F^n$ and the set of matrices  with $n$ rows and $m$ columns and entries in $F$:
\begin{eqnarray*}{\sf M}: {\rm Hom}_F(F^m, F^n) &\stackrel{\sim}{\to}& {\sf Mat}(n\times m; F) \\ f&\mapsto& [f].\end{eqnarray*} This attaches to each linear mapping $f$ its {\bf representing matrix} ${\sf M}(f) := [f]$. The columns of this matrix are the images under $f$ of the standard basis elements of $F^m$ (\hyperref[standard]{Example \ref{standard}}):$$[f] := ( f({\vec e}_1) | f({\vec e}_2) | \cdots | f({\vec e}_m )).$$
\end{theorem}
\begin{proof}
This is immediate from \hyperref[linmapbas]{Lemma \ref{linmapbas}} which, in this case, shows that every linear mapping $f: F^m\to F^n$ determines and is completely determined by its restriction to the basis $({\vec e}_1, {\vec e}_2, \ldots , {\vec e}_m)$.
\end{proof}

\begin{exercise} Show that: the mapping ${\sf M}$ from \hyperref[linmapmat]{Theorem \ref{linmapmat}} is even an isomorphism of vector spaces, where we use the vector space structures defined in \hyperref[linmapvs]{Exercise \ref{linmapvs}} for linear mappings and \hyperref[matvs]{Exercise \ref{matvs}} for matrices.
\end{exercise}

\begin{ex} \label{kronecker} The representing matrix of the identity mapping on $F^m$ is the {\bf Identity Matrix} $$I = I_m := [\id_{F^m}] = \begin{pmatrix} 1 & & 0 & \\ & 1& & \\ & & \ddots & \\ & 0 & & 1\end{pmatrix}$$ whose entries are $I_{i,j} = \delta_{i,j}$, where $\delta_{i,j}$ is the {\bf Kronecker delta} and is defined generally by $$\delta_{i,j} = \begin{cases} 1 \quad & i=j \\ 0 & \text{otherwise}, \end{cases}$$
\end{ex}

\begin{ex}
When $m\geqslant n$ the matrix representing the mapping which ``forgets the extra coordinates" $f: F^m\to F^n, (x_1, \ldots , x_m) \mapsto (x_1, \ldots , x_n)$ is $$[f] = \begin{pmatrix} 1 & & 0 & &0 \cdots 0 \\ & \ddots & &  & \\ & & \ddots & & \\ & 0 & & 1 & 0\ldots 0\end{pmatrix}$$
\end{ex}
\begin{ex}
The matrix that represents ``permuting the coordinates" $g:  F^2 \to F^2, (x,y) \mapsto (y,x)$ is: $$[g] = \begin{pmatrix} 0 & 1 \\ 1 & 0 \end{pmatrix}$$
\end{ex}

\begin{ex} \label{refl}
Let $f: \mathbb{R}^2 \to \mathbb{R}^2$ be reflection about the straight line making an angle $\alpha$ with the $x$-axis. Then $$[f] = \begin{pmatrix} \cos 2\alpha & \sin 2\alpha \\ \sin 2\alpha & -\cos 2\alpha
\end{pmatrix}$$ Indeed this is obvious from the diagram, because for instance $f(\vec{e}_1) = (\cos 2\alpha)\vec{e}_1 + \sin 2\alpha \vec{e}_2$.
\end{ex}
$$
\begin{tikzpicture}[scale=3]
\colorlet{anglecolor}{red!50!black}
\draw (-1.5,0) -- (1.5,0);
  \draw (0,-1.5) -- (0,1.5);
  \draw (0,1) .. controls (0.555,1) and (1,0.555) .. (1,0) .. controls (1, -0.555) and (0.555,-1) .. (0,-1);
 \filldraw[fill=red!20] (0,0) -- (3mm,0pt) arc(0:30:3mm);
  \draw (15:2mm) node[anglecolor] {$\alpha$};
  \filldraw[rotate=30, fill=red!20] (0,0) -- (3mm,0pt) arc(0:30:3mm);
  \draw[rotate=30] (15:2mm) node[anglecolor] {$\alpha$};
\draw[ultra thick, red, rotate=30] (-1.5,0) -- (1.5,0);
\draw[->, very thick] (0,0) -- (1,0)  node[anchor=north west] {$\vec{e}_1$};
\draw[rotate=60, ->, very thick,blue] (0,0) -- (1,0) node[anchor=south] {$f(\vec{e}_1)$};
\draw[->, very thick] (0,0) -- (0,1)  node[anchor=east] {$\vec{e}_2$};
\draw[rotate=-30, ->, very thick, blue] (0,0) -- (1,0) node[anchor=north west] {$f(\vec{e}_2)$};
\draw[<->, ultra thick, red] (1.1,0.8) .. controls (1.2,0.8) and (1.3,0.7) .. (1.3,0.55) node[anchor=south west] {$f$};
\end{tikzpicture}
$$

\begin{definition}\label{matmult}
Let $n, m, \ell \in \mathbb{N}$, $F$ a field, and let $A\in {\sf Mat}(n\times m ;F)$ and $B\in{\sf Mat}(m\times \ell ;F)$ be matrices. The {\bf product} $A\circ B = AB \in {\sf Mat}(n\times \ell; F)$ is the matrix defined by $$(AB)_{ik} = \sum_{j=1}^m A_{ij}B_{jk}$$ where the entry in the matrix product $AB$ in the $i$-th row and the $k$-th column is calculated from the entries of the matrices $A$ and $B$. Specifically, for each $j$ from $1$ to $m$ multiply the $j$-th entry of the $i$-th row of $A$ by the $j$-th entry of the $k$-th column of $B$, sum all of these $m$ products together, and voila, this is the entry of the product $AB$ in the $i$-th row and $k$-th column. Matrix multiplication produces a mapping \begin{eqnarray*}{\sf Mat}(n\times m; F) \times {\sf Mat}(m\times \ell; F) &\to & {\sf Mat}(n\times \ell; F) \\
(A,B) & \mapsto & AB
\end{eqnarray*}
\end{definition}
\begin{ex} Here is the product of two matrices. The entries in green in the second row of the first matrix and the third column of the second matrix are used to produce the green entry in the second row and third column on the right: $3\times 3 + 4\times 3 = 21$.
$$\begin{pmatrix} 1 &2 \\ {\bf 3}& {\bf 4}\\ 5&6 \end{pmatrix} \circ \begin{pmatrix} 0 &2 & {\bf 3} & 1 \\ 1 & 7 & {\bf 3} & 4 \end{pmatrix} = \begin{pmatrix} 2 & 16 & 9 & 9 \\ 4 & 34 & {\bf 21} & 19 \\ 6 & 52 & 33 & 29 \end{pmatrix}
$$
  \end{ex}

 At first sight the product of two matrices appears simply to be nuts. But the notation $AB = A\circ B$, combined with the connection to composition of mappings presented in \hyperref[linmapmat]{Theorem \ref{linmapmat}} explains it.

\begin{theorem}[Composition of Linear Mappings and Products of Matrices] \label{compmat}Let $g: F^{\ell} \to F^m$ and $f: F^m \to F^n$ be linear mappings. The representing matrix of their composition is the product of their representing matrices: $$[f\circ g] = [f]\circ [g].$$
\end{theorem}
\begin{proof}
Let $(A_{ij})$ be the matrix $[f]$ and $(B_{jk})$ the matrix $[g]$. I will denote the standard bases of $F^n, F^m$ and $F^{\ell}$ by ${\vec u}_i, {\vec v}_j$ and ${\vec w}_k$. I hope this will make the calculation clearer; it would be a mess to use our usual notation of $\vec{ e}_i$ when there are three different standard bases kicking around. In this notation we have
\begin{eqnarray*}
g({\vec w}_k) & = & (B_{\ast k}) = B_{1k}\vec{v}_1 + \cdots + B_{mk}\vec{v}_m \\
f({\vec v}_j) & = & (A_{\ast j})  \hspace{0.05cm} =  A_{1j}{\vec u}_1 + \cdots + A_{nj}{\vec u}_n.
\end{eqnarray*}
From this it follows that
\begin{eqnarray*}
(f\circ g)({\vec w}_k) & = & f( B_{1k}\vec{v}_1 + \cdots + B_{mk}\vec{v}_m ) \\ & = & B_{1k}f({\vec v}_1) + \cdots + B_{mk} f({\vec v}_m) \\ & = & \sum_{j=1}^m B_{jk} f({\vec v}_j) \\ & = & \sum_{j=1}^m B_{jk} \sum_{i=1}^n A_{ij} {\vec u}_i \\ & = & \sum_{i=1}^n \left( \sum_{j=1}^m A_{ij}B_{jk} \right) {\vec u}_i.
\end{eqnarray*}
On the other hand, the entries $(C_{ik})$ of the matrix $[f\circ g]$ are defined by rule $$(f\circ g)({\vec w}_k) = C_{1k}{\vec u}_1 + \cdots + C_{nk}{\vec u}_n.$$ Comparing coefficients in these two ways to calculate $(f\circ g)({\vec w}_k)$ confirms that $C_{ik} = \sum_{j=1}^m A_{ij}B_{jk}.$
\end{proof}

\begin{proposition}[Calculating with Matrices] \label{matmakesring} Let $k, \ell, m, n\in \mathbb{N}$, $A,A'\in {\sf Mat}(n\times m; F), B,B'\in {\sf Mat}(m\times \ell; F), C\in {\sf Mat}(\ell \times k; F)$ and $I = I_m$ the $(m\times m)$-identity matrix. Then the following hold for matrix multiplication:
\begin{eqnarray*}
(A+A')B & = & AB + A'B \\ A(B+B') &=& AB + AB' \\ IB &=& B \\ AI &=& A \\ (AB)C &=& A(BC). \end{eqnarray*}
\end{proposition}
\begin{proof}[First Proof!] I'll do it by bloody-minded calculation and not a smidgeon of thought. But I won't do all the equalities, just the second, third and fifth. You can do the other two for yourself. To the second:
\begin{eqnarray*}
(A(B+B'))_{ik} &=& \sum_{j=1}^m A_{ij}(B+B')_{jk} \\ & = &\sum_{j=1}^m A_{ij}(B_{jk}+B'_{jk}) \\ & = & \sum_{j=1}^m (A_{ij}B_{jk} + A_{ij}B'_{jk})\\ & = & (AB + AB')_{ik}
\end{eqnarray*}
which shows that $A(B+B') = AB + AB'$. For the third: $(IB)_{ik} = \sum_{j} I_{ij}B_{jk} = \sum_j \delta_{ij}B_{jk} = B_{ik}$ and this gives $IB = B$. Now I'll spice up the job of typing by taking $\kappa, \lambda, \mu , \nu$ as indices for the next matrices:
\begin{eqnarray*}
((AB)C)_{\nu\kappa} &=& \sum_{\lambda=1}^{\ell} (AB)_{\nu \lambda}C_{\lambda \kappa} \\ & = & \sum_{\lambda=1}^{\ell} (\sum_{\mu=1}^m A_{\nu \mu}B_{\mu \lambda})C_{\lambda \kappa} \\ & = & \sum_{\mu, \lambda = 1}^{m,\ell} A_{\nu\mu}B_{\mu \lambda} C_{\lambda \kappa} \\
(A(BC))_{\nu \kappa} & = & \sum_{\mu =1}^m A_{\nu \mu}(BC)_{\mu \kappa} \\ & = & \sum_{\mu=1}^m A_{\nu \mu}\left(\sum_{\lambda = 1}^{\ell} B_{\mu \lambda}C_{\lambda \kappa}\right) \\ & = & \sum_{\mu, \lambda = 1}^{m,\ell} A_{\nu\mu}B_{\mu\lambda}C_{\lambda\kappa}
\end{eqnarray*}
and this shows that $(AB)C = A(BC)$.
\end{proof}
\begin{proof}[Second Proof!] I can prove the rules for matrices by applying \hyperref[linmapmat]{Theorem \ref{linmapmat}} and \hyperref[compmat]{Theorem \ref{compmat}}. For instance to show $(AB)C = A (BC)$, I use \hyperref[linmapmat]{Theorem \ref{linmapmat}} to take the corresponding linear mappings $a: F^m \to F^n, b: F^{\ell}
 \to F^m , c: F^k \to F^{\ell}$ such that $A = [a], B = [b], C= [c]$. Then, by \hyperref[compmat]{Theorem \ref{compmat}}:
 \begin{eqnarray*}
 (AB)C = ([a]\circ [b]) \circ [c] = [a\circ b]\circ [c] = [(a\circ b) \circ c] \\
 A(BC) = [a]\circ ([b]\circ [c]) = [a] \circ ([b\circ c]) = [a\circ (b\circ c)]
 \end{eqnarray*}
so that the equality $(AB)C = A(BC)$ follows from the associativity of composition of mappings $(a\circ b) \circ c = a\circ (b\circ c)$.
\end{proof}

\begin{rem}[Linear Mappings $F^m \to F^n$ as Matrix Multiplication] Since we've discussed matrix multiplication properly now, I can show you the inverse of the bijection of \hyperref[linmapmat]{Theorem \ref{linmapmat}}  ${\sf M}: {\rm Hom}_F(F^m, F^n) \stackrel{\sim}{\to} {\sf Mat}(n\times m ; F), f\mapsto [f]$, which attaches a representing matrix to each linear mapping. To do so, I think of the elements of $F^m$ and $F^n$ as column vectors. Then, given a matrix $A\in {\sf Mat}(n\times m; F)$ I consider matrix multiplication $(A\circ) : {\sf Mat}(m\times 1; F) \to {\sf Mat}(n\times 1; F)$, which I choose to write instead as: $$(A\circ) : F^m \to F^n.$$ It follows straight from the definition that the mapping \begin{equation} \label{inversemap} {\sf Mat}(n\times m ; F) \to {\rm Hom}_F(F^m, F^n), A \mapsto (A\circ)\end{equation} is the inverse to the mapping ${\sf M}$ above. {\it You should get to the point where the identification between linear mappings $F^m\to F^n$ and matrices is in your bones.}

By the way, given $x\in F^m$, I'll write  $Ax$ instead of $A\circ x$, just as you are used to.
\end{rem}

\begin{exercise}
Let $f: \mathbb{R}^2 \to \mathbb{R}^2$ be the reflection $(x,y) \mapsto (x,-y)$ as in \hyperref[reflections]{Exercise \ref{reflections}}. Show that: $\{ g\in {\rm Hom}_{\mathbb{R}}(\mathbb{R}^2, \mathbb{R}^2) : f\circ g = g\circ f \}$ is a subspace of $ {\rm Hom}_{\mathbb{R}}(\mathbb{R}^2, \mathbb{R}^2)$ and give a basis of this subspace.
\end{exercise}

\begin{exercise}
Given a matrix $A\in {\sf Mat}(n\times m; F)$ define the {\bf transposed matrix} $A^{\sf T}\in {\sf Mat}(m\times n; F)$ by the rule $(A^{\sf T})_{ij} = A_{ji}$. I refer to that by saying that $A^{\sf T}$ is obtained from $A$ by ``reflection about the main diagonal". For instance, the transposition of a column vector, i.e. an $(n\times 1)$-matrix, is a {\bf row vector}, i.e. a $(1\times n)$-matrix. Show that: $(A^{\sf T})^{\sf T} = A$. Show further that: $(AB)^{\sf T} = B^{\sf T}A^{\sf T}$.
\end{exercise}
$$
\begin{tikzpicture}
  \pgfsetmatrixcolumnsep{1mm}
  \pgfmatrix{rectangle}{center}{mymatrix}
    {\pgfusepath{}}{\pgfpointorigin}{\let\&=\pgfmatrixnextcell}
  {
  \node(a){1}; \& \node{3}; \& \node{5}; \& \node(c){7};  \&[1cm,between origins] \&[1cm,between origins] \node(b){1}; \& \node{2}; \\
 \node{2}; \& \node{4}; \& \node{6}; \& \node{8}; \& \node{=}; \& \node{3}; \& \node{4}; \\
 \& \& \& \& \& \node{5}; \& \node{6}; \\
 \& \& \& \node(b){}; \& \& \node{7}; \& \node{8}; \\
 }
 \draw [dotted,red,ultra thick,every node/.style=] (a.center) -- (b.center);
 \draw[thick] (-2.4,1) .. controls (-2.5,0.5) .. (-2.4,0);
  \draw[thick] (-0.3,1) .. controls (-0.2,0.5) .. (-0.3,0);
  \draw[thick] (1.4,1) ..controls (1.2,0)..(1.4,-1);
    \draw[thick] (2.5,1) ..controls (2.7,0)..(2.5,-1);
    \node [right] at (c.north east) {${\sf T}$};
 \draw[ultra thick, draw = red, <->] (-1.1,-0.8) .. controls (-0.6, -0.7)   ..  (-0.5, -0.3);
\end{tikzpicture}
$$

\section{Basic Properties of Matrices}\label{basicmat}
We've just seen that matrices arise naturally when we think of linear mappings between $F^m$ and $F^n$. In this section I'll recall or introduce several important properties of matrices that we will use in the rest of the course.
\begin{definition}\label{defmatinv}
A matrix $A$ is called {\bf invertible} if there exist matrices $B$ and $C$ such that $BA = I$ and $AC = I$.
\end{definition}
If we think of $A$ as a linear mapping $a: F^m \to F^n$, then \hyperref[compmat]{Theorem \ref{compmat}} shows that $A$ is invertible if and only if the mapping $a$ is an isomorphism. In particular it follows from \hyperref[dimclass]{Theorem \ref{dimclass}} that $n=m$, and so only square matrices can be invertible. For a square matrix $A$ the following conditions are then equivalent:
\begin{enumerate}
\item There exists a square matrix $B$ such that $BA = I$;
\item There exists a square matrix $C$ such that $AC = I$;
\item The square matrix $A$ is invertible.
\end{enumerate}
Let's check this. That (3) implies (1) and (2) follows from the definition of $A$ being invertible. Now suppose that instead that (1) holds. Using \hyperref[compmat]{Theorem \ref{compmat}} we see that the linear mapping $a:F^n \to F^n$ has a left inverse and so is injective. Applying the \hyperref[rnthm]{Rank-Nullity Theorem} then shows that $\dim(\im a) = n$ and so, by \hyperref[dim&contain]{Remark \ref{dim&contain}}, $\im a = F^n$ and $a$ is surjective, and hence an isomorphism. Thus (3) follows. Similarly, assuming that (2) holds, we see the linear mapping $a:F^n \to F^n$ has a right inverse and so is surjective. Applying the \hyperref[rnthm]{Rank-Nullity Theorem} then shows that  $\dim (\ker a) = 0$ so that, by \hyperref[ker=0]{Lemma \ref{ker=0}}, $a$ is injective and hence an isomorphism, proving (3) in this case.

\phantomsection
\label{GLn}
If $A$ is invertible and $a: F^n\stackrel{\sim}{\to} F^n$ is the associated vector space isomorphism, then the matrix $[a^{-1}]$ of the inverse mapping $a^{-1}$ is the unique square matrix $B$ satisfying $BA = I$ and also the unique square matrix satisfying $AB = I$. We denote this matrix by $A^{-1}$ and call it {\bf the inverse matrix of $A$}.

The invertible $(n\times n)$-matrices with entries in the field $F$ form a group under matrix multiplication, called the {\bf general linear group of $(n\times n)$-matrices}, which we write as
\phantomsection
\label{genlin}
\begin{equation*}
{\rm GL}(n; F) := {\sf Mat}(n; F)^{\times}.
\end{equation*} This group was mentioned in the Year 2 course \href{http://www.drps.ed.ac.uk/12-13/dpt/cxmath08064.htm}{Fundamentals of Pure Mathematics}.

\begin{exercise} Show that: there is a group isomorphism between $GL(2; \mathbb{F}_2)$ and $S_3$, where $\mathbb{F}_2$ is the field with two elements $\{ \overline{0}, \overline{1} \}$ and $S_3$ the symmetric group on $3$ letters.
\end{exercise}
\medskip

\noindent
{\bf Systems of Linear Equations.}
Let's go back to our original motivation of solving the following:
\begin{eqnarray*}
      a_{11} x_1 + a_{12}x_2 +  \cdots  + a_{1m}x_m & =  & b_1 \\
      a_{21} x_1 + a_{22}x_2 +  \cdots  + a_{2m}x_m & = & b_2 \\
      \vdots  \hspace{13mm} \vdots \hspace{23mm} \vdots \hspace{5mm} &&  \, \vdots \\
      a_{n1} x_1 + a_{n2}x_2 + \cdots  + a_{nm}x_m & =  & b_n
\end{eqnarray*}
I now write this in our new shorthand notation $$A x = b$$ where the left hand side features the product of the coefficient matrix $A$ with the column vector $x = (x_1, \ldots , x_m)^{\sf T}$ and where the right hand side features the column vector $b = (b_1, \ldots b_n)^{\sf T}$. Solving this system of equations amounts to describing the preimage of $b\in F^n$ of the linear mapping $(A\circ): F^m \to F^n$. In particular, the solution set of the associated homogenised system of equations is the preimage of $\vec{0}\in F^n$, in other words it is the kernel of $(A\circ): F^m \to F^n$.

The operations of \hyperref[opGE]{Gaussian elimination} can also be interpreted in this language: let $$E_{ij}$$ denote the {\bf basis matrix} with the entry $1$ in the $i$-th row and $j$-th column and $0$'s everywhere else. For $i\neq j$, the rule [{\it Row Addition}] asks to add $\lambda$-times the $j$-th row to the $i$-th row, and in matrix notation this can be written as $$(I + \lambda E_{ij})Ax = (I + \lambda E_{ij})b$$ Since $(I-\lambda E_{ij})(I+\lambda E_{ij}) = I$, it's clear that this new system of equations has exactly the same solutions as the original system. Similarly, for $i\neq j$ let $P_{ij}$ denote the matrix corresponding to the linear mapping $F^m \to F^m$ that swaps the $i$-th coordinates with the $j$-th coordinates and leaves everything else alone. Then [{\it Row Swap}] can be described in matrix notation as $$P_{ij} Ax = P_{ij}b.$$ Since $P_{ij}P_{ij} = I$ it is again clear that the new system of equations has the same solutions as the original system.

\begin{definition}
I will define an {\bf elementary matrix} to be any square matrix that differs from the identity matrix in at most one entry.
\end{definition}
All the elementary matrices with entries in a field are, with the exception of those where you take one $1$ in the identity matrix and replace it by $0$, invertible. Be warned, dear Reader, that this definition of elementary matrix differs from the \href{http://en.wikipedia.org/wiki/Elementary_matrix}{Wikipedia entry}, but the world is indeed complicated and there is not universal agreement on the definition of an elementary matrix.

\begin{theorem}\label{elmove} Every square matrix with entries in a field can be written as a product of elementary matrices.
\end{theorem}

\begin{proof}
First, I want to write the permutation matrix $P_{ij}$ as a product of elementary matrices, and I do it as follows:
$$P_{ij} = {\sf diag}(1,\ldots , 1, -1, 1, \ldots , 1) (I+E_{ij})(I-E_{ji})(I+E_{ij})$$ In this equation you should put the $(-1)$ in the $j$-th place and you should know that ${\sf diag}(\lambda_1, \ldots , \lambda_n)$ denotes the {\bf diagonal matrix} with entries $a_{ij} = 0$ for $i\neq j$ and $a_{ii} = \lambda_i$.

Next I assert that the inverse of each invertible elementary matrix is again an elementary matrix. You saw that already when I wrote $(I-\lambda E_{ij}) (I+\lambda E_{ij}) = I$.

Now let $A$ be an arbitrary square matrix. By Gaussian elimination we can find invertible elementary matrices $S_1, \ldots , S_t$ such that $S_t \cdots S_1 A$ has echelon form. It's easy to see that multiplying on the right by invertible elementary matrices corresponds to applying column operations, by which I mean the addition of a multiple of one column to another or the swap of two columns or the multiplication of one column by a non-zero scalar. These operations can be applied to bring the matrix into the form ${\sf diag}(1,\ldots , 1, 0 , \ldots 0)$ and so we deduce that there exists elementary matrices $T_1, \ldots , T_s$ such that $S_t\cdots S_1 A T_1\cdots T_s  = {\sf diag}(1,\ldots , 1, 0 , \ldots 0)$. We can write this diagonal matrix as a product of non-invertible diagonal elementary matrices $D_1, \ldots , D_r$, giving $S_t\cdots S_1 A T_1\cdots T_s = D_1 \cdots D_r$. From this it follows that $$A =  S_1^{-1}\cdots S_t^{-1} D_1\cdots D_r T_s^{-1}\cdots T_1^{-1}$$
\end{proof}
This proof proves more: for a fixed $n$ there exists an $N$ such that each $(n\times n)$-matrix can be written as a product of at most $N$ elementary matrices.

\begin{definition}\label{smithform}
Any matrix whose only non-zero entries lie on the diagonal, and which has first $1$'s along the diagonal and then $0$'s, is said to be in {\bf Smith Normal Form}.
$$
  \begin{pmatrix}
           \hlight{1} &  0    &   0    & 0  & 0  & 0  &  0 \\
          0 &   \hlight{1}   &  0    & 0 &    0    &    0    & 0 \\
           0 &  0   &   \hlight{1}    & 0 & 0 & 0       & 0 \\
           0 & 0 & 0 & 0 & 0 & 0 & 0 \\
             0 & 0 & 0 & 0 & 0 & 0 & 0 \\
   \end{pmatrix}
$$
\end{definition}
\begin{theorem}[Transformation of a Matrix into Smith Normal Form] \label{smith}
For each matrix $A\in {\sf Mat}(n\times m; F)$ there exist invertible matrices $P$ and $Q$ such that $PAQ$ is a matrix in Smith Normal Form.
\end{theorem}
\begin{proof}
Just argue as in the proof of \hyperref[elmove]{Theorem \ref{elmove}}: use Gaussian elimination to first find invertible elementary matrices $S_1, \ldots , S_t$ such that $S_t \cdots S_1 A$ is in echelon form; then find invertible elementary matrices $T_1, \ldots , T_s$ to perform column operations such that $S_t\cdots S_1AT_1\cdots T_s$ is in Smith Normal Form. Then $P = S_t\cdots S_1$ and $Q = T_1\cdots T_s$.
\end{proof}

\begin{definition}
The {\bf column rank} of a matrix $A\in {\sf Mat}(n\times m; F)$ is the dimension of the subspace of $F^n$ generated by the columns of $A$. Similarly, the {\bf row rank} of $A$ is the dimension of the subspace of $F^m$ generated by the rows of $A$.
\end{definition}

\begin{exercise} Show that: the column rank of a matrix $A\in {\sf Mat}(n\times m; F)$ equals the rank of the linear mapping $(A\circ) : F^m \to F^n$. Hence deduce that the Rank-Nullity theorem you met in \href{http://www.drps.ed.ac.uk/12-13/dpt/cxmath08057.htm}{Introduction to Linear Algebra} is the special case of \hyperref[rnthm]{Theorem \ref{rnthm}} where $V = F^m, W= F^n$ and $f = (A\circ)$.
\end{exercise}

\begin{theorem}
The column rank and the row rank of any matrix are equal.
\end{theorem}

\begin{proof}
We can interpret the column rank of the matrix $A\in {\sf Mat}(n\times m; F)$ as the dimension of the image of $$(A\circ): F^m\to F^n$$ This interpretation shows immediately that $PAQ$ has the same column rank as $A$ whenever $P$ and $Q$ are invertible matrices. Furthermore, by transposing our matrices, we then also know that $PAQ$ and $A$ have the same row rank.

By \hyperref[smith]{Theorem \ref{smith}}, we can find invertible matrices $P,Q$ such that $PAQ$ is in Smith Normal Form. But it is obvious that the column rank and the row rank are equal for a matrix in Smith Normal Form, since they both equal the number of non-zero entries on the diagonal. By the above paragraph, the same holds true for $A$ itself.
\end{proof}

\begin{definition} \label{fullrank}
I will drop the tag ``column" or ``row" now and just refer to  the {\bf rank of a matrix} and write it as ${\rm rk} A$. When the rank is as big as possible, meaning that it is equal either to the number of rows or number of columns, whichever is smaller, then the matrix has {\bf full rank}.
\end{definition}

\begin{exercise}
Find a $(3\times 3)$-matrix with all entries non-zero integers but which has rank $2$.
\end{exercise}
\medskip

\noindent
{\bf Inverting Matrices.} There is a simple procedure to calculate the inverse of an invertible $(n\times n)$-matrix $A$: write the identity matrix $I$ next to it, thereby producing an $(n\times 2n)$-matrix $(A|I)$. Apply elementary row operations, including multiplying a row by a non-zero scalar, in order to bring $A$ into echelon form, and then possibly further row operations to bring it into reduced echelon form: this will actually be the identity matrix. The inverse to $A$ is then what is standing in the right half of the $(n\times 2n)$-matrix. To see why this is so, suppose that the elementary operations you carried out correspond to left-multiplication by the elementary matrices $S_1, S_2, \ldots , S_t$. Then the $(n\times 2n)$-matrix becomes $$(S_t\cdots S_2S_1A | S_t\cdots S_2S_1I)$$
When you reach the equality $S_t\cdots S_2S_1A = I$, i.e. when you have brought $A$ into reduced echelon form, then by multiplying both sides of this equality by $A^{-1}$ you see that $$S_t\cdots S_2 S_1 I = S_t\cdots S_2 S_1A A^{-1} = IA^{-1} = A^{-1}$$ and so the matrix in the second half is indeed $A^{-1}$.

\section{Abstract Linear Mappings and Matrices}
In \hyperref[linmat]{Section \ref{linmat}} you saw that linear mappings $F^m\to F^n$ were the same as $(n\times m)$-matrices over $F$. By \hyperref[lincombiso]{Theorem \ref{lincombiso}}, choosing an ordered basis for a finite dimensional $F$-vector space $V$ produces an isomorphism between $F^{n}$ and $V$ where $n= \dim V$. Using this it is therefore clear that linear mappings $V\to W$ between abstract vector spaces with given ordered bases can also be represented by matrices.

The only danger in representing linear mappings by matrices is keeping track of which ordered bases are being used. But a notation has been invented for this, and similarly to the electricians' colour-coding for the wiring of plugs, this notation makes it impossible for a mathematician to err.

\begin{theorem}[Abstract Linear Mappings and Matrices] \label{abslinmap}
Let $F$ be a field, $V$ and $W$ vector spaces over $F$ with ordered bases $\mathcal{A} = ( \vec{v}_1, \ldots , \vec{v}_m)$ and $\mathcal{B} = (\vec{w}_1, \ldots , \vec{w}_n)$. Then to each linear mapping $f: V\to W$ we associate a {\bf representing matrix} ${}_{\mathcal{B}}[f]_{\mathcal{A}}$ whose entries $a_{ij}$ are defined by the identity
$$f({\vec v}_j)~ =~ a_{1j} \vec{w}_1 + \cdots + a_{nj} \vec{w}_n\in W~.$$ This produces a bijection, which is even an isomorphism of vector spaces: \begin{eqnarray*}{\sf M}_{\mathcal{B}}^{\mathcal{A}}: {\rm Hom}_F(V,W)&\stackrel{\sim}{\to}& {\sf Mat}(n\times m; F)  \\ f & \mapsto & {}_{\mathcal{B}}[f]_{\mathcal{A}}\end{eqnarray*}
\end{theorem}
We call ${\sf M}_{\mathcal{B}}^{\mathcal{A}}(f) = {}_{\mathcal{B}}[f]_{\mathcal{A}}$ the {\bf representing matrix of the mapping $f$ with respect to the bases $\mathcal{A}$ and $\mathcal{B}$}. The columns of this matrix give the coordinates of the image of a basis vector from $\mathcal{A}$ with respect to the basis $\mathcal{B}$ . If $V$ is $m$-dimensional and $W$ is $n$-dimensional then ${\sf M}_{\mathcal{B}}^{\mathcal{A}}(f)=(a_{ij})$ is an $n \times m$ matrix, i.e. has $n$ rows and $m$ columns.


If $f : F^m\to F^n$ is a linear mapping, we could work with the standard bases  from \hyperref[standard]{Example \ref{standard}} for $F^m$ and $F^n$ . I will label these by $\mathcal{S}(m)$ and $\mathcal{S}(n)$. A look at the definitions shows that the representing matrix $[f]$ of \hyperref[linmapmat]{Theorem \ref{linmapmat}} reappears:
$$ [f] = {}_{\mathcal{S}(n)}[f]_{\mathcal{S}(m)}.$$ Because of this, I'll usually omit reference to the standard bases, and continue to write $[f]$ instead of ${}_{\mathcal{S}(n)}[f]_{\mathcal{S}(m)}$. Moreover, for a linear mapping $f: F^m \to W$ I will abbreviate ${}_{\mathcal{B}}[f]_{\mathcal{S}(m)}$ by ${}_{\mathcal{B}}[f]$, and for $f:V\to F^n$ I will abbreviate ${}_{\mathcal{S}(n)}[f]_{\mathcal{A}}$ by $[f]_{\mathcal{A}}$.

\begin{proof}
One way to do this would be to write out a variation of the proof of \hyperref[linmapmat]{Theorem \ref{linmapmat}}, but as I mentioned at the beginning of this section there is a better way. Recall the isomorphisms $\Phi_{\mathcal{A}}: F^m \stackrel{\sim}{\to} V$ and $\Phi_{\mathcal{B}}: F^n \stackrel{\sim}{\to} W$ of \hyperref[lincombiso]{Theorem \ref{lincombiso}}. Then the representing matrix is, by definition, $$ {}_{\mathcal{B}}[f]_{\mathcal{A}} = [ \Phi_{\mathcal{B}}^{-1}\circ f \circ \Phi_{\mathcal{A}}].$$ From this it follows that we can write our mapping in the theorem as a composition of bijections:
\begin{eqnarray*}
 {\rm Hom}_F(V,W) & \stackrel{\sim}{\to} & {\rm Hom}_F(F^m, F^n) \stackrel{\stackrel{{\sf M}}{\sim}}{\to} {\sf Mat}(n\times m; F) \\
 f & \mapsto & \Phi_{\mathcal{B}}^{-1}\circ f \circ \Phi_{\mathcal{A}},
\end{eqnarray*}
where ${\sf M}: g\mapsto [g]$ of \hyperref[linmapmat]{Theorem \ref{linmapmat}} is our mapping that attaches to each linear mapping $g:F^m\to F^n$ its representing matrix.
\end{proof}


\begin{theorem}[The Representing Matrix of a Composition of Linear Mappings] \label{compabs} Let $F$ be a field and $U,V,W$ finite dimensional vector spaces over $F$ with ordered bases $\mathcal{A}, \mathcal{B}, \mathcal{C}$.   If $f:U \to V$ and $g: V\to W$ are linear mappings, then the representing matrix of the composition $g\circ f : U \to W$ is the matrix product of the representing matrices of $f$ and $g$:
$${}_{\mathcal{C}}[g\circ f]_{\mathcal{A}} = {}_{\mathcal{C}}[g]_{\mathcal{B}} \circ {}_{\mathcal{B}}[f]_{\mathcal{A}}$$
\end{theorem}
\begin{proof}[First Proof!]
First unpack the notation: $${}_{\mathcal{C}}[g\circ f]_{\mathcal{A}} = [\Phi_{\mathcal{C}}^{-1}\circ (g\circ f) \circ \Phi_{\mathcal{A}}] \quad \text{and} \quad  {}_{\mathcal{C}}[g]_{\mathcal{B}} \circ {}_{\mathcal{B}}[f]_{\mathcal{A}}  =  [\Phi_{\mathcal{C}}^{-1}\circ g \circ \Phi_{\mathcal{B}}]\circ [\Phi_{\mathcal{B}}^{-1}\circ  f \circ \Phi_{\mathcal{A}}];$$ then apply \hyperref[compmat]{Theorem \ref{compmat}}.
\end{proof}
\begin{proof}[Second Proof!]
Copy the proof of \hyperref[compmat]{Theorem \ref{compmat}}, but this time interpreting $\vec{u}_i, \vec{v}_j$ and $\vec{w}_k$ there as the vectors of our ordered bases $\mathcal{A}, \mathcal{B}$ and $\mathcal{C}$.
\end{proof}

\begin{definition}
Let $V$ be a finite dimensional vector space with an ordered basis $\mathcal{A} = (\vec{v}_1, \ldots , \vec{v}_m)$. We will denote the inverse to the bijection of \hyperref[lincombiso]{Theorem \ref{lincombiso}} $\Phi_{\mathcal{A}}: F^m \stackrel{\sim}{\to} V, (\alpha_1, \ldots ,\alpha_m)^{\sf T} \mapsto \alpha_1\vec{v}_1+\cdots + \alpha_m\vec{v}_m$ by $$\vec{v} \mapsto {}_{\mathcal{A}}[\vec{v}]$$ The column vector ${}_{\mathcal{A}}[\vec{v}]$ is called the {\bf representation of the vector $\vec{v}$ with respect to the basis $\mathcal{A}$}.
\end{definition}

\begin{theorem}[Representation of the Image of a Vector]\label{rep} Let $V,W$ be finite dimensional vector spaces over $F$ with ordered bases $\mathcal{A}, \mathcal{B}$ and let $f: V\to W$ be a linear mapping. The following holds for $\vec{v}\in V$:$${}_\mathcal{B}[f(\vec{v})] = {}_\mathcal{B}[f]_{\mathcal{A}} \circ {}_{\mathcal{A}}[\vec{v}]$$
\end{theorem}

\begin{proof}
First unpack the notation: $${}_\mathcal{B}[f(\vec{v})] = \Phi_{\mathcal{B}}^{-1} (f (\vec{v})) \quad \text{and} \quad {}_\mathcal{B}[f]_{\mathcal{A}} \circ {}_{\mathcal{A}}[\vec{v}] = [\Phi_\mathcal{B}^{-1}\circ f \circ \Phi_{\mathcal{A}}] \circ \Phi_\mathcal{A}^{-1} (\vec{v}),$$ then apply \hyperref[inversemap]{Equation \eqref{inversemap}}.
\end{proof}

Let $\mathcal{A}=(\vec{v}_1,\vec{v}_2,\dots,\vec{v}_m)$ and
$\mathcal{B}=(\vec{w}_1,\vec{w}_2,\dots,\vec{w}_n)$, and let
${}_\mathcal{B}[f]_{\mathcal{A}}=(a_{ij})$ be the $n \times m$ matrix
of the coefficients in
$$ f(\vec{v}_j)~=~\sum\limits^n_{i=1}a_{ij}\vec{w}_i \in W~.$$
For an arbitrary $\vec{v} \in V$ the expression of $\vec{v}$ as a linear combination of the $\vec{v}_j$'s
$$\vec{v}~=~\sum\limits^m_{j=1}x_j \vec{v}_j \in V$$
with coefficients $(x_1,x_2,\dots,x_m) \in F^m$ is transformed by $f$ into
an expression of $f(\vec{v}) \in W$ as a linear combination of the $\vec{w}_i$'s
$$\begin{array}{ll}
f(\vec{v})&=~\sum\limits^m_{j=1}x_j f(\vec{v}_j)\\[1ex]
&=~\sum\limits^m_{j=1}x_j (\sum\limits^n_{i=1}a_{ij}\vec{w}_i)\\[1ex]
&=~\sum\limits^n_{i=1}(\sum\limits^m_{j=1}a_{ij}x_j)\vec{w}_i \\[1ex]
&=~y_1 \vec{w}_1 + \cdots + y_n \vec{w}_n\in W
\end{array}$$
with coefficients $(y_1,y_2,\dots,y_n) \in F^n$, where
$$y_i~=~\sum\limits^m_{j=1}a_{ij}x_j~,~\hbox{or equivalently as the matrix product}~
\begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{pmatrix}~=~
(a_{ij}) \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_m \end{pmatrix} \in F^n~,$$
confirming Theorem \ref{rep}.
Note that it is necessary to write the coefficients as column vectors. (Actually,
it is also possible to write them as row vectors, but then the transformation rule is given by the matrix product
$$(y_1~y_2~\dots~y_n)~=~(x_1~x_2~\dots~x_m)(a_{ij})^T$$
with $(a_{ij})^T$ the transpose $m \times n$ matrix).

\begin{ex} \label{refl2} (Cf. \hyperref[refl]{Example \ref{refl}}.)
Let $f: \mathbb{R}^2 \to \mathbb{R}^2$ be reflection about the straight line making an angle $\alpha$ with the $x$-axis. Let $\mathcal{A} = ( \vec{v}_1, \vec{v}_2 )$ be the ordered basis of $\mathbb{R}^2$ where $\vec{v}_1 = (\cos \alpha,\sin \alpha)^{\sf T}$ and $\vec{v} = (-\sin \alpha, \cos \alpha)^{\sf T}$. Then $${}_\mathcal{A}[f]_{\mathcal{A}} = \begin{pmatrix} 1 & 0 \\ 0 & -1 \end{pmatrix}$$ This is clear because, when you look at the picture you'll see that $f(\vec{v}_1) = \vec{v}_1$ and $f(\vec{v}_2) = -\vec{v}_2$.
\end{ex}
$$
\begin{tikzpicture}[scale=3]
\colorlet{anglecolor}{red!50!black}
\draw (-1.5,0) -- (1.5,0);
  \draw (0,-1.5) -- (0,1.5);
 \draw (-1,0) .. controls (-1,0.555) and (-0.555,1) .. (0,1);
  \draw (0,1) .. controls (0.555,1) and (1,0.555) .. (1,0) .. controls (1, -0.555) and (0.555,-1) .. (0,-1);
 \filldraw[fill=red!20] (0,0) -- (3mm,0pt) arc(0:30:3mm);
  \draw (15:2mm) node[anglecolor] {$\alpha$};
\draw[ultra thick, red, rotate=30] (-1.5,0) -- (1.5,0);
\draw[rotate=30, ->, very thick,blue] (0,0) -- (1,0) node[anchor=north west, fill=white] {$\textcolor{black}{\vec{v}_1} \textcolor{black}{=} f(\vec{v}_1)$};
\draw[rotate=30, ->, very thick] (0,0) -- (0,1)  node[anchor=east, fill=white] {$\vec{v}_2$};
\draw[rotate=-60, ->, very thick, blue] (0,0) -- (1,0) node[anchor=north west] {$f(\vec{v}_2)$};
\draw[<->, ultra thick, red] (1.1,0.8) .. controls (1.2,0.8) and (1.3,0.7) .. (1.3,0.55) node[anchor=south west] {$f$};
\end{tikzpicture}
$$

\section{Change of a Matrix by Change of Basis}
\begin{definition}
Let $\mathcal{A} = (\vec{v}_1, \ldots , \vec{v}_n)$ and $\mathcal{B} = (\vec{w}_1 , \ldots , \vec{w}_n)$ be ordered bases of the same $F$-vector space $V$. Then the matrix representing the identity mapping with respect to these bases $${}_{\mathcal{B}} [\id_V]_{\mathcal{A}}$$ is called a {\bf change of basis matrix}. By definition, its entries are given by the equalities ${\vec v}_j = \sum_{i=1}^n a_{ij} {\vec w}_i$.
\end{definition}

\begin{ex} (Cf. \hyperref[refl]{Example \ref{refl}} and \hyperref[refl2]{Example \ref{refl2}}.)\label{refl3}
The change of basis matrix on $\mathbb{R}^2$ for the $\mathcal{A} = (\vec{e}_1, \vec{e}_2)$ the standard basis and $\mathcal{B} = (\vec{v}_1 , \vec{v}_2)$ the basis of \hyperref[refl2]{Example \ref{refl2}} is given by $${}_\mathcal{B}[\id]_{\mathcal{A}} = \begin{pmatrix} \cos \alpha &  \sin \alpha \\ - \sin \alpha & \cos \alpha \end{pmatrix}.$$
\end{ex}

\begin{theorem}[Change of Basis] \label{changeofbasis}
Let $V$ and $W$ be finite dimensional vector spaces over $F$ and let $f:V \to W$ be a linear mapping. Suppose that $\mathcal{A}, \mathcal{A'}$ are ordered bases of $V$ and $\mathcal{B}, \mathcal{B'}$ are ordered bases of $W$. Then $${}_\mathcal{B'}[f]_{\mathcal{A'}} = {}_\mathcal{B'}[\id_W]_{\mathcal{B}}\circ {}_\mathcal{B} [f]_{\mathcal{A}} \circ {}_{\mathcal{A}} [\id_V]_{\mathcal{A'}}$$
\end{theorem}
\begin{proof} This follows immediately from \hyperref[compabs]{Theorem \ref{compabs}} and the blindingly obvious equality $f  = \id_W \circ f \circ \id_V$.
\end{proof}

\begin{corollary} \label{cobcor}
Let $V$ be a finite dimensional vector space and let $f:V\to V$ be an endomorphism of $V$. Suppose that $\mathcal{A}, \mathcal{A'}$ are ordered bases of $V$. Then $${}_\mathcal{A'}[f]_{\mathcal{A'}} = {}_\mathcal{A}[\id_V]^{-1}_{\mathcal{A'}}\circ {}_\mathcal{A} [f]_{\mathcal{A}} \circ {}_{\mathcal{A}} [\id_V]_{\mathcal{A'}}$$
\end{corollary}

\begin{proof}
Obviously ${}_{\mathcal{A}}[\id_V]_{\mathcal{A}} = I_n$ is the identity matrix.   \hyperref[compabs]{Theorem \ref{compabs}} then gives the equality $$ {}_{\mathcal{A}} [\id_V]_{\mathcal{A'}} \circ {}_{\mathcal{A'}} [\id_V]_{\mathcal{A}} = I_n$$ from which it follows that ${}_{\mathcal{A}} [\id_V]_{\mathcal{A'}}$ is the inverse matrix of ${}_{\mathcal{A'}} [\id_V]_{\mathcal{A}}$, that is ${}_{\mathcal{A'}} [\id_V]_{\mathcal{A}}^{-1} = {}_{\mathcal{A}} [\id_V]_{\mathcal{A'}}$. The result now follows as a special case of \hyperref[changeofbasis]{Theorem \ref{changeofbasis}}.
\end{proof}

\begin{exercise} Check that this corollary agrees with the calculations made in Examples \ref{refl}, \ref{refl2} and \ref{refl3}.
\end{exercise}

I will write this out a little more explicitly: suppose that $N = {}_\mathcal{B}[f]_{\mathcal{B}}$ and $M = {}_{\mathcal{A}}[f]_{\mathcal{A}}$, then \begin{equation} \label{conjforcob} N = T^{-1}MT\end{equation} where $T = {}_{\mathcal{A}}[\id_V]_{\mathcal{B}}.$

\begin{exercise}
Let $V$ be an $F$-vector space with ordered basis $(\vec{v}_1, \ldots , \vec{v}_n)$. Show that: change of basis matrices provide a bijection \begin{eqnarray*} \{ \text{ordered bases of $V$} \} &\stackrel{\sim}{\to}& {\rm GL}(n; F) \\ \mathcal{B} &\mapsto & {}_{\mathcal{B}}[\id_V]_{\mathcal{A}} \end{eqnarray*} where $GL(n; F)$ is the group of invertible $(n\times n)$-matrices, as discussed in the \hyperref[GLn]{previous section}. Which matrices are the images of the ordered bases obtained from $(\vec{v}_1, \ldots , \vec{v}_n)$ by reordering?
\end{exercise}

\begin{theorem}[Smith Normal Form] Let $f:V\to W$ be a linear mapping between finite dimensional $F$-vector spaces. There exist an ordered basis $\mathcal{A}$ of $V$ and an ordered basis $\mathcal{B}$ of $W$ such that the representing matrix ${}_{\mathcal{B}}[f]_{\mathcal{A}}$ has zero entries everywhere except possibly on the diagonal, and along the diagonal there are $1$'s first, followed by $0$'s.
\end{theorem}

\begin{proof}
The key step is the claim proved in \hyperref[rnthm2ndpara]{the second paragraph of the proof of Theorem \ref{rnthm}}. There I showed that if I choose an ordered basis $(\vec{w}_1, \ldots , \vec{w}_r)$ of the image of $f$, and an ordered basis $(\vec{k}_1, \ldots , \vec{k}_s)$ of the kernel of $f$, then I can find a family of linearly independent vectors $(\vec{v}_1, \ldots , \vec{v}_r)$ such that $f(\vec{v}_i) = \vec{w}_i$ and such that $\mathcal{A} = (\vec{v}_1, \ldots , \vec{v}_r, \vec{k}_1, \ldots , \vec{k}_s)$ is an ordered basis of $V$.

Given the above data, I extend $(\vec{w}_1, \ldots , \vec{w}_r)$ to an ordered basis $\mathcal{B} = (\vec{w}_1, \ldots , \vec{w}_r, \vec{w}_{r+1}, \ldots , \vec{w}_n)$ of $W$. Then it is immediate that the matrix ${}_{\mathcal{B}}[f]_{\mathcal{A}}$ has the required form, where there are $r$ $1$'s on the diagonal.
\end{proof}

\begin{exercise}\label{nilpex}
An endomorphism $f:V\to V$ of an $F$-vector space is called {\bf nilpotent} if and only if there exists $d\in \mathbb{N}$ such that $f^d = 0$. Suppose that $f: V\to V$ is a nilpotent endomorphism of a finite dimensional vector space. Show that: the vector space $V$ has an ordered basis $\mathcal{A}$ such that the representing matrix ${}_{\mathcal{A}}[f]_{\mathcal{A}}$ of $f$ with respect to this basis has the form of an upper triangular matrix with only $0$'s along the diagonal. Show that: conversely, any $(n\times n)$-matrix $M$ that is upper triangular and has only $0$'s along the diagonal satisfies $M^{n} = 0$.
\end{exercise}
This exercise is closely related to the material of \hyperref[JNF]{Chapter \ref{JNF}} on the Jordan Normal Form.

\begin{definition}
The {\bf trace} of a square matrix is defined to be the sum of its diagonal entries. We denote this by $${\rm tr}(A)$$
\end{definition}

\begin{exercise}\label{traceisatrace}
Let $A$ be an $(n\times m)$-matrix and $B$ an $(m\times n)$-matrix, both with entries in the field $F$. Show that: ${\rm tr}(AB) = {\rm tr}(BA)$.
\end{exercise}

Taking $A = T^{-1}M$ and $B = T$ in \hyperref[traceisatrace]{Exercise \ref{traceisatrace}} it follows that:\begin{equation} \label{conjtr}{\rm tr}(T^{-1}MT) = {\rm tr}(M)\end{equation} for any $(n\times n)$-matrix $M$ and invertible $(n\times n)$-matrix $T$. Therefore, any endomorphism $f:V\to V$ of a finite dimensional $F$-vector space has a {\bf trace}, written \begin{equation*} {\rm tr}(f) = {\rm tr}(f|V) = {\rm tr}_F(f|V)\end{equation*} where we use the different notations to emphasise the vector space $V$ or even the ground field $F$ whenever necessary. This is defined by choosing first an ordered basis $\mathcal{A}$ of $V$ and then setting ${\rm tr}(f) = {\rm tr}({}_\mathcal{A}[f]_\mathcal{A}).$ Thanks to \hyperref[conjforcob]{\eqref{conjforcob}} and \hyperref[conjtr]{\eqref{conjtr}} this definition of ${\rm tr}(f)$ is independent of the choice of ordered basis.

\begin{exercise} Let $f:V \to W$ and $g:W\to V$ be two linear mappings where $V$ and $W$ are both finite dimensional $F$-vector spaces. Show that: ${\rm tr}(fg) = {\rm tr}(gf)$.
\end{exercise}

\begin{exercise}
Let $V$ be a finite dimensional $F$-vector space and let $f:V\to V$ be an idempotent, that is $f^2 = f$. Show that ${\rm tr}(f) = \dim (\im f)$.
\end{exercise}

\begin{exercise}
Let $V$ be a finite dimensional $F$-vector space and $f:V \to V$ a linear mapping. Show that $${\rm tr}((f\circ )|{\rm End}_F(V)) = (\dim_F V){\rm tr}(f|V)$$
\end{exercise}
\chapter{Rings and Modules}
In this chapter I will introduce rings and modules, new algebraic structures that you may not have seen before. They generalise fields and vector spaces, and in order to understand and develop the more sophisticated properties of linear algebra it is important to use these notions. As well as presenting definitions and examples, I'll also take the chance to introduce a new technique called taking quotients which will be useful for the rest of your mathematical career, whether that's two more years or a happy lifetime.

\section{Rings}
In the previous chapters, I've been vague about what exactly a field is. I stressed that you already knew: not only did you have lots of excellent examples hardwired, such as $\mathbb{R}, \mathbb{Q}$ and $\mathbb{C}$, but it was easy to pick-up new examples, such as $\mathbb{F}_3$. From the point of view of linear algebra, fields all seem quite similar and so I decided we'd gain little by studying them in that context. Hence the vagueness.

Well, you already know rings, in fact more so. For instance, the integers $\mathbb{Z}$ form a ring and so do polynomials $\mathbb{C}[X]$, the set of $(n\times n)$-matrices ${\sf Mat}(n; F)$ is a ring, and every field is a ring. But, dear Reader, you are officially now warned: rings are a zoo! And not just any zoo. They are lots and lots of wild animals within. So to proceed, you need bravery and a definition; then we can investigate.

\begin{definition}
\label{ringdef}
A {\bf ring} is a set with two operations $(R,+, \cdot)$ that satisfy:
\begin{enumerate}
\item $(R,+)$ is an abelian group;
\item $(R,\cdot)$ is a {\bf monoid}; this means that the second operation $\cdot : R\times R \to R$ is associative and that there is an {\bf identity element} $1 = 1_R \in R$, often called just the {\bf identity}, with the property that $1\cdot a = a\cdot 1 = a$ for all $a\in R$.
\item The distributive laws hold, meaning that for all $a,b,c\in R$ \begin{eqnarray*} a\cdot (b+c) &=& (a\cdot b) + (a\cdot c) \\ (a+b)\cdot c & = & (a\cdot c) + (b\cdot c).\end{eqnarray*}
\end{enumerate}
The two operations are called {\bf addition} and {\bf multiplication} in our ring. A ring in which  multiplication is commutative, that is in which $a\cdot b = b\cdot a$ for all $a,b \in R$, is a {\bf commutative ring}.
\end{definition}
The element $1\in R$ is uniquely determined as the identity element of the monoid $(R, \cdot)$. The additive identity of $(R,+)$ is called zero, and written either as $0_R$ or just $0$ when it's clear which ring I am referring to. I will very quickly forget to write the operation $\cdot$, writing instead $a\cdot b = ab$. Thus, for instance, I will write the first distributive law as $a(b+c) = ab + ac$.

In some reference books the definition of a ring is slightly different. There, instead of $(R,\cdot)$ being a monoid, it is taken only to be {\bf semigroup}, which means that multiplication is associative but there may not exist an identity element. People who use this definition, call \hyperref[ringdef]{Definition \ref{ringdef}} above a {\bf unital ring}. You should know, however, that \hyperref[ringdef]{Definition \ref{ringdef}} is by far the most common.

\begin{ex}
Just as there is the extreme \hyperref[zerovs1]{example of the zero vector space}, so too is there the {\bf null ring} or {\bf zero ring}. Here $R$ is a single element set, say $\{ 0 \}$, with the operations $0 + 0 = 0$ and $0\times 0 = 0$. I'll call any ring that is not the zero ring a {\bf non-zero ring}. Many results will start with something like ``Let $R$ be a non-zero ring, then ...": this is because the zero ring is a degenerate example of a ring with degenerate properties which are just dull and should be ruled out.
\end{ex}

\begin{ex} The integers $\mathbb{Z}$ form a commutative ring under usual addition and multiplication. If $R$ is a ring and $X$ a set, then the set ${\rm Maps}(X,R)$ form a ring under pointwise addition and multiplication, compare with \hyperref[matvs]{Exercise \ref{matvs}}. If $R$ is a ring and $n\in \mathbb{N}$ then the set of $(n\times n)$-matrices with entries in $R$ form a ring, ${\sf Mat}(n; R)$ under the usual operations of matrix addition and matrix multiplication, see \hyperref[matmakesring]{Proposition \ref{matmakesring}}. If $n\geqslant 2$ then ${\sf Mat}(n;R)$ is not commutative provided that $R$ is not the zero ring.
\end{ex}

\begin{etym}
The word ring tracks back at least as far as Hilbert who, at the end of the 19th Century, used the German word ''Zahlring" which translates to Number Ring. He was working with a very specific example from number theory in mind, and the above abstract list of axioms is a later invention for a large class of mathematical objects that include the objects he was studying. Why he used ``Zahlring" is still a bit of a mystery to me. I've seen it written that in German ``Ring" often refers to an association or group of people with shared interests. We use that in English too, as in ``Ring of Thieves". Other people write that it refers to the fact that, for the examples of that Hilbert studied, appropriate powers of each element ``cycled back" to be written as a linear combination lower powers.
\end{etym}

\begin{ex}
\label{congmodm}
Let $m\in \mathbb{Z}$ be an integer. Then the set of {\bf integers modulo $m$}, written $$\mathbb{Z}/m\mathbb{Z}$$ is a ring. The elements of $\mathbb{Z}/m\mathbb{Z}$ consist of {\bf congruence classes} of integers modulo $m$: that is the elements are the subsets $T$ of $\mathbb{Z}$ of the form $T = a + m\mathbb{Z}$ with $a\in \mathbb{Z}$. I think of these as the set of integers that have the same remainder when you divide them by $m$. I denote the above congruence class by $\overline{a}$. Obviously, $\overline{a} = \overline{b}$ is the same as $a - b\in m \mathbb{Z}$, and often I'll write $$a\equiv b \quad (\text{mod }m).$$ If $m \in \mathbb{N}_{\geqslant 1}$ then there are $m$ congruence classes modulo $m$, in other words $|\mathbb{Z}/m\mathbb{Z}| = m$, and I could write out the set as $$\mathbb{Z}/m\mathbb{Z} = \{ \overline{0}, \overline{1}, \ldots , \overline{m-1} \}$$ To define addition and multiplication I set $$\overline{a} + \overline{b} = \overline{a+b}\text{ and } \overline{a}\cdot \overline{b} = \overline{ab}.$$ Distributivity for $\mathbb{Z}/m\mathbb{Z}$ then follows from distributivity for $\mathbb{Z}$. (There's something lurking in that definition of addition and multiplication that I want to come back to later, in \hyperref[quotientsec]{Section \ref{quotientsec}}. Do you see what it is?)
\end{ex}

\begin{ex}
Working modulo $m=2$ gives two congruence classes: the elements of the congruence class $\overline{0}$ are the {\bf even numbers}, those of the congruence class $\overline{1}$ are the {\bf odd numbers}. The ring $\mathbb{Z}/2\mathbb{Z}$ is the binary ring (actually a field).
\end{ex}

\begin{ex} You might find it helps to think of the ring $\mathbb{Z}/12 \mathbb{Z}$ as the ``Ring of Time". It has twelve elements $\{ \overline{0}, \overline{1}, \ldots , \overline{11}\}$ and we have, for instance, $\overline{10} + \overline{4} = \overline{14} = \overline{2}$. In other words ``4 hours after 10 o'clock is 2 o'clock". We also have $\overline{3}\cdot \overline{8} = \overline{24} = \overline{0}$. This shows that in a ring it can sometimes happen that a product of two non-zero numbers is zero.
\end{ex}

The construction of the ring $\mathbb{Z}/m\mathbb{Z}$ from the ring $\mathbb{Z}$ is an example of a general procedure called ``taking quotients" or the ``quotient construction", to be introduced in \hyperref[FRFIT]{Section \ref{FRFIT}}. As an example of its usefulness, I'll illustrate its use in a property of numbers you've probably known since you were a child.

\begin{proposition}[Divisibility by Sum] A natural number is divisible by $3$ (respectively $9$) precisely when the sum of its digits is divisible by $3$ (respectively $9$).
\end{proposition}

\begin{proof} I'll just prove the case for $9$, and I'll prove it by example. You can work out the details yourself, and supply the case for $3$.

Suppose the natural number is $12746$. Write out its decimal expansion:
$$12746 = 1\times 10^4 + 2\times 10^3 + 7\times 10^2 + 4\times 10 + 6$$ Since $10$ is congruent to $1$ modulo $9$ I deduce that
$$12746 \equiv 1+ 2+ 7 + 4 + 6 \quad (\text{mod }9)$$
The left hand side is divisible by $9$ precisely when the right hand side is divisible by $9$, proving the proposition.
\end{proof}
	
Variations of this lead to all sorts of \href{http://en.wikipedia.org/wiki/Divisibility_rule}{other rules}, such as:
\begin{exercise}
Show that: a natural number is divisible by $11$ if and only if the alternating sum of its digits are divisible by $11$.
\end{exercise}

\begin{exercise}
Show that: an integer of the form $abcabc$, such as $123123$, is always divisible by $7$.
\end{exercise}

\begin{exercise}
Show that: an integer congruent to $3$ modulo $4$ is never the sum of two squares. Show also that: an integer congruent to $7$ modulo $8$ is never the sum of three squares.
\end{exercise}

\begin{definition} \label{fielddef} A {\bf field} is a non-zero commutative ring $F$ in which every non-zero element $a\in F$ has an inverse $a^{-1}\in F$, that is an element $a^{-1}$ with the property that $a\cdot a^{-1} = a^{-1}\cdot a = 1$.
\end{definition}

So there you are.
\begin{rem} In France a field is called a {\bf commutative field}. This is because there are lots of examples of rings that are not commutative but still have the property: for every non-zero element $a\in F$ there exists $a^{-1}\in F$ such that $a\cdot a^{-1} = a^{-1}\cdot a = 1$. In English we call such a ring either a {\bf skewfield} or a {\bf division ring}; in France they are fields! The most famous example is the ring of {\bf quaternions} which you will meet in due course.
\end{rem}

\begin{ex} The ring $\mathbb{Z}/3\mathbb{Z}$ is a field, which we have been calling $\mathbb{F}_3$. The ring $\mathbb{Z}/12\mathbb{Z}$ is not a field, because neither $\overline{3}$ nor $\overline{8}$ can be invertible as $\overline{3}\cdot \overline{8} = \overline{0}.$
\end{ex}

\begin{proposition}\label{finitefield}
Let $m$ be a positive integer. The commutative ring $\mathbb{Z}/m\mathbb{Z}$ is a field if and only if $m$ is prime.
\end{proposition}

\begin{proof}
Suppose that $\mathbb{Z}/m\mathbb{Z}$ is a field. Let $1< a < m$ be an integer. Since $\overline{a}$ is non-zero in $\mathbb{Z}/m\mathbb{Z}$, its inverse $\overline{a}^{-1}$ exists. Take $b \in \mathbb{Z}$ such that $\overline{b} = \overline{a}^{-1}$, so that $$\overline{ab} = \overline{a}\cdot \overline{b} = \overline{a}\cdot \overline{a}^{-1} = \overline{1}$$ It follows that $ab = km + 1$ for some integer $k$. Since $a$ divides $ab$ but does not divide $1$, this identity shows that $a$ cannot divide $m$. Thus $m$ is prime.

Conversely, suppose that $m$ is prime. Let $1< a < m$ be an integer. The highest common factor $(a,m)$ is $1$ and so, by the Euclidean algorithm, there exists integers $b$ and $c$ such that $$ab + mc = 1$$ In other words $\overline{a}\overline{b} = \overline{ab} = \overline{1},$ so that $\overline{a}$ is invertible. \end{proof}

If $p$ is a prime number we typically write $\mathbb{F}_p$ for the field $\mathbb{Z}/p\mathbb{Z}$ with $p$ elements.

\begin{exercise}
Find the inverse of $24$ in the field $\mathbb{F}_{37}$.
\end{exercise}

\section{Properties of Rings}
Whether or not you enjoy this section depends on your psychology. We need to derive some basic properties of rings, much like we did for vector spaces or like you must have done for groups in \href{http://www.drps.ed.ac.uk/12-13/dpt/cxmath08064.htm}{Fundamentals of Pure Mathematics}. These derivations are all elementary. If you use your intuition from group theory and from the vector spaces, then watch out for \hyperref[zerodivdef]{Definition \ref{zerodivdef}}: this behaviour is critical in rings and is a residue of the fact that rings are not fields.
\medskip

Let's get started! The following result is similar, but not the same, as the Lemmas \ref{zerovs}, \ref{-1vs} and \ref{prodzero}.
\begin{lemma}[Multiplying by zero and negatives (i.e. additive inverses)] \label{zeroinring} Let $R$ be a ring and let $a,b\in R$. Then
\begin{enumerate}
\item $0a = 0 = a0$.
\item $(-a)b = - (ab) = a(-b)$.
\item $(-a)(-b) = ab$.
\end{enumerate}
\end{lemma}

\begin{proof} (1) $0 = 0 + 0$. Therefore $0a = (0 + 0)a$, i.e. $0a = 0a + 0a$. Subtracting $0a$ from both sides we obtain $0 = 0a$. Similarly $a0 = 0$.

(2) Using Part (1) of this lemma gives $ab + (-a)b = (a + (-a))b = 0b = 0$. So by the uniqueness of negatives $(-a)b = -(ab)$. Similarly for $a(-b)$.

(3) Just follow your nose: \begin{eqnarray*} (-a)(-b) & = & -(a(-b)) \qquad \text{by (2)} \\ & = & - ( - (ab)) \qquad \text{by (2)} \\ & = &ab. \end{eqnarray*}
\end{proof}
\begin{rem}
\begin{enumerate}
\item
The distributive axiom for rings has familiar and equally easily proved consequences such as $$(a+b)(c+d) = ac + ad +bc +bd$$ $$a(b-c) = ab - ac$$ But remember, dear Reader, multiplication may not be commutative, so multiplicative factors must be kept in the correct order: $ac$ may not equal $ca$.
\item
Suppose we have a ring $R$ such that $1_R = 0_R$. Then $R$ must be the zero ring. Why? Well, if $a\in R$ then $a = a\cdot 1_R = a\cdot 0_R = 0_R$. So $0_R$ is the only element of $R$.
\end{enumerate}
\end{rem}
\begin{definition} \label{multiples}
Let $m\in \mathbb{Z}$. The {\bf $m$--th multiple $ma$ of an element}
$a$ in an abelian group $R$ is:
\begin{equation*}
ma  = \underbrace{a+ a+ + \cdots + a}_{\text{$m$ terms}} \quad \text{if $m>0$}
\end{equation*} $0a =0$, and negative multiples are defined by $(-m)a =  - (ma).$
\end{definition}

\begin{lemma}[Rules for multiples] \label{ruleformult} Let $R$ be a ring, let $a,b\in R$ and let $m,n\in \mathbb{Z}$. Then:
\begin{enumerate}
\item $m(a+b) = ma + mb$;
\item $(m+n) a = ma + na$;
\item $m(na) = (mn)a$;
\item $m (ab) = (ma)b = a(mb)$;
\item $(ma)(nb) = (mn)(ab)$.
\end{enumerate}
\end{lemma}
\begin{proof} This is trivial and boring, so I will leave the details to you. In short, (2) and (3) are the index laws in the abelian group $(R, +)$. (1) comes from the definition of multiple and the
fact that $R$ is abelian under $+$. (4) and (5) are consequences of the
distributivity law for $R$: when you fill out the details you'll experience tedium -- the
results have to be divided into cases $m>0, m=0, m<0$ and
similarly for $n$.
\end{proof}
\begin{rem} Does this remind you of \hyperref[vsdef]{Definition \ref{vsdef}}? The first three rules here are analogous to the first three axioms of a vector space. The difference is that we are only allowing multiplication by elements of $\mathbb{Z}$ instead of a field $F$. But at least it shows that you can try to use your intuition from scalar multiplication. \end{rem}

\begin{definition} Let $R$ be a ring. An element $a\in R$ is called a \textbf{unit} if it is \textbf{invertible} in $R$ or in other words {\bf has a multiplicative inverse in $R$}, meaning that there exists $a^{-1}\in R$ such that $$aa^{-1} = 1 = a^{-1}a.$$
\end{definition}

\begin{ex}
In a field, such as $\mathbb{Q}, \mathbb{R}, \mathbb{C}$, every non--zero element is a unit. In $\mathbb{Z}$ only $1$ and $-1$ are units.
\end{ex}

\begin{rem} It's obvious that the identity element $1$ is \textit{always} a unit, since $1^{-1} = 1$. It's similarly clear that if $R$ is a non-zero ring, the element $0$ is not a unit, since by \hyperref[zeroinring]{Lemma \ref{zeroinring}(1)} $0 b = 0 \neq 1$ for all $b\in R$.
\end{rem}

\begin{rem} In Physics, sadly, they have a completely different use of the word unit, indicating \href{http://en.wikipedia.org/wiki/SI}{unit of measure}. Following the discussion in \hyperref[physdim]{Remark \ref{physdim}}, a Mathematician thinks of their units as a basis element in a one-dimensional vector space over $\mathbb{R}$: for instance a second is a basis vector for the time-line, which I'll call $\vec{\mathbb{T}}$. The choice of basis vector produces an isomorphism $\mathbb{R} \stackrel{\sim}{\to} \vec{\mathbb{T}}$ thanks to Theorem \ref{lincombiso}, and so allows time to be measured by a real number. Of course, you should always make sure you know which meaning is intended.
\end{rem}

\begin{proposition}
The set $R^{\times}$ of units in a ring $R$ forms a group under multiplication.
\end{proposition}

\begin{proof}
Let $a,b\in R^{\times}$. So $a,b$ are units and hence $a^{-1}$ and $b^{-1}$ exist in $R$. So $b^{-1}a^{-1}$ exists in $R$. Then $(ab)(b^{-1}a^{-1}) = abb^{-1}a = a1a^{-1} = aa^{-1} = 1$ and similarly $(b^{-1}a^{-1})(ab) = 1$. Therefore $ab$ is a unit in $R$ with $(ab)^{-1} = b^{-1}a^{-1}$, as usual. Thus $R^{\times}$ is closed under multiplication.

Multiplication in $R^{\times}$ is associative since multiplication in $R$ is associative, and $1\in R^{\times}$ is the identity element of $R^{\times}$. Finally, it is obvious that if $a\in R^{\times}$ then $a^{-1}\in R^{\times}$, thus verifying the group axioms.
\end{proof}
I will call $R^{\times}$ the {\bf group of units of the ring $R$}.

\begin{ex} $\mathbb{Z}^{\times} = \{ 1, -1\}$ and $\mathbb{R}^{\times} = \mathbb{R}\setminus \{ 0 \}$. If $R = {\sf Mat}(n; F)$ then $R^{\times} = GL(n;F)$, the general linear group described in \hyperref[genlin]{Section \ref{basicmat}}.
\end{ex}

\begin{exercise} \label{DH} Let $p$ be a prime. Show that: $\mathbb{F}_p^{\times}$, the group of units of the field $\mathbb{F}_p$ where $p$ is prime, is a cyclic group of order $p-1$.
\end{exercise}

So far I presume you have been thinking this is quite abstract. Well, you're right, but as I hope we can discuss in a Workshop, \hyperref[DH]{Exercise \ref{DH}} is the basis of the \href{http://en.wikipedia.org/wiki/Diffie-Hellman_key_exchange}{Diffie--Hellman key exchange}, the mother and father of internet security! I'm pleased to be able to tell you that, because you will become ambassadors for Mathematics when you leave the university, whether you like it or not, and I'd like you to see that most areas of mathematics can have a surprising utilitarian value, even if it is far from their raison d'\^etre.

\begin{definition} \label{zerodivdef} In a ring $R$ a non--zero element $a$ is called a \textbf{zero--divisor} or \textbf{divisor of zero} if there exists a non--zero element $b$ such that either $ab = 0$ or $ba = 0$.
\end{definition}

Perfectly respectable rings can have zero-divisors: in ${\sf Mat}(2; \mathbb{R})$ $$\begin{bmatrix} -1 & 1 \\ -1 & 1 \end{bmatrix} \begin{bmatrix} 1 & 1\\ 1 & 1 \end{bmatrix} = \begin{bmatrix} 0 & 0 \\ 0 & 0 \end{bmatrix}$$ so that both $\begin{bmatrix} -1 & 1 \\ -1 & 1 \end{bmatrix}$ and $\begin{bmatrix} 1 & 1\\ 1 & 1 \end{bmatrix}$ are zero--divisors.
\textcolor{black}{\it Welcome to the zoo!} Mathematicians understand very well a ring like ${\sf Mat}(2; \mathbb{R})$ -- after all, it can't be so hard, it's only a $4$-dimensional $\mathbb{R}$-vector space. But the existence of zero--divisors makes it behave differently to most of the other examples you may have seen before. The ones you know so far tend, instead, to satisfy the following hypothesis:

\begin{definition} An \textbf{integral domain} is a non--zero commutative ring that has no zero--divisors.
\end{definition}
In an integral domain there are no zero--divisors and therefore the laws
\begin{enumerate}
\item $ab=0 \Rightarrow a= 0 \text{ or } b = 0, \text{ and} $
\item $a\neq 0 \text{ and } b\neq 0 \to ab \neq 0$
\end{enumerate} hold. These properties may remind you of \hyperref[prodzero]{Lemma \ref{prodzero}}.

\begin{ex} \label{unitnz} $\mathbb{Z}$ is an integral domain. Any field is an integral domain, since a unit in a ring $R$ cannot be a zero--divisor. To see this, let $R$ be a non--zero ring and let $a\in R^{\times}$ be a unit. Suppose that $ab = 0$ or $ba = 0$ for some $b\in R$. Multiplying
on the left or on the right respectively by $a^{-1}$ shows that
$a^{-1}a b = a^{-1} 0$ or $baa^{-1} = 0 a^{-1}$, so in both cases
$b = 0$. \end{ex}

\begin{ex} The ring $\mathbb{Z}/12\mathbb{Z}$ is not an integral domain, since $\overline{3}\cdot \overline{8} = \overline{0}.$
\end{ex}

\begin{proposition}[Cancellation Law for Integral Domains] \label{cancid} Let $R$ be an integral domain and let $a,b,c\in R$. If $ab=ac$ and $a\neq 0 $ then $b=c$. \end{proposition}

\begin{proof} If $ab = ac$ then $a(b-c) = 0$ so that either $a=0$ or $b-c = 0$, that is either $a=0$ or $b=c$. \end{proof}

This Law fails for rings that are not integral domains. For instance, in $R = \mathbb{Z}/12\mathbb{Z}$, there is the equality $$ \overline{8}\cdot \overline{2} = \overline{8}\cdot \overline{5}.$$

\medskip

We will now reprove \hyperref[finitefield]{Proposition \ref{finitefield}} as a special case of a general theorem

\begin{proposition}
Let $m$ be a natural number. Then $\mathbb{Z}/m\mathbb{Z}$ is an integral domain if and only if $m$ is prime.
\end{proposition}

\begin{proof} Suppose $m$ is prime. Obviously $\mathbb{Z}/m\mathbb{Z}$ is a commutative ring. Suppose that $\overline{k}\in\mathbb{Z}/m\mathbb{Z}$ is a divisor of zero. Then $\overline{k}\neq \overline{0}$ and there exists $\overline{\ell} \in \mathbb{Z}/m\mathbb{Z}$ with $\overline{\ell} \neq \overline{0}$ and $\overline{k}\cdot \overline{\ell} = \overline{0}$. Then $$k \ell \equiv 0 \text{ (mod $m$)},$$ so that $m$ divides $k\ell $. Since $m$ is prime it follows that $m$ divides $k$ or $m$ divides $\ell$, i.e. $k\equiv 0 \text{ mod $m$}$ or $\ell\equiv 0 \text{ mod $m$}$ and hence $\overline{k} = \overline{0}$ or $\overline{\ell} = \overline{0}$, a contradiction. So $\mathbb{Z}/m\mathbb{Z}$ has no divisors of zero and is an integral domain.

Suppose now that $m$ is not prime. Then $m=k\ell$ for some integers
$k,\ell$ with $1<k,\ell < m$. Then $k$ and $\ell$ are not divisible by $m$
so that $\overline{k} \neq \overline{0}$, $\overline{\ell} \neq \overline{0}$. But $\overline{k}\overline{\ell} = \overline{k\ell} =\overline{m} = \overline{0}$. So $\overline{k}$ and $\overline{\ell}$ are divisors of zero and $\mathbb{Z}/m\mathbb{Z}$ is not
an integral domain.
\end{proof}

\begin{theorem}\label{finid}
Every \textbf{finite} integral domain is a field.
\end{theorem}

\begin{proof} Let $R$ be a finite integral domain. To prove that $R$ is a field what we need to show is that every non--zero element is invertible.

Let $a$ be a non--zero element of $R$ and define the mapping
$\lambda_a : R\to R$ by $\lambda_a (b) = ab$
for all $b\in R$. If $\lambda_a(b_1) = \lambda_a(b_2)$ for some
$b_1,b_2 \in R$ then $ab_1 = ab_2$. Since $a\neq 0$ \hyperref[cancid]{the cancellation law for integral domains} yields $b_1= b_2$. This means that $\lambda_a$ is injective. Since $R$ is finite it follows automatically that $\lambda_a$ must also be surjective.

In particular $1\in \im \lambda_a$, meaning that there exists $b\in R$ with $1 = \lambda_a(b) = ab$. Since $R$ is an integral domain, multiplication is
commutative and we have $ba=1$ as well.
Therefore $a$ is invertible, with inverse $b$.
\end{proof}

\hyperref[finitefield]{Proposition \ref{finitefield}} follows immediately.

\section{Polynomials}
\begin{definition}
Let $R$ be a ring. A {\bf polynomial
over $R$} is an expression of the form $$P = a_0 + a_1 X + a_2
X^2 + \cdots + a_m X^m$$ for some non--negative integer $m$ and
elements $a_i\in R$ for $0\leqslant i\leqslant m$. The set of all
polynomials over $R$ is denoted by $R[X]$. In case $a_m$ is
non--zero, the polynomial $P$ has {\bf degree} $m$, written $\deg (P)$, and
$a_m$ is its {\bf leading coefficient}. When the leading coefficient is $1$ the polynomial is a {\bf monic polynomial}. A polynomial of degree one is called {\bf linear}, a polynomial of degree two is called {\bf quadratic}, and a polynomial of degree three is called {\bf cubic}.
\end{definition}

You will see that I have used $X$ for the variable, but I could use other letters sometimes, although they will usually be capital letters from the end of the alphabet.

Polynomials are added and
multiplied as follows:
$$(a_0 + a_1X + \cdots +a_mX^m) + (b_0 + b_1X + \cdots + b_nX^n) =
(a_0+b_0) + (a_1 + b_1)X + (a_2+b_2)X^2 + \cdots $$ and
\begin{eqnarray*} (a_0 + a_1X + &\cdots &+a_mX^m)(b_0 + b_1X + \cdots + b_nX^n) \\
&&= a_0b_0 + (a_0b_1 + a_1b_0)X + (a_0b_2 + a_1b_1 + a_2b_0)X^2 +
\cdots + a_mb_nX^{m+n}\end{eqnarray*} where $m,n\geqslant 0, a_i,b_j \in R$ for
$0\leqslant i\leqslant m$ and $0\leqslant j\leqslant n$.

\begin{definition} With these definitions the set
$R[X]$ becomes a ring called the {\bf ring of polynomials with
coefficients in $R$, or over $R$}. The zero and the identity of
$R[X]$ are the zero and identity of $R$, respectively.
\end{definition}
\begin{remarks} The elements of $R$ can be
identified with polynomials of degree $0$. I will call these polynomials {\bf constant}. You should notice from the multiplication rule that if $R$ is commutative, then so too is $R[X]$.
\end{remarks}
\medskip

You might ask why one would want to study polynomial rings with coefficients from an arbitrary ring $R$. One good reason is that polynomials in more one than variable play an important r\^ole in many areas of mathematics, and particularly in \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath11120.htm}{Algebraic Geometry}.  An example of a cubic polynomial in two variables is $Y^2 - X^3 - X - 1$; usually I would write this belongs to $\mathbb{C}[X,Y]$. But sometimes it is useful to prove properties of polynomial rings in several variables by induction: to do this I identify $\mathbb{C}[X,Y]$ with $R[X]$ where $R = \mathbb{C}[Y]$ and then try to show that a property that $R$ has can be inherited by $R[X]$. You've just seen this: if $R$ is commutative then so too is $R[X]$. \hyperref[degadd]{Lemma \ref{degadd}} below gives another example of this behaviour: if $R$ is an integral domain then so too is $R[X]$.

\begin{lemma}
\label{degadd}
(i) If $R$ is a ring with no zero-divisors, then $R[X]$ has no zero-divisors and $\deg (PQ) = \deg(P) + \deg(Q)$ for non-zero $P,Q \in R[X]$.\\
(ii) If $R$ is an integral domain then so is $R[X]$.
\end{lemma}
\begin{proof}
(i) If $R$ has no zero-divisors then the leading coefficient of the product $PQ$ is the product of the leading coefficients of $P$ and of $Q$, so that
$PQ \neq 0$ if and only if $P \neq 0$ and $Q \neq 0$.\\
(ii) Immediate from the observation that if $R$ is commutative then so is $R[X]$, and from (i).
\end{proof}

\begin{exercise} Show that: if $R$ is an integral domain then $R[X]^{\times} = R^{\times}$. Show by counterexample that this is false if $R$ is not an integral domain.
\end{exercise}

\begin{theorem}[Division and Remainder]
\label{euclid} Let $R$ be an integral domain and  let $P, Q\in R[X]$ with $Q$ monic.
Then there exists unique $A, B\in R[X]$ such
that
$ P = AQ + B$ and $\deg (B) < \deg (Q)$ or $B = 0.$
\end{theorem}
\begin{proof}
I begin by choosing a polynomial $A$ that makes $\deg (P-AQ)$ as small as possible. Since the degree is a non-negative integer this is always possible. Suppose that $\deg (P-AQ) \geqslant \deg (Q)$, say $P -AQ = a_rX^r + \cdots + a_0$ with $a_r\neq 0$ and $r\geqslant d = \deg (Q)$. Then $P - (A+a_rX^{r-d})Q$ has smaller degree than $P-AQ$, contradicting my original choice of $A$. So it must be that $P-AQ$ has degree strictly less than the degree of $Q$. This shows the existence of an $A$ and a $B = P-AQ$ as in the statement of the theorem.

I have also claimed that both $A$ and $B$ are unique, so let's check that now. Suppose that $A'$ and $B'$ also satisfy the conclusions of the theorem, that is $P = A'Q + B'$ with $\deg (B') < d$. Then $$ 0 = P - P = (A - A')Q + (B-B') $$ so that $(A-A')Q = B'-B$. But $(B'-B)$ has degree strictly less than $d$, while $(A-A')Q$  has degree greater than or equal to $d$, unless $A - A' = 0$ (cf \hyperref[degadd]{Lemma \ref{degadd}}). I deduce that $A = A'$ and from this it follows that $B = P - AQ = P - A'Q = B'$ too.
\end{proof}

\begin{ex} The above proof is not only pretty slick, but, if you look closely, it also tells you how to divide in practice, finding $A$ and $B$. It's a variation on long division of integers. Here's an example for polynomials with real coefficients. Let $P = X^5 - 7 X^4 - 16 X^3 - 17 X + 2$ and $Q = X^3 - 5X +4$. Then
\begin{eqnarray*}
X^5 - 7 X^4 - 16 X^3 - 17 X + 2 & = & X^2 ( X^3 - 5X +4) - 7X^4 - 11 X^3 - 4X^2 - 17X + 2 \\ & = & X^2 ( X^3 - 5X +4) - 7X ( X^3 - 5X +4) - 11 X^3 - 39X^2 + 11X +2 \\ & = & (X^2 - 7X -11) ( X^3 - 5X +4) - 39 X^2 - 44X + 46
\end{eqnarray*}
So $A = X^2 - 7X -11$ and $B = -39X^2 -44X + 46$.
\end{ex}

\begin{definition} \label{rootdef}Let $R$ be a commutative ring and $P\in R[X]$ a polynomial. Then the polynomial $P$ can be {\bf evaluated} at the element $\lambda \in R$ to produce $P(\lambda)$ by replacing the powers of $X$ in the polynomial $P$ by the corresponding powers of $\lambda$. In this way we have a mapping $$R[X] \to {\rm Maps}(R,R)$$ This is the precise mathematical description of thinking of a polynomial as a function. An element $\lambda \in R$ is a {\bf root} of $P$ is $P(\lambda) = 0$.
\end{definition}

\begin{ex} Let $ R = \mathbb{C}$ and $P  = X^3 + 1$. Then $\pi \in \mathbb{C}$ and $P(\pi) = \pi^3 +1 \in \mathbb{C}$. The complex number $e^{2\pi \sqrt{-1}/3}$ is a root of $P$.
\end{ex}

\begin{ex}
Let $R = \mathbb{R}[Y]$ and $P = (Y^2 +1)X^2 - (Y^3+2Y^2+Y)X - 2Y -4\in R[X]$. Then $$P(Y+2) = (Y^2+1)(Y+2)^2 - (Y^3+2Y^2 + Y)(Y+2)  - 2Y - 4 = 0$$ so that $Y+2\in R$ is a root of $P$.
\end{ex}

\begin{exercise}
Show that: the mapping $R[X] \to {\rm Maps}(R,R)$ from \hyperref[rootdef]{Definition \ref{rootdef}} is not injective when $R = \mathbb{F}_p$, $p$ a prime. Hint: Fermat's Little Theorem. (This means that in general, even over fields, a polynomial is not just a special type of function!)\end{exercise}

\begin{proposition}\label{divlinfact}
Let $R$ be a commutative ring, let $\lambda\in R$ and $P(X)\in R[X]$. Then $\lambda$ is a root of $P(X)$
if and only if $(X-\lambda)$ divides $P(X)$.
\end{proposition}

\begin{proof} If $X-\lambda$ divides $P(X)$ then $P(X)=(X-\lambda)Q(X)$ for some $Q(X) \in R[X]$ and 
$$P(\lambda)~=~0 \cdot Q(\lambda)~=~0  \in R~.$$
Conversely, suppose $P(X)=\sum\limits^n_{k=0}a_kX^k \in R[X]$ is such that $P(\lambda)=0 \in R$. For any $k \geqslant 0$
$$X^k-\lambda^k~=~\begin{cases}
(X-\lambda)\sum\limits^{k-1}_{j=0}\lambda^jX^{k-j-1}&{\rm if}~k \geqslant 1\\
0&{\rm if}~k=0
\end{cases}\in R[X]~,$$
so that 
$$\begin{array}{ll}
P(X)&=~P(X)-P(\lambda)\\[1ex]
&=~\sum\limits^n_{k=0}a_kX^k-\sum\limits^n_{k=0}a_k\lambda^k\\[1ex]
&=~(X-\lambda)(\sum\limits^n _{k=1}a_k(\sum\limits^{k-1}_{j=0}\lambda^jX^{k-j-1})) \in R[X]~,
\end{array}$$
so that $(X-\lambda)$ divides $P(X)$.
\end{proof}

This has a pleasant consequence.

\begin{theorem}
Let $R$ be a field, or more generally an integral domain. Then a non-zero polynomial $P\in R[X]\smallsetminus \{0\}$ has at most $\deg (P)$ roots in $R$.
\end{theorem}
\begin{proof}
Suppose that $\lambda_1, \ldots, \lambda_m$ are distinct roots of
$P$ in $R$. By \hyperref[divlinfact]{Proposition \ref{divlinfact}} $P = A(X-\lambda_1)$ for some $A \in R[X]$ with $\deg(A) = \deg (P) -1$ by Lemma \ref{degadd}. Evaluating this equality at $\lambda_i$ with $i\geqslant 2$ gives an equality in $R$: $0 = P(\lambda_i) = A(\lambda_i) (\lambda_i-\lambda_1)$. Since $\lambda_i-\lambda_1$ is not zero and $R$ is an integral domain, it follows that $A(\lambda_i) = 0$ and $\lambda_2, \ldots , \lambda_m$ are distinct roots of $A$. The theorem follows by induction.
\end{proof}

\begin{definition} \label{algclsdef}
A field $F$ is {\bf algebraically closed} if each non-constant polynomial $P\in F[X]\setminus F$ with coefficients in our field has a root in our field $F$.
\end{definition}

\begin{ex}
The field of real numbers, $\mathbb{R}$, is not algebraically closed. For instance, $X^2 +1$ has no root in $\mathbb{R}$.
\end{ex}

\begin{theorem}[Fundamental Theorem of Algebra]
The field of complex numbers, $\mathbb{C}$, is algebraically closed.
\end{theorem}
There are a lot of proofs of this absolutely basic result: see \href{http://mathoverflow.net/questions/10535/ways-to-prove-the-fundamental-theorem-of-algebra}{this Mathoverflow question} and \href{http://en.wikipedia.org/wiki/Fundamental_theorem_of_algebra}{this Wikipedia entry}. Probably the most elementary is by complex analysis and you will meet it in \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath10067.htm}{Honours Complex Variables}. Isn't it a beautiful thing how different mathematical topics are interwoven?

\begin{theorem} \label{decpoly}
If $F$ is an algebraically closed field, then every non-zero polynomial $P\in F[X]\setminus \{0\}$ {\bf decomposes into linear factors} $$P = c(X-\lambda_1) \cdots (X-\lambda_n)$$ with $n\geqslant 0, c\in F^{\times}$ and $\lambda_1, \ldots , \lambda_n \in F$. This decomposition is unique up to reordering the factors.
\end{theorem}
\begin{proof}If $P$ is a constant polynomial, there is nothing to show. So assume that $P$ is not constant. Since $F$ is algebraically closed $P$ has a root $\lambda_1\in F$. By \hyperref[divlinfact]{Proposition \ref{divlinfact}} $P = A(X-\lambda_1)$. Now induct on $\deg (P)$.
\end{proof}

\section{Homomorphisms, Ideals and Subrings}
\label{quotientsec}
To venture further into the jungle of rings, we need to add ring homomorphisms to our toolkit. These are easy to define -- I hope you can already guess the definition -- but they lead us inevitably to subrings and to ideals. Subrings and ideals are similar to subgroups and normal subgroups from \href{http://www.drps.ed.ac.uk/12-13/dpt/cxmath08064.htm}{Fundamentals of Pure Mathematics}, and they play a key role in a great deal of algebra.

\begin{definition}\label{ringmorphism}
Let $R$ and $S$ be rings. A mapping $f: R\longrightarrow S$ is a \textbf{ring homomorphism} if the following hold for all $x,y\in R$:
\begin{eqnarray*}
f (x+y)  &=& f(x) + f(y) \\
f(xy)  &=& f(x) f(y)
\end{eqnarray*}
\end{definition}

\begin{ex} \label{ex1} The inclusion $\mathbb{Z}\hookrightarrow \mathbb{Q}$ is a ring homomorphism.
\end{ex}

\begin{ex} \label{ex2}
For each $m\in \mathbb{Z}$, the mapping $f: \mathbb{Z} \to \mathbb{Z}/m\mathbb{Z}$ defined by $f(a) = \overline{a}$ is a ring homomorphism.
\end{ex}

\begin{exercise} \label{ex3}
Let $R$ be a commutative ring and $\lambda \in R$. The mapping $f: R[X]\to R$ defined by $f(P) = P(\lambda)$ for all $P\in R[X]$, as defined in \hyperref[rootdef]{Definition \ref{rootdef}}, is a ring homomorphism.
\end{exercise}

\begin{exercise}
\label{ex4}
Let $R$ be a commutative ring, $n$ a positive integer and $M\in {\sf Mat}(n; R)$. The mapping $f: R[X]\to {\sf Mat}(n;R)$ defined by $$f(a_tX^t + a_{t-1}X^{t-1}+ \cdots + a_1X + a_0) = a_tM^t + a_{t-1}M^{t-1} + \cdots + a_1M + a_0$$ is a ring homomorphism. You used this in the second exercise of the second Workshop.
\end{exercise}

\begin{rem}
\begin{enumerate}
\item As usual, a homomorphism is simply a mapping that ``preserves the structure". Rings have the operations of addition and multiplication and I ask that these are preserved; vector spaces have addition and scalar multiplication and a \hyperref[deflinmap]{linear mapping} must preserve these.
\item However, the definition of a ring $R$ \ref{ringdef} includes the requirement that $R$ have an identity element $1=1_R \in R$, but the definition of a ring homomorphism \ref{ringmorphism} $f:R \to S$ does not require that $f(1_R)=1_S \in S$.
For example, the zero function $0:R \to S;x \mapsto 0$ is a ring homomorphism.
The element $f(1_R) \in S$ is an idempotent, meaning that $f(1_R)^2=f(1_R) \in S$ (Exercise
\ref{projalong}) or equivalently such that $f(1_R)(f(1_R)-1_S)=0_S \in S$. Thus if $S$ has no zero-divisors either $f(1_R)=0_S$ (in which case $f=0$ is the zero ring homomorphism) or $f(1_R)=1_S$.
\item In \href{http://www.drps.ed.ac.uk/12-13/dpt/cxmath08064.htm}{Fundamentals of Pure Mathematics} you saw that a group homomorphism had to preserve the group multiplication. Once you understand this pattern, its pays off handsomely to  observe the similarities and differences that occur in examples of homomorphisms: this leads to \href{http://en.wikipedia.org/wiki/Category_theory}{Category Theory} which is a profound and powerful way of viewing swathes of mathematics, and whose influence is increasing.
\item Given the above, you know that you already know that an isomorphism is used in the same way for ring homomorphisms as for linear mappings or group homomorphisms, including the notation $\cong$ for isomorphism between two rings.
\item As you did in \hyperref[linmapcomp]{Exercise \ref{linmapcomp}} and \hyperref[linmapiso]{Exercise \ref{linmapiso}} for linear mappings, it is easy to check that the composition of two ring homomorphisms is a ring homomorphism, and that the inverse of a ring isomorphism is a ring isomorphism.
\end{enumerate}
\end{rem}

If you temporarily ignore multiplication, a ring homomorphism is a group homomorphism between rings, regarded as groups under addition. So Group Theory gives:

\begin{lemma} Let $R$ and $S$ be rings and $f: R \to S$ a ring homomorphism. Then for all $x,y\in R$ and $m\in \mathbb{Z}$:
\begin{enumerate}
\item $f (0_R) = 0_S$, where $0_R$ and $0_S$ are the zeros of $R$ and $S$ respectively;
\item $f (-x) = - f(x)$;
\item $f (x-y) = f(x) - f(y)$;
\item $f(mx) = m f (x)$,
\end{enumerate} where $mx$ denotes the $m$-th multiple of $x$, as in \hyperref[multiples]{Definition \ref{multiples}}.
\end{lemma}



\begin{rem}
\begin{enumerate}
\item It is easy to deduce from the multiplicative property of a ring homomorphism that $$f(x)^n = (f (x))^n$$ for all $x\in R$ and $n\in \mathbb{N}$.
\item But, dear Reader, you do need to tread carefully around the behaviour of identity elements under ring homomorphisms. Examples \ref{ex1}--\ref{ex2} and Exercises \ref{ex3}--\ref{ex4} all have the property that $f(1_R) = 1_S$, but this need not always be true. It is possible to find rings $R,S$ and a ring homomorphism $f : R\to S$ such that $f (1_R) \neq 1_S$. An example is $f : \mathbb{R} \to {\sf Mat}(2;\mathbb{R})$ defined by $$f (x) = \begin{pmatrix} x & 0 \\ 0 & 0 \end{pmatrix} \qquad \text{for all $x\in \mathbb{R}$}.$$
\end{enumerate}
\end{rem}

Now I arrive at the first necessary consequence of ring homomorphisms. I hope you are already convinced of the importance of the image and kernel for homomorphisms. You spent considerable time on this in  \href{http://www.drps.ed.ac.uk/12-13/dpt/cxmath08064.htm}{Fundamentals of Pure Mathematics}, and in this course an early highlight has been the \hyperref[rnthm]{Rank Nullity Theorem}. So what are kernels and images for ring homomorphisms? The annoying thing is that they are probably not {\it exactly} what you first think them to be.
\medskip

First we take kernels.
\begin{definition}
\label{defideal} A subset $I$ of a ring $R$ is an
\textbf{ideal}, written $I \unlhd R$, if the following hold:
\begin{enumerate}
\item $I\neq \emptyset$;
\item $I$ is closed under subtraction;
\item for all $i\in I$ and $r\in R$ we have $ri,ir \in I$.
\end{enumerate}
Condition (3) says that \textbf{$I$ is closed under
multiplication by elements of $R$}.
\end{definition}

\begin{ex}
In any ring $R$, $\{ 0 \}$ and $R$ are ideals of $R$. Condition (3) in \hyperref[defideal]{Definition \ref{defideal}} holds for $\{ 0 \}$ because $r0 = 0r = 0 \in \{ 0 \}$ thanks to \hyperref[zeroinring]{Lemma \ref{zeroinring}.1}.
\end{ex}

\begin{ex}
The set $m\mathbb{Z}$ is an ideal of $\mathbb{Z}$. Condition (3) in \hyperref[defideal]{Definition \ref{defideal}} holds because $b(ma) = (ma)b = m (ab) \in m\mathbb{Z}$ for all $a,b\in \mathbb{Z}$.
\end{ex}

\begin{ex} The set $$I = \left\{ \begin{pmatrix} 0 & b \\ 0 & d \end{pmatrix} : b,d \in \mathbb{R} \right\} \subset {\sf Mat}(2; \mathbb{R})$$ is not an ideal. It fails Condition (3) in \hyperref[defideal]{Definition \ref{defideal}}: although it is true that $ri \in I$ for all $i\in I$ and $r\in R$, it is not true that $ir\in I$ for all $i\in I$ and $r\in R$. For instance: $$ \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix} \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix} = \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} \notin I $$
\end{ex}

As for vector spaces in \hyperref[generatingsubsp]{Proposition \ref{generatingsubsp}}, there is a cheap but useful way to construct ideals.

\begin{definition}\label{genideals} Let $R$ be a commutative ring and let $T \subset R$. Then the {\bf ideal of $R$ generated by $T$} is the set $${}_R\langle T \rangle =  \{  r_1t_1 + \cdots + r_mt_m : t_1, \ldots , t_m \in T, r_1, \ldots , r_m\in R\},$$ together with the zero element in the case $T =\emptyset$. If $T = \{t_1, \ldots , t_n \}$, a finite set, I will often abuse notation by writing ${}_R\langle t_1, \ldots , t_n \rangle$ instead of ${}_R\langle \{ t_1, \ldots , t_n \}\rangle$.
\end{definition}

\begin{ex} Let $m\in \mathbb{Z}$. Then ${}_\mathbb{Z}\langle m \rangle = m\mathbb{Z}$.
\end{ex}

\begin{ex} Let $P\in \mathbb{R}[X]$. Then ${}_{\mathbb{R}[X]}\langle P \rangle = \{ AP: A\in \mathbb{R}[X]\} = \{ Q:  P \text{ divides } Q \text{ in } \mathbb{R}[X]\}$.
\end{ex}

\begin{proposition}
Let $R$ be a commutative ring and let $T\subseteq R$. Then ${}_R\langle T
\rangle$ is the smallest ideal of $R$ that contains $T$.
\end{proposition}

\begin{proof}
 The Proposition contains two claims: the first that ${}_R\langle T \rangle$ is an ideal of $R$; the second that it is the smallest such ideal. This is just as clear as the proof of \hyperref[generatingsubsp]{Proposition \ref{generatingsubsp}}, and follows also from \hyperref[intideal]{Lemma \ref{intideal}}. Let me give a few details.

To prove that ${}_R\langle T \rangle $ is an ideal, note first that it is non-empty since $0_R\in {}_R\langle T \rangle$. Given two arbitrary elements $r_1t_1 + \cdots +r_mt_m, r'_1t'_1 + \cdots + r'_{n}t'_n \in {}_R\langle T \rangle$ their difference equals $ r_1t_1 + \cdots +r_mt_m + (- r'_1)t'_1 + \cdots +(- r'_{n})t'_n$ which is  in ${}_R\langle T \rangle$ by definition. Finally, if $r\in R$ then $r( r_1t_1 + \cdots + r_mt_m) = (rr_1)t_1 + \cdots + (rr_m)t_m \in I$ and similarly for right multiplication by $r$.

For minimality, observe that if $I$ is an ideal and $t_1, \ldots , t_m \in I$ then necessarily $ r_1t_1 + \cdots + r_mt_m \in I$.
\end{proof}

\begin{definition} \label{prindef}
Let $R$ be a commutative ring. An ideal $I$ of $R$ is called a
\textbf{principal ideal} if $I = \langle t \rangle$ for some
$t\in R$.
\end{definition}

\begin{ex}  $\{0\}$ and $R$ are always principal ideals in a commutative ring, generated by the element $0$ and the element $1_R$ respectively.
\end{ex}

You'll find out later that it is a difficult but important problem to decide whether an ideal is a principal
ideal or not. Every ideal of $\mathbb{C}[X]$ is principal; conversely there are ideals in the polynomial ring $\mathbb{C}[X,Y]$ that are not principal.
 \medskip

 Now I'm going to show you why ideals are necessary, fundamental and unavoidable:

\begin{definition} Let $R$ and $S$ be rings with zero elements $0_R$ and $0_S$ respectively and let $f:R \to S$ be a ring homomorphism. Since $f$ is in particular a group homomorphism from $(R,+)$ to $(S,+)$, the \textbf{kernel} of $f$ already has a meaning: $$\ker f = \{ r\in R: f(r) = 0_S\}.$$
\end{definition}

\begin{proposition}\label{kerideal} Let $R$ and $S$ be rings and $f:R \to S$ a ring homomorphism. Then $\ker f$ is an ideal of $R$.
\end{proposition}
\begin{proof} You saw in \href{http://www.drps.ed.ac.uk/12-13/dpt/cxmath08064.htm}{Fundamentals of Pure Mathematics} that $\ker f$ is a subgroup of $(R,+)$. So that gives Conditions (1) and (2) in \hyperref[defideal]{Definition \ref{defideal}}. All that I have left to prove is that if $k\in \ker f$ and $r\in R$, then $kr,rk \in \ker f$. But this is clear to you and me: $$f(kr) = f(k)f(r) = 0_S f(r) = 0_S \qquad \text{and} \qquad f(rk) = f(r)f(k) = f(r) 0_S = 0_S.$$
\end{proof}

\begin{ex} \label{kerz} Let $f : \mathbb{Z} \to \mathbb{Z}/m\mathbb{Z}$ be the homomorphism of \hyperref[ex2]{Example \ref{ex2}}. Then $$\ker f = \{ a \in \mathbb{Z}: f(a) = \overline{0} \} =  \{ a\in \mathbb{Z} : a \text{ divisible by }m \} = m\mathbb{Z}.$$
\end{ex}

There are some easy-to-prove results about ideals and kernels that you have seen for vector spaces (see \hyperref[ker=0]{Lemma \ref{ker=0}}, \hyperref[intvssub]{Exercise \ref{intvssub}} and \hyperref[prodvs]{Exercise \ref{prodvs}}) or for groups. I'm going to write them out but not prove them: their proofs are obtained by modifying the proofs you already know in those other cases, so there's no point sentencing you to twenty year's of boredom; there are far more interesting things to be thinking about.

\begin{lemma} \label{injring} $f$ is injective if and only if $\ker f = \{ 0 \}$. \end{lemma}

\begin{lemma} \label{intideal} The intersection of any collection of ideals of a ring $R$ is an ideal of $R$. \end{lemma}

\begin{lemma} Let $I$ and $J$ be ideals of a ring $R$. Then $$I+J = \{ a+ b: a\in I, b\in J\}$$ is an ideal of $R$. \end{lemma}

Then we take images.

\begin{definition} Let $R$ be a ring. A subset $R'$ of $R$ is a \textbf{subring} of $R$ if $R'$ itself is a ring under the operations of addition and multiplication defined in $R$.
\end{definition}

\begin{ex}
In any ring $R$, $\{0\}$ and $R$ are subrings.
\end{ex}
\begin{exercise}
$\mathbb{Z}[\sqrt{-1}] = \{ a + b\sqrt{-1} : a,b \in \mathbb{Z} \}$
is a subring of $\mathbb{C}$. This subring is called the ring of {\bf Gaussian integers}. It is fundamental in elementary \href{http://www.drps.ed.ac.uk/11-12/dpt/cxmath10036.htm}{Number Theory}.
\end{exercise}
\begin{ex}
\label{submatrices}
If $F$ is a field, then for any $m,n\in \mathbb{N}$ with $m\leqslant n$ ${\sf Mat}(m; F)$ is a
subring of ${\sf Mat}(n; F)$. How? Well consider ${\sf Mat}(m;F )$ as the subset of ${\sf Mat}(n;F)$ all of whose entries beyond the $m$-th
column or below the $m$-th row are zero.
\end{ex}

If you ever need to decide whether or not a subset $R'$ of a ring $R$ is a subring, I'd suggest that you use the following!

\begin{proposition}[Test for a subring] \label{testforsubring}Let $R'$ be a subset of a ring $R$. Then $R'$ is a subring if and only if
\begin{enumerate}
\item $R'$ has a multiplicative identity, and
 \item $R'$ is closed under
subtraction:  $a,b\in R' \to a-b \in R'$, and
\item $R'$ is
closed under multiplication.
\end{enumerate}
\end{proposition}
\begin{proof} If $R'$ is a subring then its obvious that (1), (2), (3) hold.

Conversely suppose (1), (2) and (3) hold. By (1) and (2) and the
subgroup test for a group from \href{http://www.drps.ed.ac.uk/12-13/dpt/cxmath08064.htm}{Fundamentals of Pure Mathematics}, $R'$ under
addition is a subgroup of $R$ and therefore an abelian group. The
associative law for multiplication holds for elements of $R'$ since
it holds for elements of $R$. That, coupled with (1) and (3),
proves that $R'$ is a monoid under multiplication.  Finally, the distributive laws hold for elements of $R'$ since they hold for elements of $R$.

It follows that $R'$ is a ring, and therefore a subring of $R$.
\end{proof}

 \begin{ex}
Suppose that $I$ is an ideal of $R$. Then it is usually not a subring of $R$. For although it  satisfies Properties (2) and (3) in \hyperref[testforsubring]{Proposition \ref{testforsubring}}, it may well fail Property (1). For instance, $m\mathbb{Z}$ has a multiplicative identity if and only if $m = 0$ or $1$.
\end{ex}

\begin{rem} At the risk of being tedious, dear Reader, let me point out that for subrings you need tread carefully around the identity elements of a ring and a subring. Just because $R'$ is a subring of $R$ does not mean that $1_R$ and $1_{R'}$ are equal. For a counterexample to equality, see \hyperref[submatrices]{Example \ref{submatrices}}.
\end{rem}

\begin{proposition}\label{imring}
Let $R$ and $S$ be rings and $f : R\longrightarrow S$ a ring homomorphism.
\begin{enumerate}
\item If $R'$ is a subring of $R$ then $f (R')$ is a subring of $S$. In particular, $\im f$ is a subring of $S$.
\item Assume that $f (1_R) = 1_S$. Then if $x$ is a unit in $R$, $f (x)$ is a unit in $S$ and $(f(x))^{-1} = f(x^{-1})$. In this case $f$ restricts to a group homomorphism $f|_{R^{\times}} : R^{\times} \to S^{\times}$.
\end{enumerate}
\end{proposition}

\begin{proof}
(1) This follows easily from the \hyperref[testforsubring]{Test for a Subring}.

(2) If $x\in R^{\times}$ then $x^{-1}$ exists and $$f(x)f(x^{-1}) = f(xx^{-1}) = f(1_R) = 1_S$$ and similarly $f(x^{-1})f(x) = 1_S$ so that $f(x)\in S^{\times}$ with inverse $f(x^{-1})$.
\end{proof}


\begin{rem} It is \textit{not} true that the intersection of two subrings of $R$ is a subring of $R$. For example, let $$R' = \left\{ \begin{pmatrix} a & b & 0 \\ 0 & a & 0 \\ 0 & 0 & 0 \end{pmatrix} : a,b\in \mathbb{Q} \right\}, \qquad R'' = \left\{ \begin{pmatrix} c & d & e \\ 0 & c & f \\ 0 & 0 & c\end{pmatrix} : c,d,e,f\in \mathbb{Q}\right\}.$$ Then $R', R''$ are both subrings of ${\sf Mat}(3;\mathbb{Q})$, but their intersection $R'\cap R''$ is not since it does not have an identity.
\end{rem}

\section{Equivalence Relations}
\label{equivrel}
This is a short section, you have met this material in \href{http://www.drps.ed.ac.uk/11-12/dpt/cxmath08059.htm}{Proofs and Problem Solving}. I will use the material in the \hyperref[FRFIT]{section on factor rings} and I will also give here the avatar for the first isomorphism theorem, which you will meet properly in that section too.

\begin{definition}
A {\bf relation} $R$ on a set $X$ is a subset $R\subseteq X\times X$. In this context, and only in this context, instead of writing $(x,y)\in R$, I will write $xRy$. Then $R$ is an {\bf equivalence relation on $X$} when for all elements $x,y,z\in X$ the following hold:
\begin{enumerate}
\item {\bf Reflexivity}: $xRx$;
\item {\bf Symmetry}: $xRy \Leftrightarrow yRx$;
\item {\bf Transitivity}: $(xRy \text{ and }yRz) \to xRz$.
\end{enumerate}
\end{definition}

\begin{ex}\label{conjugacy}
Let $F$ be a field and $n$ a positive integer. Define $\simeq$ to be the relation of {\bf conjugacy} on ${\sf Mat}(n; F)$: $A\simeq B$ if and only if there exists an invertible matrix $X\in {\sf Mat}(n;F)$ such that $B = XAX^{-1}$. Then $\simeq$ is an equivalence relation. This is obvious! $A\simeq A$ using $X = I_n$; if $A\simeq B$ then there exists invertible $X$ with $B= XAX^{-1}$ so that $A = X^{-1}BX$; finally if $A\simeq B$ and $B\simeq C$ then there exist invertible $X$ and $Y$ with $B = XAX^{-1}$ and $C= YBY^{-1}$. It follows that $C = (YX) A (YX)^{-1}$, so that $A\simeq C$.
\end{ex}

\begin{exercise}
\label{snfeq}
Show that: the relation $\approx$ on ${\sf Mat}(n\times m; F)$, defined by $A\approx B$ if there exist $P\in GL(n; F)$ and $Q\in GL(m;F)$ such that $B = PAQ$, is an equivalence relation.
\end{exercise}

\begin{exercise}
\label{imvseq}
Show that: isomorphism is an equivalence relation on finite dimensional vector spaces over a field $F$.
\end{exercise}

\begin{definition}
\label{thisneedsalabel}
Suppose that $\sim$ is an equivalence relation on a set $X$. For $x\in X$ the set $E(x) := \{ z\in X: z\sim x\}$ is called the {\bf equivalence class of $x$}. A subset $E\subseteq X$ is called an {\bf equivalence class} for our equivalence relation if there is an $x\in X$ for which $E = E(x)$. An element of an equivalence class is called a {\bf representative} of the class. A subset $Z\subseteq X$ containing precisely one element from each equivalence class is called a {\bf system of representatives} for the equivalence relation.
\end{definition}

\phantomsection
\label{equivclasspartition}
As you know: reflexivity gives $x\in E(x)$, from which it follows easily that for $x,y\in X$ the following are equivalent:
\begin{enumerate}
\item $x\sim y$;
\item $E(x) = E(y)$;
\item $E(x)\cap E(y)\neq \emptyset$.
\end{enumerate}

\begin{ex}
\label{congex}
Given an integer $m\in \mathbb{Z}$, let $\equiv$ be the relation on $\mathbb{Z}$ of ``congruence modulo $m$"  from \hyperref[congmodm]{Example \ref{congmodm}}. This is an equivalence relation where I denoted  the equivalence classes $E(x)$ by $\overline{x}$ in \hyperref[congmodm]{Example \ref{congmodm}}. If $m$ is positive, then $\{0, 1, \ldots , m-1\}$ is a system of representatives, as is $\{ a , a+1, \ldots , a+m-1 \}$ for any integer $a$.
\end{ex}

\begin{exercise}
Show that: the $(n\times m)$-matrices over $F$ in \hyperref[smithform]{Smith Normal Form} form a system of representatives for the equivalence relation of \hyperref[imvseq]{Exercise \ref{snfeq}}.
\end{exercise}

\begin{exercise}
Show that: The set $\{ F^n: n\in \mathbb{Z}_{\geqslant 0}\}$ is a system of representatives for the equivalence relation of \hyperref[imvseq]{Exercise \ref{imvseq}}. Show that: $\{F[X]_{<n}: n\in \mathbb{Z}_{\geqslant 0}\}$ is another system of representatives for the equivalence relation of \hyperref[imvseq]{Exercise \ref{imvseq}}.
\end{exercise}

\begin{definition}Given an equivalence relation $\sim$ on the set $X$ I will denote the {\bf set of equivalence classes}, which is a subset of the power set $\mathcal{P}(X)$, by $$(X/\!\!\sim) := \{ E(x) : x\in X\}$$ There is a canonical mapping ${\sf can}: X \to (X/\!\!\sim), x\mapsto E(x)$. It is obviously a surjection.
\end{definition}

Here are some examples of equivalence relations on a set $X$ with an algebraic structure such that the set of equivalence classes $X/\!\!\sim$ has the same algebraic structure and the canonical mapping
 ${\sf can}: X \to (X/\!\!\sim)$ is a surjective homomorphism preserving the structure:
 \begin{enumerate}
 \item Given an abelian group $A$ and a subgroup $B\subseteq A$ define an equivalence relation
 $\sim$ on $A$ by $x \sim y$ if $x-y \in B$, so that the equivalence class of $x \in A$ is
 $E(x)=B+x=\{b+x \,\vert\, b \in B\} \subseteq A$. The quotient set $A/\!\!\sim=A/B$ is
 an abelian group with $E(x)+E(y)=E(x+y)$, and  ${\sf can}: A \to A/B$ is a surjective homomorphism of abelian group with kernel ${\sf can}^{-1}(0)=B$. The abelian group $A/B$ is the {\bf quotient abelian group} of $A$ by the subgroup $H$.
 \item Given a group $G$ and a normal subgroup $H\subseteq G$ define an equivalence relation
 $\sim$ on $G$ by $x \sim y$ if $xy^{-1}\in H$, so that the equivalence class of $x \in G$ is
 the coset  $E(x)=xH=Hx \subseteq G$. The quotient set $G/\!\!\sim=G/H$ is
 a group with $E(x)E(y)=E(xy)$, and  ${\sf can}: G \to G/H$ a surjective homomorphism of groups
 with kernel ${\sf can}^{-1}(1)=H$.  ((1) is just the abelian version).
 The group $G/H$ is the {\bf quotient group} of $G$ by the normal subgroup $H$.
  Lagrange's Theorem: if $G$ is finite then
 $$\vert G \vert = \vert H \vert \,\vert G/H\vert~,$$
 which is proved by noting that  each coset $E(x)$ has exactly $\vert H \vert$ elements, and that $G$ is the disjoint union of $\vert G/H\vert$ cosets.
 \item   Given an $F$-vector space $V$ and a subspace $W \subseteq V$ define an equivalence relation
 $\sim$ on $V$ by $x \sim y$ if $x-y \in W$, so that the equivalence class of $x \in V$ is
 the subset $E(x)=W+x \subset V$ (exactly as in (a)). The quotient set $V/\!\!\sim=V/W$ is
 an $F$-vector space with $\lambda E(x)=E(\lambda x)$ $(\lambda \in F)$, and  ${\sf can}: V \to V/W$ a surjective linear map with kernel ${\sf can}^{-1}(0)=W$.
 The vector space $V/W$ is the {\bf quotient vector space} of $V$ by the subspace $W$.
   If $V$ is finite-dimensional then so are $W$ and $V$, and by Example \ref{vsquot} below
 $${\rm dim}(V/W)={\rm dim}(V)-{\rm dim}(W)~.$$
 \item In Section \ref{FRFIT} below we shall consider the equivalence relations $\sim$ on rings $R$
 such that the quotient $R/\!\!\sim$ is a ring and the canonical mapping
 ${\sf can}: R \to (R/\!\!\sim)$ is a surjective homomorphism of rings.
 \end{enumerate}


\begin{ex} Let $\equiv$ be the equivalence relation on $\mathbb{Z}$ of ``congruence modulo $m$" from \hyperref[congex]{Example \ref{congex}}. Then $ (\mathbb{Z}/\!\!\equiv) = \mathbb{Z}/m\mathbb{Z}$.
\end{ex}
\medskip

\noindent
\textcolor{black}{\bf \it A very important Remark!}\label{veryimportant}
Suppose that $\sim$ is an equivalence relation on $X$. If $f: X\to Z$ is a mapping with the property that $x\sim y \to f(x) = f(y)$, then there is a unique mapping $\overline{f}: (X/\!\!\sim) \to Z$ with $f = \overline{f}\circ {\sf can}$. Its definition is easy: $\overline{f}(E(x)) = f(x)$. This property is called the {\bf universal property of the set of equivalence classes}.

$$
\begin{tikzpicture}[descr/.style={fill=white,inner sep=2.5pt}]
\matrix (m) [matrix of math nodes, row sep=3em,
column sep=3em]
{ X & (X/\sim) \\
& Z \\ };
\path[->,font=\scriptsize]
(m-1-1) edge node[auto] {${\sf can}$} (m-1-2)
edge node[auto,swap] {$f$} (m-2-2);
\path[->,dashed, font=\scriptsize]
(m-1-2) edge node[auto] {$\overline{f}$} (m-2-2);
\end{tikzpicture}
$$
\medskip

Now suppose that $f: X\to Z$ is an arbitrary mapping. I can define a relation on $X$ by $x\sim y \Leftrightarrow f(x) = f(y)$. It is trivial to check that this is an equivalence relation (do it!). Moreover \begin{equation}\label{avatar} \overline{f}: (X/\!\!\sim) \stackrel{\sim}{\to} \im f\end{equation} is a bijection where, as usual, $\im f = \{ f(x): x\in X\}$. This bijection is the avatar of the first isomorphism theorem.

\begin{definition} \label{welldefdef} I say that $g: (X/\!\!\sim) \to Z$ is {\bf well-defined} if I can find a mapping $f: X\to Z$ such that $f$ has the property $x\sim y \to f(x) = f(y)$ and $g = \overline{f}$.
\end{definition}

\begin{exercise}
Define a relation $\sim$ on $\mathbb{N}\times \mathbb{N}$ by $(x,y)\sim (a,b) \Leftrightarrow x+b = y+a$.
\begin{enumerate}
\item[(a)] Show that: $\sim$ is an equivalence relation.
\item[(b)] Let $\overline{\mathbb{N}} = \mathbb{N}\times \mathbb{N} / \!\!\sim$. Show that: addition on $\mathbb{N}$ induces a well-defined addition on $\overline{\mathbb{N}}$.
\item[(c)] Show that: with this addition, $\overline{\mathbb{N}}$ is an abelian group.
\item[(d)] Show that: ${\sf nat}: \mathbb{N}\to \overline{\mathbb{N}}$ is an additive mapping where ${\sf nat} (a) = E((a+n,n))$ for any $n\in \mathbb{N}$ (that is ${\sf nat}( a + b) = {\sf nat}(a) + {\sf nat}(b)$).
\item[(e)] Show that $\overline{\mathbb{N}}$ is isomorphic as a group to $(\mathbb{Z}, +)$.
\end{enumerate}
\end{exercise}


\section{Factor Rings and the First Isomorphism Theorem}
\label{FRFIT}
I mentioned in the lectures why Lagrange's Theorem from \href{http://www.drps.ed.ac.uk/12-13/dpt/cxmath08064.htm}{Fundamentals of Pure Mathematics} is very useful: it reduces one aspect of complex objects, groups, to something substantially simpler, arithmetic. But I think its proof is not completely satisfying for such a fundamental result, at least at first. If you don't remember the proof, go back to last year's notes or \href{http://en.wikipedia.org/wiki/Lagrange's_theorem_(group_theory)#Proof_of_Lagrange.27s_Theorem}{look here}. It uses left or right cosets. Cosets make the proof pretty clear -- you can't forget it...right? -- but nonetheless they seem to appear as if by magic. One of the jobs of this section is to demystify them. The other job is to prove the most powerful algebraic theorem you've seen yet. Oh, and by the way, by the end of this section you will be considerably more mature as a mathematician than you were when you began reading it. All that in only five pages?
\medskip

I'm going to deal with rings rather than groups -- we are, after all, in a chapter entitled ``Rings and Modules". But everything I write translates naturally to groups, and so in particular explains cosets and normal subgroups in that context.
\medskip

I want to take as a starting point ring homomorphisms $$f: R\to S.$$ I've already stressed the importance of homomorphisms for vector spaces; the same is true for rings, so it is a reasonable beginning. I showed you in \hyperref[avatar]{Equation \eqref{avatar}} how a mapping between two sets produced a bijection. I can apply this logic to $f$, since it is in particular a mapping between two sets. This tells me to consider the following equivalence relation on $R$:
$$ x\sim y \Leftrightarrow f(x)=f(y).$$ So far, so good; but now I will use that $f$ is not just any mapping, but a ring homomorphism. This means that $$x\sim y \Leftrightarrow f(x) = f(y) \Leftrightarrow f(x-y) = 0_S \Leftrightarrow x-y \in \ker f.$$ So what are the equivalence classes in this example? I claim that $$E(x) = x + \ker f := \{ x +k : k\in \ker f \}.$$ I think this is obvious: by definition, $y\in E(x)$ if and only if $y\sim x$ if and only if $y-x\in \ker f$; this happens if and only $y = x + k$ for some $k\in \ker f$ and so indeed $E(x) = x + \ker f$.
\bigskip

Now stand back for a minute and look what we have just deduced:
\begin{itemize}
\item the rule $x \sim y \Leftrightarrow x-y \in \ker f$ is an equivalence relation;
\item the equivalence classes are the sets $x + \ker f$ for $x\in R$;
\item the set of equivalence classes $(R/\sim)$ is a ring, isomorphic to a subring of $S$.
\end{itemize}
Actually, I haven't mentioned the last property. But \eqref{avatar} shows that $(R/\sim) \stackrel{\sim}{\to} \im f$ is a bijection. By \hyperref[imring]{Proposition \ref{imring}.1} $\im f$ is a subring of $S$ and I can use this bijection to make $(R/\sim)$ a ring by declaring this bijection to be an isomorphism.
\bigskip

Now stand even further back. Remember that $\ker f$ is an ideal of $R$ by \hyperref[kerideal]{Proposition \ref{kerideal}}. Everything I say below now becomes completely natural, and I hope obvious\footnote{Possibly after several re-readings!}.
\bigskip

\begin{definition} Let $I\unlhd R$ be an ideal in a ring $R$. The set $$ x+I := \{ x + i : i\in I \} \subseteq R$$ is a {\bf coset of $I$ in $R$} or the {\bf coset of $x$ with respect to $I$ in $R$}.
\end{definition}
\begin{rem} This a special case of cosets of a subgroup of a group. By \hyperref[defideal]{Definition \ref{defideal} (1) and (2)}, $I$ is a subgroup of the abelian group $(R, +)$. So $x+I$ is the left coset of $x$ with respect to $I$ in sense of group theory; since $(R, +)$ is abelian it is also the right coset of $x$ with respect to $I$. From this it follows from the Rules for Cosets that you learned in \href{http://www.drps.ed.ac.uk/12-13/dpt/cxmath08064.htm}{Fundamentals of Pure Mathematics} that there is an equivalence relation on $R$ defined by $$x\sim y \Leftrightarrow x-y \in I$$ whose equivalences classes $E(x)$ are the cosets $x+I$. In particular, by the comments following \hyperref[equivclasspartition]{Definition \ref{thisneedsalabel}} for each $x,y\in R$ either $x+I = y+I$ or $(x+I)\cap (y+I) = \emptyset$ according to $x\sim y$ or not.
\end{rem}

\begin{definition}
Let $R$ be a ring, $I\unlhd R$ an ideal, and $\sim$ the equivalence relation defined by $x\sim y \Leftrightarrow x-y\in I$. Then $R/I$, {\bf the factor ring of $R$ by $I$} or {\bf the quotient of $R$ by $I$}, is the set $(R/\sim)$ of cosets of $I$ in $R$.
\end{definition}

Obviously, by what I've just written, there's a theorem to prove to justify the name ``factor ring". Here it is:

\begin{theorem}\label{factorringthm}
Let $R$ be a ring and $I\unlhd R$ an ideal. Then $R/I$ is a ring, where the operation of addition is defined by
$$ (x+I ) \dotplus (y+ I )  = (x+y) +I \qquad \text{for all $x,y\in R$}$$ and multiplication is defined by  $$(x+I)\cdot (y+I) = xy + I \qquad \text{for all $x,y\in R$}.$$
\end{theorem}
I've included $\dotplus$ and $\cdot$ here for the usual didactic reason: it will help you keep track of the addition and multiplication defined in $R/I$ as opposed to in $R$. I'll drop them again immediately after the proof.

There's quite a lot to do to prove this result, but it mostly revolves around checking that the above definitions are well defined. This means, for example, the following: it might be that there is an equality of sets $x+I = x'+I$ with distinct $x, x' \in R$. If so, it had better be that $(x+I)(y+I)$ and $(x'+I)(y+I)$ give the same answer. But the first is $xy+I$ while the second is $x'y+I$. It's my job to explain why $xy+I = x'y +I$. If that were not true, then the multiplication would not be well-defined because I would not get a unique answer on multiplying the cosets $x+I$ and $y+I$ together. Writing this differently, I need to explain in this case why the mapping $g: R/I \to R/I$ defined by $g(x+I) = xy +I$ is well-defined, in the sense of \hyperref[welldefdef]{Definition \ref{welldefdef}}.
\begin{proof}
I first prove that $R/I$ is an abelian group under addition.

I begin by showing that addition is well-defined. Suppose the $x,x'\in R$ are such that $x+I = x'+I$ and $y,y\in R$ are such that $y+I = y'+I$. I need to prove that $$(x+I) \dotplus (y+I) = (x'+I) \dotplus (y'+I)$$ This, by definition, is the same as proving that $(x+y)+I = (x'+y')+I$, which in turn is the same as checking that $(x+y)-(x'+y') \in I$. This last statement is obvious, since we know by assumption that $x-x'\in I$ and $y-y'\in I$ and since $I$ is an ideal we get $(x-x') + (y-y')\in I$.

That's the first piece of hard work. It is obvious that $0+I$ is the additive identity: $$(0+I) \dotplus (x+I) = x+I = (x+I)\dotplus (0+I)$$ It is similarly obvious that $-x + I$ is the inverse to $x+I$: $$(-x+I) \dotplus (x+I) = (-x + x) + I = 0 +I = (x+I) \dotplus (-x+I).$$ Finally, associativity of addition follows from the associativity in $R$: \begin{eqnarray*} ((x+I)\dotplus(y+I))\dotplus (z+I) &=& ((x+y)+I) \dotplus (z+I) \\ &=& ((x+y)+z)+I \\ &=& (x+(y+z))+I \\ &=& (x+I)\dotplus((y+I)\dotplus (z+I))\end{eqnarray*}

Now to multiplication. Suppose that \begin{equation} \label{coseteq} x+ I = x' + I \qquad \text{and} \qquad y+ I = y' + I\end{equation} for some $x,x',y,y' \in R$. I must show that \begin{equation*} (x+I)\cdot (y+ I ) = (x' +I)\cdot (y'+I).\end{equation*}
By (\ref{coseteq}) we have $x-x' = i$ and $y-y' = j$ for some $i,j\in I$. Thus
\begin{eqnarray*} xy - x'y'&= &(x' +i) (y'+j) - x'y' \\ & = & x'y' + iy' + x'j + ij - x'y' \\ & = & iy' + x'j + ij . \end{eqnarray*}  Since $I$ is an ideal and $i,j\in I$ I know $iy', x'j, ij \in I$ and hence $iy' + x'j +ij \in I$. Thus $xy - x'y' \in I$, which is to say $xy + I = x'y' +I$. Thus $$ (x+I)\cdot (y+I) = xy + I = x'y' + I = (x'+I)\cdot (y'+I)$$ as required.

$R/I$ is clearly closed under multiplication. For all $x,y,z\in R$ \begin{eqnarray*} (x+I)\cdot \left((y+I)\cdot (z+I)\right) &=& (x+I)\cdot (yz+I) \\ & = & x(yz) +I \\ & =& xyz + I \end{eqnarray*} and similarly for $\left((x+I)\cdot (y+I)\right) \cdot (z+I)$ so that multiplication is associative. Furthermore, for any $x\in R$ $$(x+I)\cdot (1 + I) = (x1) + I = x + I = (1x) + I =  (1+I)\cdot (x+I),$$ so that $(R/I, \cdot)$ is a monoid.

Finally, the distributive axioms hold: \begin{eqnarray*} (x+I)\cdot \left( (y+I) \dotplus (z+I) \right) &=& (x+I)\cdot( (y+z)+I ) \\ &=& x(y+z) + I \\ &=& (xy + xz) + I \\ &=& (xy+I)\dotplus (xz+I) \\ &=& (x+I)\cdot (y+I) \dotplus (x+I)\cdot (z+I)\end{eqnarray*}

So, as I claimed, $R/I$ is a ring.
\end{proof}

\begin{ex}  For any $m\in \mathbb{Z}$, $\mathbb{Z} /m\mathbb{Z}$ is a ring. But you knew that, didn't you? But do think again about the comment in parentheses made at the end of \hyperref[congmodm]{Example \ref{congmodm}}. \end{ex}

\begin{ex} \label{compasquot} Let $R = \mathbb{R}[X]$ and $I = {}_{\mathbb{R}[X]}\langle X^2+1 \rangle$. Then $R/I$ is isomorphic to the complex numbers! See \hyperref[compfact]{Example \ref{compfact}} for the explanation.
\end{ex}

\begin{exercise}
Let $R$ be a ring and let $I$ be an ideal of $R$. Show that: if $R$ is commutative then so is $R/I$.
\end{exercise}
\begin{exercise}
Let $R$ be a ring and let $I$ be an ideal of $R$. Show that: $R/I$ is a non--zero ring if and only $I \neq R$
\end{exercise}
\begin{exercise}
Let $R$ be a ring and let $I$ be a proper ideal of $R$, that is $I\neq R$. Show that: if $r \in R^{\times}$, then $r+I \in (R/I)^{\times}$ with $(r+I)^{-1} = r^{-1} + I$.
\end{exercise}

You should compare the following theorem with the \hyperref[veryimportant]{very important remark} in Section \ref{equivrel}
\begin{theorem}[The Universal Property of Factor Rings] \label{uniring}Let $R$ be a ring and $I$ an ideal of $R$.
\begin{enumerate}
\item The mapping ${\sf can}: R\to R/I$ sending $r$ to $r+I$ for all $r\in R$ is a surjective ring homomorphism with kernel $I$.
\item If $f: R\to S$ is a ring homomorphism with $f(I) = \{ 0_S \}$, so that $I\subseteq \ker f$, then there is a unique ring homomorphism $\overline{f} : R/I \to S$ such that $f = \overline{f}\circ {\sf can}$.
\end{enumerate}
\end{theorem}
I like to remember this theorem by drawing a diagram:
$$
\begin{tikzpicture}[descr/.style={fill=white,inner sep=2.5pt}]
\matrix (m) [matrix of math nodes, row sep=3em,
column sep=3em]
{ R & R/I \\
& S \\ };
\path[->,font=\scriptsize]
(m-1-1) edge node[auto] {${\sf can}$} (m-1-2)
edge node[auto,swap] {$f$} (m-2-2);
\path[->,dashed, font=\scriptsize]
(m-1-2) edge node[auto] {$\overline{f}$} (m-2-2);
\end{tikzpicture}
$$
The second part of the Theorem states that $f$ {\bf factorises uniquely} through the canonical mapping to the factor whenever the ideal $I$ is sent to zero.

\begin{ex} We have $$\begin{tikzpicture}[descr/.style={fill=white,inner sep=2.5pt}]
\matrix (m) [matrix of math nodes, row sep=3em,
column sep=3em]
{ \mathbb{Z} & \mathbb{Z}/24\mathbb{Z} \\
& \mathbb{Z}/12 \mathbb{Z} \\ };
\path[->,font=\scriptsize]
(m-1-1) edge node[auto] {${\sf can}$} (m-1-2)
edge node[auto,swap] {$f$} (m-2-2);
\path[->,dashed, font=\scriptsize]
(m-1-2) edge node[auto] {$\overline{f}$} (m-2-2);
\end{tikzpicture}
$$
where $f: \mathbb{Z} \to \mathbb{Z}/12\mathbb{Z}$ is the canonical mapping. Here the induced mapping $\overline{f}$ sends the coset $a + {24}\mathbb{Z}$ to the coset $a + {12}\mathbb{Z}$.
\end{ex}
\begin{proof}
The first statement is pretty clear: it is certainly a surjective mapping where the elements of $I$ are precisely those being sent to $0_{R/I} $. That it is a ring homomorphism follows from the definition of addition and multiplication in $R/I$: $$ {\sf can}(x) + {\sf can}(y) = (x+I)+(y+I) = (x+y)+I = {\sf can}(x+y)$$ and $${\sf can} (x) {\sf can} (y) = (x+I)(y+I) = (xy) + I = {\sf can} (xy)$$

For the second claim, note that because of the condition $f(I) = \{ 0 \}$, any coset of $I$ in $R$ gets sent to a single element in $S$: $$f(x+ I) = f(x) + f(I) = \{ f(x) \}$$ I'll call this element $\overline{f}(x+I)$ so that $\overline{f}(x+I) = f(x)$ and $f(x+I) = \{ \overline{f}(x+I) \}$. This produces the only possible mapping $\overline{f}$ satisfying $f = \overline{f} \circ {\sf can}$. To check that $\overline{f}$ is a ring homomorphism, use that $$\overline{f}((x+I) + (y+I)) = \overline{f}((x+y)+I) = f(x+y) = f(x) + f(y) = \overline{f}(x+I) + \overline{f}(y+I)$$ and $$\overline{f}((x+I)(y+I)) = \overline{f}(xy+I) = f(xy) = f(x)f(y) = \overline{f}(x+I)\overline{f}(y+I)$$
\end{proof}

\begin{theorem}[First Isomorphism Theorem for Rings] \label{fitr} Let $R$ and $S$ be rings. Then every ring homomorphism $f : R\longrightarrow S$ induces a ring isomorphism $$\overline{f}: R/\ker f \stackrel{\sim}{\to} \im f.$$ \end{theorem}

\begin{ex}\label{compfact}
Let's go back to \hyperref[compasquot]{Example \ref{compasquot}}. \hyperref[rootdef]{Evaluation} at $\sqrt{-1}$ produces a ring homomorphism $f: \mathbb{R}[X] \to \mathbb{C}$. By \hyperref[euclid]{Theorem \ref{euclid}} each polynomial $P\in \mathbb{R}[X]$ can be written uniquely as $P = A(X^2+1) + B$ where $B = a +bX $ is a linear polynomial. Then $f(P) = f(B) = a+b\sqrt{-1}$ which shows that $f$ is surjective and that $P\in \ker f$ if and only if $a = b = 0$, so that ${}_{\mathbb{R}[X]}\langle X^2 + 1 \rangle = \ker f$. Applying the First Isomorphism Theorem completes the claim that $R/I \cong \mathbb{C}$.
\end{ex}

\begin{proof} Clearly $\overline{f}$ is surjective. It's injective by \hyperref[injring]{Lemma \ref{injring}} since the only element in the kernel of $\overline{f}$ is the coset $0 + \ker f$, the zero element of $R/\ker f$.  \end{proof}

This completes the explanation and generalisation of all the comments I made at the introduction to this Section. And better still, you learned one additional fact: thanks to \hyperref[uniring]{Theorem \ref{uniring}.1}, each ideal $I$ of $R$ is the kernel of at least one ring homomorphism, namely ${\sf can}: R\to R/I$.  So ideals really are kernels, and kernels really are ideals.

\section{Modules and All That}
{Do not misunderstand the following waffle as the basis for a philosophical theory of anything!} I've now shown many of the basic properties of rings and their homomorphisms. I also showed in \hyperref[fielddef]{Definition \ref{fielddef}} that the definition of a ring generalises that of a field. Generalisation for its own sake is at best a derivative activity, but it is a strong argument to propose part of the art of mathematics to find the {\bf correct level of generality}. So what might that vague phrase mean? Well, I'd suggest \textcolor{black}{\it power, beauty and ubiquity}. A theory being developed should:
\begin{enumerate}
\item[(a)] have decent theorems (the \textcolor{black}{\it power}),
\item[(b)] proceed naturally and hang together well (the \textcolor{black}{\it beauty})
\item[(c)] unify  many examples or be widely applicable (the \textcolor{black}{\it ubiquity}).
\end{enumerate}
For the generalisation from fields to rings, think about the First Isomorphism Theorem for (a) and (b), and the natural examples I introduced such as $\mathbb{Z}$ and $\mathbb{C}[X]$ -- which are only the very beginning -- for (c).

Now ratiocinate! You might speculate that I should be able to define something like a vector space, generalising scalars from fields to rings. But for this to be at the correct level of generality, I'd need to have natural theorems and convincing examples of whatever this structure is.

\begin{definition} \label{moduledef} A {\bf (left) module $M$ over a ring $R$} is a pair consisting of an abelian group $M = (M,\dotplus)$ and a mapping
\begin{eqnarray*} R\times M &\to & M  \\ (r , a) &\mapsto& ra
\end{eqnarray*}
such that for all $r, s \in R$ and $a,b \in M$ the following identities hold:
\begin{eqnarray*}
r (a\dotplus b) &=& (ra) \dotplus (rb) \\ (r+s) a &=& (ra) \dotplus (sa) \\ r(s a) &=& (rs)a \\ 1_R a &=& a
\end{eqnarray*}
The first two laws are the {\bf Distributive Laws}; the third law is called the {\bf Associativity Law}. I will often also call a left module $M$ over a ring $R$ an {\bf $R$-module}.
\end{definition}

\begin{rem} Don't worry: there is a notion of right $R$-module! I'll leave you to work out what it must be. Everything I do in this course for left $R$-modules could also have been done equally for right $R$-modules; but since I've just mentioned that, I won't discuss right $R$-modules again.  \end{rem}

 \begin{ex}
If $R=F$ is a field then $R$--modules are just $F$-vector spaces. This is obvious: when I latexed \hyperref[moduledef]{Definition \ref{moduledef}} I cut-and-pasted \hyperref[vsdef]{the definition of a vector space} and then replaced the each appearance of the field $F$ in the original with the ring $R$.
 \end{ex}
 \begin{ex}
Let $R = \mathbb{Z}$. A $\mathbb{Z}$-module is exactly the same as an abelian group. Since any module is an abelian group by definition, I am therefore claiming the converse: any abelian group $M$ is a $\mathbb{Z}$-module. This is straightforward, and just depends on the Rules for Multiples in abelian groups, which you saw in \href{http://www.drps.ed.ac.uk/12-13/dpt/cxmath08064.htm}{Fundamentals of Pure Mathematics} and I which outlined again in \hyperref[ruleformult]{Lemma \ref{ruleformult}}.
 \end{ex}
 \begin{ex}
 Let $I$ be an ideal in a ring $R$. Then $I$ is an
 $R$--module using the multiplication in the ring. In particular,
 $R$ itself is an $R$--module.
  \end{ex}
  \begin{ex}
  The singleton $\{ 0 \}$ is an $R$-module for any $R$, with addition and multiplication defined by $0 \dotplus 0 = 0$ and $r0 = 0$ for all $r\in R$. It is the {\bf zero module} or {\bf trivial module}.
  \end{ex}
  \begin{exercise} \label{matexmod} Let $S$ be a ring and let $R = {\sf Mat}(n; S)$, the ring of $(n\times n)$-matrices with coefficients in $S$. Let $M = S^n$. Show that: $M$ is an $R$-module under the operations of componentwise addition and matrix multiplication.
  \end{exercise}
  \begin{exercise}\label{forlater}
  Let $V$ be an $F$-vector space for some field $F$ and let $\phi\in {\rm End}(V)$ be an endomorphism of $V$. Show that: $V$ is an $F[X]$-module under the operation $$ (a_mX^m + a_{m-1}X^{m-1} + \cdots +a_1X + a_0 ) \vec v =  a_m\phi^m(\vec{v}) + a_{m-1}\phi^{m-1}(\vec{v}) + \cdots +a_1\phi(\vec{v}) + a_0 \vec{v}.$$ I will denote this $F[X]$-module by $V_\phi$.
    \end{exercise}

 \begin{ex}
Given a ring $R$ and $R$-modules $M_1, \ldots, M_n$, the cartesian product  $M_1\times M_2\times \cdots \times
 M_n$ is an $R$--module if I define addition and multiplication as follows:
 $$
 (a_1, \ldots, a_n) + (b_1, \ldots, b_n) = (a_1 + b_1 , \ldots
 , a_n + b_n)$$ and $$ r(a_1,\ldots , a_n) = (ra_1, \ldots ,
 ra_n)$$ for all $r\in R$ and $a_i, b_i \in M$.
 This is denoted $M_1 \oplus \cdots \oplus M_n$ and called the {\bf
 direct sum}. It generalises the construction for vector spaces given in \hyperref[prodvs]{Exercise \ref{prodvs}}.
 \end{ex}

You should be able to guess what I am going to say now. After all, the first thing I did after defining vector spaces was to conduct a hygiene-check.

\begin{lemma} Let $R$ be a ring and $M$ an $R$--module.
 \begin{enumerate}
 \item $0_R a = 0_M$ for all $a\in M$.
 \item $r0_M = 0_M$ for all $r\in R$.
 \item $(-r)a = r (-a) = - (ra)$ for all $r\in R, a\in M$. Here the first negative is a negative in $R$, the last two are negatives in $M$.
 \end{enumerate}
\end{lemma}
\begin{proof} Exactly the same as the proofs for \hyperref[zerovs]{Lemma \ref{zerovs}}, \hyperref[-1vs]{Lemma \ref{-1vs}} or \hyperref[zeroinring]{Lemma \ref{zeroinring}.2},  and the first statement of \hyperref[prodzero]{Lemma \ref{prodzero}}.
\end{proof}
What happened to the analogue of the second statement of \hyperref[prodzero]{Lemma \ref{prodzero}}? That stated: if $\lambda \in F$ and $\vec{v} \in V$ satisfy $\lambda \vec{v} = \vec{0}$ then either $\lambda = 0$ or $\vec{v} = 0$. Well, remember how you proved it: you assumed that $\lambda$ was non-zero, multiplied by $\lambda^{-1}\in F$ and then deduced that $\vec{v}=\vec{0}$. See the appearance of $\lambda^{-1}$ in that argument? That's why you can't deduce such a result for $R$-modules in general.

\begin{ex}
Let $R = {\sf Mat}(2; \mathbb{C})$ and $M = \mathbb{C}^2$ as in \hyperref[matexmod]{Exercise \ref{matexmod}}. Then $$\begin{pmatrix} 0 & 1 \\ 0 & 0  \end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}$$
\end{ex}

\begin{ex}
Let $V = \mathbb{C}^2$ and $\phi\in {\rm End}(V)$ whose representative with respect to the standard basis is $$[\phi] = \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}$$ Then $X \vec{e}_1 = \vec{0}$ in the $F[X]$-module $V_\phi$, defined as in \hyperref[forlater]{Exercise \ref{forlater}}.
\end{ex}

 I next introduce homomorphisms between $R$-modules. You know what the definition will be: a structure-preserving mapping.

\begin{definition} Let $R$ be a ring and let $M,N$ be $R$--modules. A mapping $f:M \to N$ is
an {\bf $R$--homomorphism} or {\bf homomorphism} if the following hold for all $a,b\in M$ and $r\in R$
\begin{eqnarray*}
f(a+b) &=& f(a) + f(b)\\
f(ra) &=& rf(a)
\end{eqnarray*}
The {\bf kernel} of $f$ is $\ker f = \{ a\in M: f(a) = 0_N\}
\subseteq M$ and the {\bf image} of $f$ is $\im f = \{
f(a) : a\in M\} \subseteq N.$  If $f$ is a bijection then it is an {\bf $R$-module isomorphism} or {\bf isomorphism}, I write $M\cong N$ and say
$M$ and $N$ are {\bf isomorphic}.
\end{definition}
As usual, the composition of two $R$-homomorphisms is again an $R$-homomorphism.
\begin{ex}
 If $M$ and $N$ are $R$--modules then the map $f: M\to N$ defined by $f(a) = 0_N$
for all $a\in M$ is {\it always} an $R$--homomorphism.
\end{ex}

\begin{ex} If $R$ is a field then $F$-homomorphisms are just linear mappings, of course.
\end{ex}

\begin{ex}
Let
$M,N$ be abelian groups. Then any {\it group} homomorphism $f:
M\to N$ is also a $\mathbb{Z}$--homomorphism.
\end{ex}


\begin{exercise} \label{easyexforme}
Let $F$ be a field and let $V$ and $W$ be the $F$-vector spaces $F^2$ and $F^3$ respectively. Let $$\phi = \begin{pmatrix} 0 & 1 \\ 0 & 0\end{pmatrix} \text{ and } \psi =  \begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{pmatrix} $$ and consider the $F[X]$-modules $V_{\phi}$ and $W_{\psi}$ as in \hyperref[forlater]{Exercise \ref{forlater}}. Show that: the mappings $$f: V_\phi \to W_{\psi}, \quad \begin{pmatrix} x \\ y \end{pmatrix} \mapsto  \begin{pmatrix} x \\ y \\ 0 \end{pmatrix} \qquad \text{and} \qquad g: W_\psi \to V_{\phi}, \quad \begin{pmatrix} x \\ y \\ z \end{pmatrix} \mapsto  \begin{pmatrix} z\\ 0 \end{pmatrix} $$ are $F[X]$-homomorphisms.
\end{exercise}

What now? That's right, find out what structure the kernel and the image inherit.

\begin{definition}
A non--empty subset $M'$ of an $R$--module $M$ is a {\bf
submodule} if $M'$ is an $R$--module with respect to the operations
of the $R$--module $M$ {\bf restricted} to $M'$.
\end{definition}

\begin{ex}
 If $V$ is a vector space over a field $R$, then the
submodules of $V$ are the vector subspaces of $V$.
\end{ex}
\begin{ex}
Let $M$ be
an abelian group. Regarding $M$ as a $\mathbb{Z}$--module, its submodules
are precisely the subgroups of $M$.
\end{ex}
\begin{ex}
 For any ring $R$,
regarded as an $R$--module, the ideals of $R$ 
are precisely the submodules of $R$. 
\end{ex}

\begin{ex}
Let $F$ be a field and let $W_{\psi}$ be defined as in \hyperref[easyexforme]{Example \ref{easyexforme}}. The subspaces $\langle \vec{e}_1 \rangle$ and $\langle \vec{e}_1, \vec{e}_2 \rangle$ are $F[X]$-submodules of $W_{\psi}$, whereas $\langle \vec{e}_2 \rangle$ is not.
\end{ex}
\begin{proposition}[Test for a submodule] \label{testforsubmod} Let $R$ be a ring and let $M$ be an $R$--module. A
subset $M'$ of $M$ is a {\it submodule} if and only if
\begin{enumerate} \item $0_M \in M'$
\item $a,b\in M' \Rightarrow a-b\in M'$ \item $r\in R, a\in
M' \Rightarrow ra\in M'$.
\end{enumerate}
\end{proposition}
\begin{proof} This is just as easy as it was in \hyperref[testforsubring]{Proposition \ref{testforsubring}} If $M'$ is a submodule of $M$ then the properties (1)--(3) are obvious. Suppose on the other hand that $M'$ satisfies (1)--(3). Then, by the test for a subgroup, (1) and (2) show that $M'$ is a subgroup of the abelian group $M$. The binary operation $R\times M\to M$ restricts to an operation $R\times M' \to M'$ by Property (3). The axioms for an $R$-module then hold for $M'$ since they hold for the bigger set $M$.\end{proof}

Now go back to \hyperref[highersubdef]{Remark \ref{highersubdef}} and see if that makes sense, now that you've seen {\bf subobjects} defined for vector spaces, for rings, and for modules.

\begin{lemma} Let $f:M\to N$ be an $R$--homomorphism.
Then $\ker f$ is a submodule of $M$ and $\im f$ is a submodule of $N$.
\end{lemma}

\begin{proof} Obviously, I should use \hyperref[testforsubmod]{Lemma \ref{testforsubmod}} in both cases.

For the kernel: (1) $f(0_M) = 0_N$, since $f$ is a homomorphism of abelian groups. So $0_M \in \ker f$.
(2) Let $a,b\in \ker f$. Then $f (a-b) = f(a) -
f(b) =  0_N.$ Hence $a-b\in \ker f$.
(3) Let $a\in \ker f, r\in R$. Then $f (ra) = rf(a) =
r0_N = 0_N$ so that $ra\in \ker f$.

It's not going to be any more exciting for the image: (1) $0_N = f (0_M)$. So $0_N \in \im f$.
(2) Let $a,b\in \im f$. Then $a = f(c), b= f(d)$ for
some $c,d\in M$. Then $a-b = f(c) - f(d) = f(c-d)
\in \im f.$
(3) Let $a\in \im f, r\in R$. Then $a=f(c)$ for some
$c\in M$. Then $ra = rf (c) = f (rc) \in \im f.$
\end{proof}
As usual, the following lemma holds true because it only needs that $f$ is a homomorphism of abelian groups:

\begin{lemma} \label{injmodule} Let $R$ be a ring, let $M$ and $N$ be $R$-modules and let $f: M \to N$ be an $R$-homomorphism. Then $f$ is injective if and only if $\ker f = \{ 0_M \}$. \end{lemma}

\begin{definition} Let $R$ be a ring, $M$ an $R$-module and let $T \subseteq M$. Then the {\bf submodule of $M$ generated by $T$} is the set $${}_R\langle T \rangle =  \{  r_1t_1 + \cdots + r_mt_m : t_1, \ldots , t_m \in T, r_1, \ldots , r_m\in R\},$$ together with the zero element in the case $T =\emptyset$. If $T = \{t_1, \ldots , t_n \}$, a finite set, I will often abuse notation by writing ${}_R\langle t_1, \ldots , t_n \rangle$ instead of ${}_R\langle \{ t_1, \ldots , t_n \}\rangle$. The module $M$ is {\bf finitely generated} if it is generated by a finite set: $M = {}_R\langle t_1, \ldots , t_n \rangle$. It is called {\bf cyclic} if it is generated by a singleton: $M = {}_R\langle t \rangle$.
\end{definition}

\begin{ex} A cyclic group is the same thing as a cyclic $\mathbb{Z}$-module. \end{ex}

\begin{ex} Let $R$ be a commutative ring. Then the \hyperref[genideals]{ideal generated by $T\subseteq R$} is the same thing as the submodule of $R$ generated by $T$. A \hyperref[prindef]{principal ideal} is the same thing as a cyclic submodule of $R$.
\end{ex}

\begin{ex}
Let $F$ be a field and let $W_{\psi}$ be defined as in \hyperref[easyexforme]{Example \ref{easyexforme}}. Then $W_{\psi}$ is a cyclic $F[X]$-module, generated by the element $\vec{e}_3\in W_{\psi}$.\end{ex}

\begin{ex}  $\{0_M \}$ is always a cyclic submodule of an $R$-module $M$, generated by the element $0_M$.
\end{ex}

There are some easy-to-prove results about submodules that you have seen for vector subspaces and for ideals in this course or for groups. I'm going to write them out but not prove them: their proofs are obtained by modifying the proofs you already know in those other cases, so there's no point sentencing you to another twenty years of boredom; there are far more interesting things to be thinking about. In all of the statements $R$ is a ring and $M$ is an $R$-module.

\begin{lemma}
Let $T\subseteq M$. Then ${}_R\langle T
\rangle$ is the smallest submodule of $M$ that contains $T$.
\end{lemma}

\begin{lemma} \label{intsubmod} The intersection of any collection of submodules of $M$ is a submodule of $M$. \end{lemma}

\begin{lemma} Let $M_1$ and $M_2$ be submodules of a $M$. Then $$M_1+M_2 = \{ a+ b: a\in M_1, b\in M_2\}$$ is a submodule of $M$. \end{lemma}

Finally we reach factor modules and the First Isomorphism Theorem for modules.

\begin{thedef} Let $R$ be a ring, $M$ an $R$-module and $N$ a submodule of $M$. For each $a\in M$ the {\bf coset of $a$ with respect to $N$ in $M$} is $$a+N = \{ a+ b: b\in N\}$$ It is a coset of $N$ in the abelian group $M$ and so is an equivalence class for the equivalence relation $a\sim b \Leftrightarrow a-b\in N$. I define $M/N$,
{\bf the factor of $M$ by $N$} or {\bf the quotient of $M$ by $N$}, to be the set $(M/\sim)$ of all cosets of $N$ in $M$. This becomes an $R$--module by introducing the operations of addition and multiplication as follows:
\begin{eqnarray*} (a+N) \dotplus (b+N) &=& (a+b) + N \\ r(a+N) &=& ra + N
\end{eqnarray*}
for all $a,b\in M, r\in R$. I must check that this is well-defined. For addition this follows by the same argument as in the first half of the \hyperref[factorringthm]{Proof of Theorem \ref{factorringthm}}. For multiplication, let $a,b\in M$ be such that $a+N = b+N$. This means that $a-b\in N$, and since $N$ is a submodule I have that $r(a-b) \in N$. So \begin{eqnarray*} r(a+N) - r(b+N) &=& (ra+N) - (rb +N) \\ & = & (ra-rb) + N \\ & =& r(a-b) + N \\ & = & 0 + N.\end{eqnarray*} Thus $r(a+N) = r(b+N)$ as required.

The zero of $M/N$ is the coset $0_{M/N} = 0_M +
N$. The negative of $a+N \in M/N$ is the coset $-(a+N) = (-a) + N$.

The $R$-module $M/N$ is the {\bf factor module} of $M$ by the submodule $N$.
\end{thedef}

\begin{exercise}\label{vsquot}
Let $R=F$ be a field, $V$ an $F$-vector space (= $F$-module) and $W \subseteq V$ a submodule of $V$, which is just a subspace of $V$. The quotient $V/W$ is again an $F$-vector space, naturally called the {\it quotient vector space}.  The canonical projection ${\sf can}:V \to V/W$ is a linear mapping. Assume that $\dim V = m$ is finite, although if I had mentioned the correct things from set theory an appropriate version of the following would be true in general. By the \hyperref[subspacesmall]{Dimension Estimate for Vector Subspace} $\dim W = n \leqslant m$. Let $\{ \vec{v}_1, \ldots , \vec{v}_n\}$ be a basis for $W$ and extend it to a basis $\{\vec{v}_1, \ldots , \vec{v}_n, \vec{v}_{n+1}, \ldots , \vec{v}_m\}$ of $V$ by using the \hyperref[steinitz]{Steinitz Exchange Theorem}. Show that: $\{\vec{v}_{n+1} + W, \ldots , \vec{v}_m +W\}$ is a basis for the vector space $V/W$. In particular, deduce that $\dim V/W = \dim V - \dim W$.
\end{exercise}
You should compare the following theorem with the \hyperref[veryimportant]{very important remark} in Section \ref{equivrel} and with \hyperref[uniring]{Theorem \ref{uniring}}.

\begin{theorem}[The Universal Property of Factor Modules] \label{unimodule} Let $R$ be a ring, let $L$ and $M$ be $R$-modules, and $N$ a submodule of $M$.
\begin{enumerate}
\item The mapping ${\sf can}: M\to M/N$ sending $a$ to $a+N$ for all $a\in M$ is a surjective $R$-homomorphism with kernel $N$.
\item If $f: M\to L$ is an $R$-homomorphism with $f(N) = \{ 0_L \}$, so that $N\subseteq \ker f$, then there is a unique homomorphism $\overline{f} : M/N \to L$ such that $f = \overline{f}\circ {\sf can}$.
\end{enumerate}
\end{theorem}
I like to remember this theorem by drawing a diagram:
$$
\begin{tikzpicture}[descr/.style={fill=white,inner sep=2.5pt}]
\matrix (m) [matrix of math nodes, row sep=3em,
column sep=3em]
{ M & M/N \\
& L \\ };
\path[->,font=\scriptsize]
(m-1-1) edge node[auto] {${\sf can}$} (m-1-2)
edge node[auto,swap] {$f$} (m-2-2);
\path[->,dashed, font=\scriptsize]
(m-1-2) edge node[auto] {$\overline{f}$} (m-2-2);
\end{tikzpicture}
$$
The second part of the Theorem states that $f$ {\bf factorises uniquely} through the canonical mapping to the factor whenever the submodule $N$ is sent to zero.

\begin{proof} This is proved in a completely analogous manner to the proof of \hyperref[uniring]{Theorem \ref{uniring}}.
\end{proof}

\begin{theorem}[First Isomorphism Theorem for Modules] \label{fitm}
Let $R$ be a ring and let $M$ and $N$ be $R$-modules. Then every $R$-homomorphism $f : M\longrightarrow N$ induces an $R$-isomorphism $$\overline{f}: M/\ker f \stackrel{\sim}{\to} \im f.$$
\end{theorem}

\begin{proof}
There's not really anything to do! If follows from everything we've done, just as in \hyperref[fitr]{Theorem \ref{fitr}}.
 \end{proof}

\begin{rem}
If we apply this theorem in the special case when $R= F$ is a field, then we get the {\bf First Isomorphism Theorem for $F$-vector spaces}. By \hyperref[vsquot]{Exercise \ref{vsquot}} the dimension of $M/\ker f$ is $\dim M - \dim \ker f$, and as isomorphic vector spaces have the same dimension we deduce from the First Isomorphism Theorem a new proof of the \hyperref[rnthm]{Rank-Nullity Theorem} as a corollary:
$$\dim M - \dim (\ker f) = \dim (\im f).$$
\end{rem}

\begin{rem}
If we apply this theorem in the special case when $R= \mathbb{Z}$, the integers, then we get the {\bf First Isomorphism Theorem for Abelian Groups}, which is a special case of the {\bf First Isomorphism Theorem for Groups}.
\end{rem}

\begin{ex}
Let $g: W_{\psi} \to V_{\phi}$ be the $F[X]$-homomorphism defined in \hyperref[easyexforme]{Exercise \ref{easyexforme}}. Then $$\ker g = \left\{ \begin{pmatrix} a \\ b\\ 0 \end{pmatrix} : a,b \in F \right\} \text{ and } \im g = \left\{ \begin{pmatrix} c \\ 0 \end{pmatrix} : c\in F\right\}.$$ So in this case the First Isomorphism Theorem states that $$\frac{F^3}{\{ (a,b,0)^{\sf T}\}} \stackrel{\sim}{\to} \{ (c,0)^{\sf T} \}.$$ I hope this looks natural to you! Note that on both the left hand side and the right side multiplication by the element $X\in F[X]$ sends any element to $0$.
\end{ex}

\begin{exercise}[Second Isomorphism Theorem for Modules] Let $N,K$ be submodules of an $R$--module $M$. Show that:
$K$ is a submodule of $N+K = \{ b+c : b\in N, c\in K\}$ and $N\cap
K$ is a submodule of $N$. Show further that: $$\frac{N+K}{K} \cong
\frac{N}{N\cap K}.$$
\end{exercise}

\begin{exercise}[The Third Isomorphism Theorem for Modules] Let $N,K$ be submodules of an $R$--module $M$, where
$K\subseteq N$. Show that: $N/K$ is a submodule of $M/K$ and
$$\frac{M/K}{N/K} \cong M/N.$$
\end{exercise}

\chapter{Determinants and Eigenvalues Redux}

You met determinants of real $(n\times n)$-matrices in \href{www.drps.ed.ac.uk/12-13/dpt/cxmath08057.htm}{Introduction to Linear Algebra}. You must have had several reactions, I'd guess: (i) ``Huh?"; (ii) ``That's horrible to calculate"; (iii) ``Oh, but it's quite useful". There is no ``Huh?" with determinants, and in this chapter you will see why: I'll explain why the definition is inevitable. I will also deal rigorously with the content of \href{www.drps.ed.ac.uk/12-13/dpt/cxmath08057.htm}{Introduction to Linear Algebra}, and then I'll show you how matrices over a specific commutative ring -- that is not a field -- prove the Cayley-Hamilton Theorem for matrices over a field.

\section{The Sign of a Permutation}
The definition of determinants begins with the {symmetric group}, which you met this in \href{http://www.drps.ed.ac.uk/12-13/dpt/cxmath08064.htm}{Fundamentals of Pure Mathematics}:

\begin{definition} The group of all permutations of the set $\{ 1, 2, \ldots , n \}$, also known as bijections from $\{1,2, \ldots ,n \}$ to itself, is denoted by $\mathfrak{S}_n$ and called the {\bf $n$-th symmetric group}. It is a group under composition and it has $n!$ elements.

A {\bf transposition} is a permutation that swaps two elements of the set and leaves all the others unchanged.
 \end{definition}

 \begin{definition}  \label{permdefs} An {\bf inversion} of a permutation $\sigma \in \mathfrak{S}_n$ is a pair $(i,j)$ such that $1\leqslant i < j \leqslant n$ and $\sigma(i) > \sigma(j)$. The number of inversions of the permutation $\sigma$ is called the {\bf length of $\sigma$} and written $\ell (\sigma)$. In formulas:
 $$ \ell (\sigma) = | \{ (i,j): i< j \text{ but } \sigma(i)> \sigma(j) \}|$$
The {\bf sign of $\sigma$} is defined to be the parity of the number of inversions of $\sigma$. In formulas:
$${\sf sgn}(\sigma) = (-1)^{\ell (\sigma)}$$
A permutation whose sign is $+1$, in other words which has even length, is called an {\bf even permutation}, while a permutation whose sign is $-1$, in other words which has odd length, is called an {\bf odd permutation}.
\end{definition}

\begin{ex}  If I represent the permutation by a diagram, as I do below for $(1 \, 2 \, 4 \, 5 \, 3) \in \mathfrak{S}_6$, then the length is the number of crossings:
 $$
\begin{tikzpicture}
\draw [<-|] (0,0) node[below] {$1$} -- (2,1.5) node[above] {$3$};
\draw [<-|] (1,0) node[below] {$2$} -- (0,1.5) node[above] {$1$};
\draw [<-|] (2,0) node[below] {$3$} -- (4,1.5) node[above] {$5$};
\draw [<-|] (3,0) node[below] {$4$} -- (1,1.5) node[above] {$2$};
\draw [<-|] (4,0) node[below] {$5$} -- (3,1.5) node[above] {$4$};
\draw [<-|] (5,0) node[below] {$6$} -- (5,1.5) node[above] {$6$};
\end{tikzpicture}
$$
So $\ell ((1 \, 2 \, 4 \, 5 \, 3)) = 4$: the inversions are $(1,3), (2,3), (2,5), (4,5)$.
\end{ex}

\begin{ex} \label{transodd} The identity of $\mathfrak{S}_n$ is always the only permutation with length zero. The transposition that swaps $i$ and $j$, leaving everything else unchanged, has length $2|i-j| - 1$, which you can see best from the picture below. In particular, all transpositions are odd permutations.
$$
\begin{tikzpicture}
\draw [<-|] (-1,0) node[below] {$\cdot$} -- (-1,1.5) node[above] {$\cdot$};
\draw [<-|] (0,0) node[below] {$\cdot$} -- (0,1.5) node[above] {$\cdot$};
\draw [<-|] (1,0) node[below] {$i$} -- (4,1.5) node[above] {$j$};
\draw [<-|] (2,0) node[below] {$\cdot$} -- (2,1.5) node[above] {$\cdot$};
\draw [<-|] (3,0) node[below] {$\cdot$} -- (3,1.5) node[above] {$\cdot$};
\draw [<-|] (4,0) node[below] {$j$} -- (1,1.5) node[above] {$i$};
\draw [<-|] (5,0) node[below] {$\cdot$} -- (5,1.5) node[above] {$\cdot$};
\end{tikzpicture}
$$
\end{ex}

\begin{lemma}[Multiplicativity of the Sign] \label{multsign} For each $n\in \mathbb{N}$ the sign of a permutation produces a group homomorphism ${\sf sgn}: \mathfrak{S}_n \to \{+1, -1\}$ from the symmetric group to the two-element group of signs. In formulas:
$${\sf sgn}(\sigma \tau) = {\sf sgn}(\sigma) {\sf sgn} (\tau) \quad \text{for all } \sigma, \tau \in \mathfrak{S}_n$$
\end{lemma}
\begin{proof} I need to introduce a new notation. Given a non-zero real number $a \in \mathbb{R}\setminus 0$ let $[a] \in \{+1,-1\}$ be the sign of $a$. I can then restate the definition of the sign of a permutation $\sigma$ by \begin{equation} \label{signdefprose} {\sf sgn} (\sigma) = \prod_{i< j} [\sigma(j)- \sigma(i)]\end{equation} Given another permutation $\tau$ it's clear that $$ \prod_{i< j} [\sigma\tau(j)- \sigma\tau(i)] = \prod_{i< j} \frac{[\sigma(\tau(j))- \sigma(\tau(i))]}{ [\tau(j)- \tau(i)]} \prod_{i< j} [\tau(j)- \tau(i)]$$ So, using \eqref{signdefprose}, that means that I must show that \begin{equation} \label{signtocheck} \prod_{i< j} [\sigma(j)- \sigma(i)] = \prod_{i< j} \frac{[\sigma(\tau(j))- \sigma(\tau(i))]}{ [\tau(j)- \tau(i)]}\end{equation} This is elementary. Since $\tau$ is a bijection there is an equality $$\{ (i,j): i< j \} = \{ (\tau(i), \tau(j)): i< j \text{ and } \tau (i) < \tau(j)  \} \cup \{ (\tau (j), \tau(i)): i< j \text{ and } \tau (i) > \tau(j) \}$$ I can then replace the indexing set $\{ (i,j) : i< j \}$ on the right hand side of \eqref{signtocheck} by this to obtain the equality I want.\end{proof}
\begin{rem}
There's a better proof which was presented to you in \href{http://www.drps.ed.ac.uk/12-13/dpt/cxmath08064.htm}{Fundamentals of Pure Mathematics}. For each $\sigma \in \mathfrak{S}_n$ there is an associated ring homomorphism $\sigma: \mathbb{Z}[X_1, \ldots , X_n] \to \mathbb{Z}[X_1, \ldots , X_n]$ that is defined by swapping the variables according to $\sigma$, namely $\sigma: X_i \mapsto X_{\sigma(i)}$. It's clear that $\tau (\sigma P ) = (\tau \sigma) P$ for each polynomial $P\in \mathbb{Z}[X_1, \ldots , X_n]$. Consider a special polynomial, called the {\bf discriminant}: $$P = \prod_{i<j} (X_i - X_j)$$ which automatically has the property that $\sigma P = {\sf sgn}(\sigma) P$. From this it follows  that $${\sf sgn}(\tau) {\sf sgn}(\sigma) P = \tau (\sigma P) = (\tau \sigma) P = {\sf sgn}(\tau \sigma) P$$ From this, I earn the statement of \hyperref[multsign]{Lemma \ref{multsign}}.
\end{rem}

\begin{definition} For $n\in \mathbb{N}$, the set of even permutations in $\mathfrak{S}_n$ forms a subgroup of $\mathfrak{S}_n$ because it is the kernel of the group homomorphism ${\sf sgn}: \mathfrak{S}_n \to \{ +1, -1 \}$. This group is the {\bf alternating group} and is denoted $A_n$.
\end{definition}

\begin{exercise}\label{bringtothefront}
Show that: the permutation that brings $i$ to the first place while changing the order of no other number has $i-1$ inversions. In particular ${\sf sgn}(\sigma) = (-1)^{i-1}$.
$$
\begin{tikzpicture}
\draw [<-|] (-1,0) node[below] {$1$} -- (3,1.5) node[above] {$i$};
\draw [<-|] (0,0) node[below] {$\cdot$} -- (-1,1.5) node[above] {$\cdot$};
\draw [<-|] (1,0) node[below] {$\cdot$} -- (0,1.5) node[above] {$\cdot$};
\draw [<-|] (2,0) node[below] {$\cdot$} -- (1,1.5) node[above] {$\cdot$};
\draw [<-|] (3,0) node[below] {$\cdot$} -- (2,1.5) node[above] {$\cdot$};
\draw [<-|] (4,0) node[below] {$\cdot$} -- (4,1.5) node[above] {$\cdot$};
\draw [<-|] (5,0) node[below] {$\cdot$} -- (5,1.5) node[above] {$\cdot$};
\end{tikzpicture}
$$

\end{exercise}

\begin{exercise} \label{symgentrans}
Show that: every permutation in $\mathfrak{S}_n$ can be described as a product of transpositions of neighbouring numbers, that is of the permutations $(i \,\, i\!+\!1)$ swapping $i$ and $i+1$ for some $1\leqslant i \leqslant n-1$.
\end{exercise}

\section{Determinants and What They Mean}

\begin{definition} \label{detdef}
Let $R$ be a commutative ring and $n\in \mathbb{N}$. The {\bf determinant} is a mapping ${\sf det}: {\sf Mat}(n; R) \to R$ from square matrices with coefficients in $R$ to the ring $R$ that is given by the following formula:
$$A = \begin{pmatrix} a_{11} &\cdots & a_{1n} \\ \vdots & \ddots & \vdots \\ a_{n1} & \cdots & a_{nn} \end{pmatrix} \mapsto {\sf det} (A) = \sum_{\sigma \in \mathfrak{S}_n} {\sf sgn}(\sigma) a_{1\sigma(1)} \ldots a_{n\sigma(n)}$$
The sum is over all permutations of $n$, and the coefficient ${\sf sgn}(\sigma)$ is the sign of the permutation $\sigma$ defined above in \hyperref[permdefs]{Definition \ref{permdefs}}. This formula is called the {\bf Leibniz formula}. The degenerate case $n=0$ assigns the value $1$ as the determinant of the ``empty matrix".
\end{definition}

\begin{rem}
The determinant determines whether or not a linear system of $n$ equations in $n$ unknowns has a unique solution. Hence the name.
\end{rem}

\begin{ex} \label{firstdetcalc}
It is simple to calculate:
\begin{eqnarray*}
{\sf det} (a) & = & a \\
{\sf det} \begin{pmatrix} a & b \\ c& d \end{pmatrix} & = & ad - bc \\
{\sf det} \begin{pmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{pmatrix} & = & \begin{matrix} a_{11}a_{22}a_{33} + a_{12}a_{23}a_{31} + a_{13}a_{21}a_{32} \\ - a_{31}a_{22}a_{13} - a_{32}a_{23}a_{11} - a_{33}a_{21}a_{12} \end{matrix}
\end{eqnarray*}
The $(3\times 3)$-case is easy to remember -- it looks just like a trellis fence:
$$
\begin{tikzpicture}
  \foreach \x in {0,-1,-2}
    \foreach \y in {0,-1}
    {
      \draw [thick, dashed, red] (-\x+\y,\y) -- +(-1,-1) ;
\draw[thick, red] (\x - \y, \y) -- +(1, -1);
}
\foreach \x in {1,2,3}
\foreach \y in {1,2,3}
{
      \draw (-3+\x,1-\y) node[fill=white]{$a_{\y \x}$};
    }
    \foreach \x in {1,2}
\foreach \y in {1,2,3}
{
      \draw (\x,1-\y) node[gray, fill=white]{$a_{\y \x}$};
    }
 \draw[very thick] (-2.3,0.3) .. controls (-2.6,-0.6) and (-2.6,-1.4) .. (-2.3,-2.3);
  \draw[very thick] (0.3,0.3) .. controls (0.6,-0.6) and (0.6, -1.4) .. (0.3,-2.3);
\end{tikzpicture}
$$
For $n\geqslant 4$ there are $n! \geqslant 24$ terms in the Leibniz formula, and it's not fun to calculate. I'll remind you in \hyperref[howtocalc]{Remark \ref{howtocalc}} how to calculate determinants more efficiently in such cases. \end{ex}

\begin{ex} \label{tridet} An $n \times n$ matrix $A=(a_{ij})$ is upper triangular if $a_{ij}=0$ for $i>j$.
The determinant of $A$ is the product of the entries $a_{ii}$  along the diagonal. This follows because the only permutation $\sigma \in \mathfrak{S} _n$ that satisfies $i \leqslant \sigma (i)$ for all $i$ is the identity permutation, so the only non-zero summand in the definition of the determinant occurs for $\sigma = \id$. The same holds for a lower triangular matrix.
\end{ex}

\begin{exercise} \label{blockdet}
By adapting the argument in the above example, show that: the determinant of a block-upper triangular matrix with square blocks along the diagonal is the product of the determinants of the blocks along the diagonal
$$ {\sf det} \left(
\begin{array}{c | c | c | c}
A_1 & \ast & \ast & \ast \\ \hline
0 & A_2 & \ast & \ast \\ \hline 0 & 0 & \ddots & \ast \\ \hline 0 & 0 & 0 & A_t
\end{array}\right) = {\sf det}(A_1) {\sf det}(A_2) \cdots {\sf det}(A_t).
$$
\end{exercise}
\medskip

I'll now explain the meaning of the determinant in order to motivate my development of the theory. To do this I need to use real matrices and I'm going to consider only $(2\times 2)$-matrices. Using the standard basis for $\mathbb{R}^2$ such matrices correspond to linear mappings $L:\mathbb{R}^2 \to \mathbb{R}^2$ and since my spatial intuition of two-dimensional space is well-developed I will use this description.
\medskip

\noindent
{\bf The connection between determinants and volumes}. Each such linear mapping $L$ has an ``area scaling factor" $sc(L)$ which I define as the amount that $L$ changes the area, ${\sf vol}(U)$, of a region $U$ in $\mathbb{R}^2$. In other words ${\sf area}(LU) = sc(L) {\sf area}(U)$. I claim that $$sc(L) = | {\sf det}(L) |$$ To see this I consider the properties that the mapping $sc: {\sf Mat}(2; \mathbb{R}) \to \mathbb{R}_{\geqslant 0}$, defined by $L\mapsto sc(L)$, must have:
\begin{enumerate}
\item
It should be ``multiplicative": $sc(LM) = sc(L)sc(M)$;
\item
Dilating an axis should increase the area of a region by the amount of the dilation: $sc({\sf diag}(a,1)) = sc({\sf diag}(1,a)) = |a|$;
\item
A shear transformation should leave the area of a region unchanged: $sc(D) = 1$ for $D$ an upper or a lower triangular matrix with ones on the diagonal.
\end{enumerate}
These should be obvious to you; if not, \href{http://halg.s3-website-eu-west-1.amazonaws.com}{experiment with this applet}, written by Henri Maurer, a former Honours Algebra student, which visualises linear mappings.

Recall from \hyperref[elmove]{Theorem \ref{elmove}} that each square matrix can be written as a product of elementary matrices. Since each elementary matrix either dilates one of the axes or is a shear transformation, there can be therefore at most one mapping $sc:  {\sf Mat}(2; \mathbb{R}) \to \mathbb{R}_{\geqslant 0}$ satisfying the three properties above. In \hyperref[detismult]{Theorem \ref{detismult}} I will prove that $\det (LM) = \det(L) \det (M)$, and together with \hyperref[tridet]{Example \ref{tridet}} which calculates the determinant for an upper or lower triangular matrix, this shows that $M\mapsto |{\sf det}(M)|$ is a mapping with the three prescribed properties. So it must be that $sc = |{\sf det}|$, as claimed. In other words the absolute value of the determinant is the area scaling factor of the corresponding linear mapping.
\medskip

\noindent
{\bf The connection between determinants and orientation}. The sign of the determinant of an invertible real $(2\times 2)$-matrix shows whether the corresponding endomorphism of $\mathbb{R}^2$ preserves or reverses orientation. To comprehend orientation I imagine a clock face inside the region $U$ I'm going to apply $L$ to: if, after applying $U$, the clock face is still the correct way round then $L$ preserves orientation; if it is the wrong way round, then $L$ reverses orientation. I think of this property as a mapping sending an invertible linear transformation $L: \mathbb{R}^2 \to \mathbb{R}^2$ to $\epsilon (L) \in \{ +1, -1 \}$ as follows:
$$
\epsilon (L) = \begin{cases} +1 \quad & \text{$L$ preserves the orientation} \\
-1 & \text{$L$ reverses the orientation}
\end{cases}
$$

In the \hyperref[multsign]{proof of Lemma \ref{multsign}} I used the notation $[a]$ for the sign of a non-zero real number $a$ and I'll do that again here.  I claim that $$\epsilon (L) = [{\sf det}(L) ]$$ To see this let's consider the properties that the mapping $\epsilon : GL(2; \mathbb{R}) \to \{ +1, -1\}$, defined by $L\mapsto \epsilon(L)$, must have:
\begin{enumerate}
\item
It should be ``multiplicative": $\epsilon(LM) = \epsilon(L)\epsilon(M)$;
\item
Dilating an axis should change the orientation by the sign of the amount of the dilation: $\epsilon({\sf diag}(a,1)) = \epsilon({\sf diag}(1,a)) = [a]$;
\item
A shear transformation should preserve the orientation: $\epsilon(D) = 1$ for $D$ an upper or a lower triangular matrix with ones on the diagonal.
\end{enumerate}
Again if these are not obvious to you, \href{http://halg.s3-website-eu-west-1.amazonaws.com}{look at the dot and the cross on the square in the applet}.

Since each square matrix can be written as a product of elementary matrices by \hyperref[elmove]{Theorem \ref{elmove}}, there can be at most one mapping $\epsilon:  GL(2; \mathbb{R}) \to \{ +1, -1\}$ that satisfies the three properties above. But \hyperref[detismult]{Theorem \ref{detismult}} will show that $\det (LM) = \det(L) \det (M)$, and together with \hyperref[tridet]{Example \ref{tridet}} which calculates the determinant for an upper or lower triangular matrix, this shows that $M\mapsto [{\sf det}(M)]$ is a mapping with the three prescribed properties. So $\epsilon  = [{\sf det }]$, as claimed. In other words, the sign of the determinant determines if the linear mapping preserves or reverses orientation.
\medskip

A similar analysis works in higher dimensions, and in particular for real $(3\times 3)$-matrices where instead of ``area scaling factor" I would write ``volume scaling factor" and where instead of a clock face I would use the right-hand-rule. You can find the details of this point of view worked out succinctly in arbitrary dimension in Chapter 5 of the \href{http://catalogue.lib.ed.ac.uk/vwebv/holdingsInfo?searchId=1244&recCount=10&recPointer=6&bibId=710930}{Linear Algebra book by Peter Lax}.

 It should now be intuitively clear why ${\sf det} (L) \neq 0$ is equivalent to $L$ being invertible, which I will show rigorously in \hyperref[detinv]{Theorem \ref{detinv}}.

\begin{rem}
Perhaps you should now think about the child's question ``Why does a mirror switch left and right, but not up and down?". The correct answer is that the mirror no more switches left and right than it does up and down, but rather it switches backwards and forwards.
\end{rem}

\section{Characterising the Determinant}
In the previous section I've shown you that the determinant of a real square matrix is related both to volume and to orientation. Determinants exist and are critical for more than just real matrices, however, so now I'm going to explain another important interpretation of the determinant which makes sense for an arbitrary field.

\begin{definition} \label{bilindef} Let $U,V$ and $W$ be $F$-vector spaces. A {\bf bilinear form on $U\times V$ with values in $W$} is a mapping $H: U\times V \to W$ which is a linear mapping in both of its entries. This means that it must satisfy the following properties for all $u_1, u_2\in U$ and $v_1, v_2 \in V$ and all $\lambda \in F$:
\begin{eqnarray*}
 H(u_1+u_2, v_1) &=& H(u_1, v_1) + H(u_2,v_1) \\
H(\lambda u_1, v_1) &=& \lambda H(u_1, v_1) \\
H(u_1, v_1+v_2) &=& H(u_1,v_1) + H(u_1,v_2) \\
H(u_1, \lambda v_1) &=& \lambda H(u_1, v_1)
\end{eqnarray*}
The first two conditions state that for any fixed $v\in V$ the mapping $H(-, v): U\to W$ is linear; the final two conditions state that for any fixed $u\in U$ the mapping $H(u, -): V\to W$ is linear. If $U,V$ and $W$ are clear from the context I will simply say that $H$ is a {\bf bilinear form}. A bilinear form $H$ is {\bf symmetric} if $U=V$ and $$H(u,v) = H(v,u) \quad \text{for all $u,v \in U$} $$ while it is {\bf alternating} or {\bf antisymmetric} if $U=V$ and $$H(u,u) = 0\quad \text{for all $u\in U$} $$
\end{definition}

\begin{rem}
Suppose that $H: U\times U\to W$ is an antisymmetric bilinear form on $U$ with values in $W$. Then for all $u,v\in U$: \begin{eqnarray*}
0 & = & H(u+v, u+v) \\ & = & H(u,u+v) + H(v, u+v) \\ & = & H(u,u) + H(u,v) + H(v,u) + H(v,v) \\ & = & H(u,v) + H(v,u)
\end{eqnarray*} Therefore an antisymmetric form always satisfies $H(u,v) = -H(v,u)$, hence the name.   On the other hand, if $H$ is a bilinear form satisfying $H(u,v) = - H(v,u)$ for all $u,v\in U$, then taking $u=v$ gives $H(u,u) = -H(u,u)$ from which it follows that $H(u,u) + H(u,u) = 0$. As long as $1_F + 1_F \neq 0_F$ I deduce that $H(u,u)=0$ and so the form is antisymmetric. But remember that you know a field $F = \mathbb{F}_2$ in which $1_F + 1_F = 0_F$ so you do need to be careful.
\end{rem}

\begin{definition}
Let $V_1, \ldots , V_n, W$ be $F$-vector spaces. A mapping $H: V_1\times V_2\times \cdots \times V_n \to W$ is a {\bf multilinear form} or just {\bf multilinear} if for each $j$ the mapping $V_j \to W$ defined by $v_j \mapsto H(v_1, \ldots , v_j, \ldots , v_n)$, with the $v_i\in V_i$ arbitrary fixed vectors of $V_i$ for $i\neq j$,  is linear. In the case $n=2$, this is exactly the \hyperref[bilindef]{definition of a bilinear mapping} given above.
\end{definition}

\begin{definition}
Let $V$ and $W$ be $F$-vector spaces. A multilinear form $H: V\times \cdots \times V\to W$ is {\bf alternating} if it vanishes on every $n$-tuple of elements of $V$ that has at least two entries equal, in other words if:
$$ (\exists \, i\neq j \text{ with }v_i=v_j) \to H(v_1, \ldots , v_i, \ldots , v_j, \ldots , v_n) = 0$$
In the case $n=2$, this is exactly the \hyperref[bilindef]{definition of an alternating or antisymmetric bilinear mapping} given above.
\end{definition}

\begin{rem} \label{altdesc} An alternating multilinear form $H$ has the property \begin{equation} \label{anti} H (v_1,  \ldots , v_i, \ldots , v_j, \ldots , v_n) = - H ( v_1, \ldots , v_j, \ldots , v_i, \ldots , v_n)\end{equation}  for all $v_1, \ldots , v_n \in V$. Combining this with \hyperref[multsign]{Lemma \ref{multsign}} and \hyperref[transodd]{Exercise \ref{symgentrans}} shows that for any $\sigma \in \mathfrak{S}_n$ $$H (v_{\sigma(1)},  \ldots , v_{\sigma(n)}) = {\sf sgn}(\sigma) H ( v_1,  \ldots , v_n)$$ Conversely, if \eqref{anti} holds for a multilinear form  $H$ and arbitrary $v_1, \ldots , v_n \in V$ then $H$ is alternating provided $1_F + 1_F \neq 0_F$.
\end{rem}

\begin{theorem}[Characterisation of the Determinant] \label{detchar}
Let $F$ be a field. The mapping $${\sf det}:{\sf Mat}(n; F) \to F$$ is the unique alternating multilinear form on $n$-tuples of column vectors with values in $F$ that takes the value $1_F$ on the identity matrix.
\end{theorem}
There is a little jiggery-pokery going on here: I am simultaneously considering elements of ${\sf Mat}(n;F)$ as $(n\times n)$-matrices over $F$ and as ordered lists $n$ column vectors -- the columns of the matrix. That's why I can think of the determinant as a mapping $${\sf det}: F^n \times \cdots \times F^n \to F, (v_1, \ldots , v_n) \mapsto {\sf det} (v_1 | \cdots | v_n)$$
\begin{proof}
It's obvious from the Leibniz formula in \hyperref[detdef]{Definition \ref{detdef}} that the determinant is multilinear and that it evaluates to $1_F$ on the identity matrix. To show that it is alternating, suppose that column $i$ and $j$ of an $(n\times n)$-matrix $A$ are equal. If I take $\tau \in \mathfrak{S}_n$ to be the transposition that switches $i$ and $j$, the equality of columns means that $a_{ij} = a_{i \tau(j)}$ for any $i$ and $j$. This implies that $a_{1\sigma(1)} \ldots a_{n\sigma(n)} = a_{1\tau\sigma(1)} \ldots a_{n\tau\sigma(n)}$ for any $\sigma\in \mathfrak{S}_n$. Furthermore ${\sf sgn} (\sigma) = - {\sf sgn}(\tau \sigma)$ by \hyperref[multsign]{Lemma \ref{multsign}} and \hyperref[transodd]{Example \ref{transodd}}. Since $\tau^2 = \id_{\mathfrak{S}_n}$, $H = \{\id_{\mathfrak{S}_n}, \tau \}\leqslant \mathfrak{S}_n$ is the subgroup of $\mathfrak{S}_n$ generated by $\tau$. Let $X \subset \mathfrak{S}_n$ be a set of right coset representatives of $H$ in $\mathfrak{S}_n$, so that there is a disjoint union $$\bigcup_{\sigma \in X} H \sigma = \mathfrak{S}_n$$ Then \begin{eqnarray*}{\sf det} (A) &=& \sum_{\sigma \in \mathfrak{S}_n} {\sf sgn}(\sigma) a_{1\sigma(1)} \ldots a_{n\sigma(n)} \\ &=& \sum_{\sigma \in X} \left({\sf sgn}(\sigma) a_{1\sigma(1)} \ldots a_{n\sigma(n)}+ {\sf sgn}(\tau \sigma) a_{1\tau\sigma(1)} \ldots a_{n\tau\sigma(n)}\right) = 0.\end{eqnarray*} This confirms that ${\sf det}$ is alternating and multilinear.\footnote{The argument here works verbatim to show that if $A$ is an $(n\times n$)-matrix with coefficients in a commutative ring, then ${\sf det}(A) = 0$ if two columns of $A$ are equal.}

Now I must prove that there exists no other mapping $d: {\sf Mat}(n; F)\to F$ with the stated properties. Thanks to multilinearity, the mapping $d$ will be completely determined by its values on $n$-tuples of basis vectors (this is a simple generalisation of the argument of \hyperref[linmapbas]{Lemma \ref{linmapbas}}). In other words $d$ is determined by its values $$ d( e_{\sigma(1)} | \cdots | e_{\sigma (n)} )$$ where $\sigma : \{ 1, \ldots , n \} \to \{ 1, \ldots , n \}$ is an {\it arbitrary} mapping. But $d$ is assumed to be alternating, so that means that if $\sigma (i) = \sigma (j)$ for some $i\neq j$, then $d( e_{\sigma(1)} | \cdots | e_{\sigma (n)} )= 0$. In other words if $\sigma$ is not bijective, i.e. $\sigma \notin \mathfrak{S}_n$, then $d( e_{\sigma(1)} | \cdots | e_{\sigma (n)} ) = 0$. It follows from \hyperref[altdesc]{Remark \ref{altdesc}} that $$d( e_{\sigma(1)} | \cdots | e_{\sigma (n)} )=  \begin{cases} {\sf sgn}(\sigma) d( e_{1} | \cdots | e_{n} ) \quad & \sigma \in \mathfrak{S}_n \\ 0 & \text{otherwise} \end{cases}$$
Finally, $d( e_{1} | \cdots | e_{n} )=1$ by assumption, and so $d$ and ${\sf det}$ agree on all $n$-tuples of basis vectors and hence $d = {\sf det}$, as claimed.
\end{proof}

\begin{exercise} \label{bestchar}
Adapt the second part of the \hyperref[detchar]{proof of Theorem \ref{detchar}} to show that: if $d: {\sf Mat}(n; F)\to F$ is an alternating multilinear form on $n$-tuples of column vectors with values in $F$, then $$d(A) = d(e_1| \cdots | e_n) {\sf det}(A)$$ for all $A\in {\sf Mat}(n;F)$.
\end{exercise}

\section{Rules for Calculating with Determinants}
\begin{theorem}[Multiplicativity of the Determinant]\label{detismult} Let $R$ be a commutative ring and let $A, B \in {\sf Mat}(n;R)$. Then $${\sf det}(AB) = {\sf det}(A) {\sf det}(B).$$
\end{theorem}
\begin{proof}[First proof]
Let $\mathfrak{T}_n = {\rm Maps} (\{1, \ldots , n\}, \{1, \ldots , n\})$. I calculate!
\begin{eqnarray*}
{\sf det}(AB) & = & \sum_{\sigma \in \mathfrak{S}_n} {\sf sgn}(\sigma) \prod_{i=1}^n (AB)_{i\sigma(i)} \\ & = & \sum_{\sigma \in \mathfrak{S}_n} {\sf sgn}(\sigma) \prod_{i=1}^n \sum_{j=1}^n a_{ij} b_{j\sigma(i)} \\
& = & \sum_{\sigma\in \mathfrak{S}_n, \kappa \in \mathfrak{T}_n} {\sf sgn}(\sigma) a_{1\kappa(1)}b_{\kappa(1) \sigma(1)} \ldots a_{n \kappa(n)} b_{\kappa (n) \sigma (n)} \\
& = &\sum_{\kappa\in \mathfrak{T}_n} a_{1\kappa(1)} \ldots a_{n \kappa(n)} \sum_{\sigma \in \mathfrak{S}_n} {\sf sgn}(\sigma) b_{\kappa(1) \sigma(1)}\ldots b_{\kappa (n) \sigma (n)} \\
& = & \sum_{\kappa\in \mathfrak{T}_n}  a_{1\kappa(1)} \ldots a_{n \kappa(n)} {\sf det} (B_{\kappa})
\end{eqnarray*} where $B_{\kappa}$ is the matrix obtained by using the rows of $B$ labelled $\kappa(1), \ldots , \kappa (n)$ for its rows $1$ to $n$. It follows from \hyperref[detchar]{the footnote in the proof of Theorem \ref{detchar}} that ${\sf det}(B_{\kappa}) =0$ if $\kappa \notin \mathfrak{S}_n$ and that ${\sf det}(B_{\kappa}) = {\sf sgn}(\kappa) {\sf det} (B)$ if $\kappa\in \mathfrak{S}_n$. From this it follows that $$ \sum_{\kappa\in \mathfrak{T}_n}  a_{1\kappa(1)} \ldots a_{n \kappa(n)} {\sf det} (B_{\kappa})
 = \sum_{\kappa\in \mathfrak{S}_n}  {\sf sgn}(\kappa) a_{1\kappa(1)} \ldots a_{n \kappa(n)} {\sf det} (B) = \det (A) \det (B)$$ which is exactly what was required.
\end{proof}

\begin{proof}[Second proof, for fields only]
Consider the mappings ${\sf Mat}(n; F)\to F$ given by $B\mapsto {\sf det}(A) {\sf det}(B)$ and by $B \mapsto {\sf \det}(AB)$. Both are multilinear and alternating as functions in the columns of $B$, and produce ${\sf det}(A)$ when evaluated at $B=I_n$. It follows from \hyperref[bestchar]{Exercise \ref{bestchar}} that the mappings must be equal.
\end{proof}

\begin{theorem}[Determinantal Criterion for Invertibility] \label{detinv} The determinant of a square matrix with entries in a field $F$ is non-zero if and only if the matrix is invertible.
\end{theorem}
\begin{proof}
Let $F$ be the field and let $A\in {\sf Mat}(n;F)$ be the matrix. I must show that $${\sf det}(A) \neq 0 \Leftrightarrow A \text{ is invertible}
$$
Suppose first that $A$ is invertible. Then there exists a matrix $B = A^{-1}$ such that $AB = I_n$. I use the \hyperref[detismult]{multiplicativity of the determinant} to deduce that ${\sf det}(A) {\sf det}(B) = {\sf det}(I_n) = 1$ from which it follows that ${\sf det}(A) \neq 0$. This proves $\Leftarrow$. Conversely, if $A$ is not invertible then it does not have full rank, as in \hyperref[fullrank]{Definition \ref{fullrank}}.  Therefore I can find a column vector of $A$, which without loss of generality I will assume to be the first column, that is linearly dependent on the other column vectors. This means there exist $\lambda_2, \ldots , \lambda_n \in F$ such that $a_{\ast 1} = \lambda_2 a_{\ast 2} + \cdots + \lambda_n a_{\ast n}.$ The multilinearity and alternating properties of the determinant then show \begin{eqnarray*} {\sf det}(A) & = & {\sf det}(\lambda_2 a_{\ast 2} + \cdots + \lambda_n a_{\ast n} | a_{\ast 2} | \cdots | a_{\ast n}) \\ & = & \lambda_2 {\sf det}(a_{\ast 2}| a_{\ast 2} | \cdots | a_{\ast n}) + \cdots + \lambda_n {\sf det}(a_{\ast n}|a_{\ast 2} | \cdots | a_{\ast n}) \\ & = & \lambda_2 0 + \cdots + \lambda_n 0 \\ & = & 0\end{eqnarray*} This proves the other implication $\to$.
\end{proof}

\begin{rem} \label{detisinvariant}
There are two quick consequences of \hyperref[detismult]{Theorem \ref{detismult}} and \hyperref[detinv]{Theorem \ref{detinv}} . First, if $A$ is invertible then ${\sf det}(A^{-1}) = {\sf det}(A)^{-1}$. From this, the second consequence follows: if $B$ is a square matrix $B$ then \begin{equation} \label{detconjeq} {\sf det}( A^{-1}BA) = {\sf det}(B)\end{equation} You should recognise this second fact as {\bf very important}. Why? Well, remember that if $f\in {\rm End}(V)$ is an endomorphism of some finite dimensional vector space $V$, then a choice of ordered basis $\mathcal{A}$ for $V$ produces a square matrix ${}_{\mathcal{A}}[f]_{\mathcal{A}}$ and hence I can associate the scalar ${\sf det}({}_{\mathcal{A}}[f]_{\mathcal{A}})$ to $f$ and $\mathcal{A}$. If $\mathcal{B}$ is another ordered basis of $V$, then I get another scalar ${\sf det}({}_{\mathcal{B}}[f]_{\mathcal{B}})$. But combining \eqref{conjforcob} with \eqref{detconjeq} shows that these two scalars are equal. In other words, I can define the {\bf determinant of the endomorphism $f$}, ${\sf det}(f|V)$ or just ${\sf det}(f)$ if the vector space $V$ is clear from the context, as this scalar and this is independent of the choice of basis I use to calculate it (in other words, well-defined!). If you think back to the description of the determinant of an endomorphism of $\mathbb{R}^2$ as a measuring scaling and orientation, this shouldn't surprise you.
\end{rem}



\begin{exercise} Recall from \hyperref[complextoreal]{Exercise \ref{complextoreal}} that a finite dimensional $\mathbb{C}$-vector space $V$ can be considered as an $\mathbb{R}$-vector space too, which I will denote $V_{\mathbb{R}}$ . Let $f\in {\rm End}_{\mathbb{C}}(V)$ be an endomorphism of $V$ as $\mathbb{C}$-vector space. Show that: $f$ is also an endomorphism of $V_\mathbb{R}$ and that ${\sf det}(f|{V_{\mathbb{R}}})  = |{\sf det}(f|V)|^2$.
 \end{exercise}

\begin{lemma}\label{dettrans}
The determinant of a square matrix and of the transpose of the square matrix are equal, that is for all $A\in {\sf Mat}(n;R)$ with $R$ a commutative ring $${\sf det}(A^{\sf T}) = {\sf det}(A)$$
\end{lemma}
\begin{proof}
By definition, $${\sf det}(A^{\sf T}) = \sum_{\sigma\in\mathfrak{S}_n} {\sf sgn}(\sigma) a_{\sigma (1) 1} \ldots a_{\sigma(n)n}$$ If $\tau = \sigma^{-1}$ then ${\sf sgn}(\tau) = {\sf sgn}(\sigma)$ (this is obvious: think of the pictures I used, for instance, when discussing ${\sf sgn}$) and also $a_{\sigma (1) 1} \ldots a_{\sigma(n)n} = a_{1\tau (1)} \ldots a_{n\tau(n)}$, where this equality might swap the order of multiplication, which is fine since all $a_{ij}\in R$, a commutative ring. It follows that $${\sf det}(A^{\sf T}) = \sum_{\tau \in \mathfrak{S}_n} {\sf sgn}(\tau) a_{1\tau (1)} \ldots a_{n\tau(n)} = {\sf det}(A)$$
\end{proof}

\begin{rem} \label{howtocalc} Apart from showing that the determinant is a natural object, and apart from being a powerful abstract tool, \hyperref[detchar]{Theorem \ref{detchar}} is very useful for calculation. Why? Recall from \hyperref[firstdetcalc]{Example \ref{firstdetcalc}} that the determinant of an $(n\times n)$-matrix is unpleasant to calculate if $n\geqslant 4$. Combined with \hyperref[dettrans]{Lemma \ref{dettrans}}, however, the theorem demonstrates that Gaussian elimination fits perfectly with the determinant: performing [{\it Row Addition}] doesn't change the determinant, while performing [{\it Row Swap}] changes the sign only. When Gaussian elimination is completed the final matrix has staircase form so is upper triangular from which the determinant can be easily calculated as in \hyperref[tridet]{Example \ref{tridet}} by multiplying the entries of the diagonal together.
\end{rem}

Any niggling doubt you might have harboured about the definition of a determinant should now disappear, for I am now going to compare the determinant with the version you were shown in \href{www.drps.ed.ac.uk/12-13/dpt/cxmath08057.htm}{Introduction to Linear Algebra}.

\begin{definition} \label{cofactor}
Let $A\in {\sf Mat}(n;R)$ for some commutative ring $R$ and natural number $n$. Let $i$ and $j$ be integers between $1$ and $n$. Then the {\bf $(i,j)$ cofactor of $A$} is $C_{ij} = (-1)^{i+j} {\sf det}(A\langle i, j\rangle)$ where $A\langle i, j\rangle$ is the matrix I obtain from $A$ be deleting the $i$-th row and the $j$-th column.
$$
\begin{tikzpicture}
 \pgfsetmatrixcolumnsep{1mm}
  \pgfmatrix{rectangle}{center}{mymatrix}
    {\pgfusepath{}}{\pgfpointorigin}{\let\&=\pgfmatrixnextcell}
\& \&
  \node{$a_{11}$}; \& \node{$a_{12}$}; \& \node(c){$a_{13}$}; \& \&  \\
  \node{$C_{23} = (-1)^{2+3}{\sf det}$};  \&  \node(a){$a_{21}$}; \& \node{$a_{22}$}; \& \node(b){$a_{23}$}; \& \node{$ = -a_{11}a_{32} + a_{31}a_{12}$};  \\ \& \node{$a_{31}$}; \& \node{$a_{32}$}; \& \node(d){$a_{33}$}; \& \&  \\ }
 \draw [red,ultra thick,every node/.style=] (a.west) -- (b.east);
 \draw [red,ultra thick,every node/.style=] (c.north) -- (d.south);
 \draw[thick] (-1.4,0.7) .. controls (-1.6,0) .. (-1.4,-0.7);
  \draw[thick] (1.0,0.7) .. controls (1.2,0) .. (1.0,-0.7);
\end{tikzpicture}
$$

\end{definition}

\begin{theorem} [Laplace's Expansion of the Determinant] \label{laplacedet}
Let $A = (a_{ij})$ be an $(n\times n)$-matrix with entries from a commutative ring $R$. For a fixed $i$ the {\bf $i$-th row expansion of the determinant} is $${\sf det}(A) = \sum_{j=1}^n a_{ij} C_{ij}$$
and for a fixed $j$ the {\bf $j$-th column expansion of the determinant} is $${\sf det}(A) = \sum_{i=1}^n a_{ij} C_{ij}$$
\end{theorem}
\begin{proof}
Since ${\sf det}(A) = {\sf det}(A^{\sf T})$ I only need to prove the second of the two formulas. Now I've already shown you in \hyperref[bringtothefront]{Exercise \ref{bringtothefront}} that the determinant of a square matrix only changes by a factor of $(-1)^{j-1}$ when I move the $j$-th column of the matrix all the way through to the first column. So that means that it will be enough for me to prove the second statement for the case $j=1$, that is by expanding down the first column.

Write the matrix as a list of column vectors $A = (a_{\ast 1}|a_{\ast 2}| \ldots | a_{\ast n})$. I write the first column as a linear combination of the standard basis vectors $$a_{\ast 1} = a_{11} e_1 + \cdots + a_{n1}e_n$$ The multilinearity of the determinant then shows that $${\sf det}(A) = \sum_{i=1}^n a_{i1} {\sf det}(e_i | a_{\ast 2} | \ldots | a_{\ast n})$$ Now I move the $i$-th row of the matrix $(e_i |  a_{\ast 2} | \ldots | a_{\ast n})$ to the top, at the cost of multiplying by the factor $(-1)^{i-1}$. The matrix now has the form of a block-upper triangular matrix
$$
\left(\begin{array}{c|c} 1 & \ast \rule[-1ex]{0pt}{2ex} \\ \hline 0 & A\langle i,j \rangle
\end{array} \right)$$ It follows from \hyperref[blockdet]{Exercise \ref{blockdet}} then that $${\sf det}(e_i | a_{\ast 2} | \ldots | a_{\ast n}) = (-1)^{i-1} \det(A\langle i, j\rangle)$$ as required.  \end{proof}

\begin{definition} \label{defadjugate} Let $A$ be an $(n\times n)$-matrix with entries in a commutative ring $R$. The {\bf adjugate matrix} ${\sf adj}(A)$ is the $(n\times n)$-matrix whose entries are ${\sf adj}(A)_{ij} = C_{ji}$ where $C_{ji}$ the $(j,i)$-cofactor as defined in \hyperref[cofactor]{Definition \ref{cofactor}}.
\end{definition}

\begin{theorem} [Cramer's Rule] \label{Cramer} Let $A$ be an $(n\times n)$-matrix with entries in a commutative ring $R$. Then $$A\cdot {\sf adj}(A) = ({\sf det} A)I_n$$
\end{theorem}

\begin{rem} In many sources, such as \href{http://en.wikipedia.org/wiki/Cramer's_rule}{Wikipedia}, Cramer's Rule means the formula: $$x_i = \frac{{\sf det}(a_{*1} | \ldots | b_{*} | \ldots | a_{*n} )}{{\sf det}(a_{*1} | \ldots |a_{*i} |\ldots | a_{*n})}$$ for solving in a field $F$ the system $A \vec{x} = \vec{b}$ of $n$ linear equations in $n$ unknowns, provided that a unique solution exists. A unique solution exists if and only if $A$ is invertible. So, instead of applying the Gaussian algorithm, you can calculate lots of determinants, replacing the $i$-th column of $A$ by the given solution vector $\vec{b}$. It turns out that if you implement this rule on a computer, it has the same efficiency as the Gaussian algorithm. The relationship between this version of Cramer's rule and \hyperref[Cramer]{Theorem \ref{Cramer}} is gotten by successively taking the vector $\vec{b}$ in the system of linear equations to be the standard basis elements $\vec{e}_i$ with $1\leqslant i \leqslant n$.
\end{rem}

\begin{proof}
I have to prove for each $i$ and $k$ that $\sum_{j=1}^n a_{ij} {\sf adj}(A)_{jk} = \delta_{ik} {\sf det}(A)$. In other words that $$\sum_{j=1}^n a_{ij} C_{kj} = \delta_{ik} {\sf det}(A)$$ The case $i=k$ then states that $$\sum_{j=1}^n a_{ij} C_{ij} = {\sf det}(A)$$ and this is just the formula for the $i$-th row expansion of the determinant. Now suppose that $i\neq k$ and let $\tilde{A}$ be the matrix all of whose entries agree with those of $A$ except along the $k$-th row, where I replace the entries $a_{kj}$ for $1\leqslant j \leqslant n$ with $a_{ij}$. Then the $k$-th row expansion of the determinant of $\tilde{A}$ gives $${\sf det}(\tilde{A}) = \sum_{j=1}^n \tilde{a}_{kj} C_{kj} = \sum_{j=1}^n a_{ij} C_{kj}$$ But the $i$-th and $k$-th rows of $\tilde{A}$ are equal, so the multilinearity of the determinant implies that ${\sf det}(\tilde{A}) = 0$, as required.
\end{proof}

\begin{corollary} [Invertibility of Matrices] A square matrix with entries in a commutative ring $R$ is invertible if and only if its determinant is a unit in $R$. That is, $A\in {\sf Mat}(n;R)$ is invertible if and only if ${\sf det}(A) \in R^{\times}$.
\end{corollary}
So for instance, an integral matrix $A\in {\sf Mat}(n;\mathbb{Z})$ is invertible if and only ${\sf det}(A)$ is $1$ or $-1$, since $\mathbb{Z}^{\times} = \{ \pm 1\}$. On the other hand, a matrix $A \in {\sf Mat}(n;F)$ with entries in a field $F$ is invertible if and only if ${\sf det}(A) \neq 0$ since $F^{\times}$ consists of the non-zero elements of $F$.

\begin{proof}
If $A,B\in {\sf Mat}(n;R)$ are matrices such that $AB = I_n$, then ${\sf det}(A) {\sf det}(B) = {\sf det}(I_n) = 1_R$ by \hyperref[detismult]{Theorem \ref{detismult}}. Therefore ${\sf det}(A)$ is a unit in $R$. On the other hand, if ${\sf det}(A)$ is a unit in $R$ then I can consider the matrix $B = {\sf det}(A)^{-1} {\sf adj}(A) \in {\sf Mat}(n; R)$, and by \hyperref[Cramer]{Cramer's Rule} it has the property that $AB = I_n$. This is not yet enough for invertibility because I need also to prove that there exists some $C\in {\sf Mat}(n;R)$ such that $CA = I_n$, see \hyperref[defmatinv]{Definition \ref{defmatinv}}. Since ${\sf det}(A^{\sf T}) = {\sf det}(A)$ is  a unit, I can use \hyperref[Cramer]{Cramer's Rule} again to find another matrix $\tilde{C}\in {\sf Mat}(n;R)$ such that $A^{\sf T}\tilde{C} = I_n$. Taking the transpose of this equation gives  $\tilde{C}^{\sf T}A = I_n$ since $(A^{\sf T})^{\sf T} = A$ and $I_n^{\sf T} = I_n$ and transposition swaps the order of multiplication. So $C = \tilde{C}^{\sf T}$ does the trick.
\end{proof}

\begin{rem} Of course, in the above proof $B$ and $C$ must be equal. For if I multiply the equation $CA = I_n$ on the right by $B$ I get the equation $C = B$.
\end{rem}

\section{Eigenvalues and Eigenvectors}
\begin{definition}
Let $f: V\to V$ be an endomorphism of an $F$-vector space $V$. A scalar $\lambda \in F$ is an {\bf eigenvalue of $f$} if and only if there exists a non-zero vector $\vec{v} \in V$ such that $f(\vec{v})  = \lambda \vec{v}$. Each such vector is called an {\bf eigenvector of $f$ with eigenvalue $\lambda$}. For any $\lambda \in F$, the {\bf eigenspace of $f$ with eigenvalue $\lambda$} is $$E(\lambda, f) = \{ \vec{v}\in V: f(\vec{v}) = \lambda \vec{v} \}$$
\end{definition}

\begin{ex} An eigenvector of a linear mapping with eigenvalue $1$ is the exactly the same as a non-zero \hyperref[fixer]{fixed point} of $f$. An eigenvector of a linear mapping with eigenvalue $0$ is the exactly the same as a non-zero element in the kernel of the linear mapping.
\end{ex}

\begin{ex}\label{intuitexev} Here are few intuitive examples. Turning a sheet of paper by $90$ degrees anti-clockwise has no real eigenvalues. On the other hand, rotating it by $180$ degrees has the eigenvalue $-1$. Reflecting the piece of paper has an eigenvector with eigenvalue of $1$, gotten from any non-zero vector along the axis-of-symmetry. Differentiation of real polynomials has only one eigenvalue, $0$, with a corresponding eigenvector being any non-zero constant polynomial.
\end{ex}

\begin{exercise}
Let $f: V\to V$ be an endomorphism of an $F$-vector space $V$. Show that for any $\lambda \in F$
$E(\lambda ,f)$  is a subspace of $V$.
\end{exercise}

When $V$ is finite dimensional for almost all $\lambda \in F$ the subspace $E(\lambda, f)$ is zero because there are no eigenvectors with eigenvalue $\lambda$. Said more precisely, the non-zero elements of $E(\lambda,f)$ are the eigenvectors of $f$ with eigenvalue $\lambda$. But there is a fantastic theorem on eigenvectors and eigenvalues. It should surprise you, except for the fact that you met it in \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath08057.htm}{Introduction to Linear Algebra} and you have been using all over \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath10066.htm}{Honours Differential Equations}. I will prove this a bit later in this section, but I was impatient to show it to you.

\begin{theorem} [Existence of Eigenvalues] \label{evalsexist} Each endomorphism of a non-zero finite dimensional vector space over an algebraically closed field has an eigenvalue.
\end{theorem}

\begin{rem}
You saw already in \hyperref[intuitexev]{Example \ref{intuitexev}} that the algebraically closed hypothesis in the theorem is necessary: rotation in $\mathbb{R}^2$ by $90$ degrees had no eigenvalues. That the vector space should be non-zero in the statement of the theorem is natural -- how else could I produce a non-zero vector? -- but that the space should be finite dimensional is not obvious. But you can quickly find an example of an endomorphism of an infinite dimensional vector space with no eigenvalue: namely, let $V = \mathbb{C}[X]$ and take the endomorphism to multiplication by $X$, that is $P\mapsto XP$.
\end{rem}

\begin{definition}
Let $R$ be a commutative ring and let $A\in {\sf Mat}(n;R)$ be a square matrix with entries in $R$.
The polynomial ${\sf det}(A-x I_n) \in R[x]$ is called the {\bf characteristic polynomial of the matrix $A$}. It is denoted by $$\chi_A(x) := {\sf det}(A- x I_n)$$ where $\chi$ stands for $\chi$aracteristic.
\end{definition}

\begin{rem}
Previously in the notes I wrote $R[X]$ for the ring of polynomials on one variable with coefficients in the ring $R$, and suddenly $R[x]$ has appeared instead. I'm going to use this notation for the rest of this chapter. I prefer lower case $x$ for the variable to upper case $X$ right now, simply to make it absolutely clear that $x$ is a variable and not a matrix. Personally I find that displaying ${\sf det}(A-XI_n)$ makes me confused as I start to think that $X$ is a square matrix. Which it is not supposed to be. So I'll stick with ${\sf det}(A-xI_n)$.
\end{rem}


\begin{theorem} [Eigenvalues and Characteristic Polynomials] \label{evalchar}
Let $F$ be a field and $A\in {\sf Mat}(n;F)$ a square matrix with entries in $F$. The eigenvalues of the linear mapping $A: {F}^n \to {F}^n$ are exactly the roots of the characteristic polynomial $\chi_A$.
\end{theorem}
\begin{proof}This turns out to be easy! The following are equivalent for a scalar, $\lambda \in F$:
\begin{eqnarray*}
(\lambda \text{ is an eigenvalue of }A) & \Leftrightarrow & \exists \, v\neq 0 \text{ with } Av = \lambda v \\
 & \Leftrightarrow & \exists \, v\neq 0 \text{ with } (A- \lambda I_n)v = 0 \\
  & \Leftrightarrow & {\rm ker} ( A - \lambda I_n) \neq 0 \\
   & \Leftrightarrow & {\sf det}(A - \lambda I_n) = 0 \\
    & \Leftrightarrow & \chi_A(\lambda) = 0
    \end{eqnarray*}
    \end{proof}

\begin{exercise}
Let $F$ be a field and $A\in {\sf Mat}(n;F)$ a square matrix with coefficients in $F$. Show that: the characteristic polynomial of $A$ has the form $$\chi_A(x) = (-x)^n + {\rm tr}(A) (-x)^{n-1} + \cdots + {\sf det}(A).$$ Prosaically, the leading coefficient is $(-1)^n$, the next term has the trace of $A$ as its coefficient, and the constant term is the determinant of $A$.
\end{exercise}

\begin{rem}\label{charend}
(1)   Recall from  \hyperref[conjugacy]{Example \ref{conjugacy}} that square matrices $A,B \in {\sf Mat}(n;R)$ of the same size are {\it conjugate} if
$$B~=~P^{-1}AP \in {\sf Mat}(n;R)$$
for an invertible $P \in {\sf GL}(n;R)$. Conjugacy is an equivalence relation on ${\sf Mat}(n;R)$.
(The definition makes sense for any commutative ring $R$, although we shall be mainly concerned with the case of a field).\\ 
(2) The motivation for conjugacy comes from the various matrix representations of an endomorphism
$f:V \to V$ of an $n$-dimensional vector space $V$ over a field $F$. Let $A=(a_{ij})=_{\mathcal A}[f]_{\mathcal A}$, $B=(b_{ij})=_{\mathcal B}[f]_{\mathcal B} \in {\sf Mat}(n;F)$ be the matrices of $f$ with respect to bases ${\mathcal A}=(\vec{v}_1,\vec{v}_2,\dots,\vec{v}_n)$,
${\mathcal B}=(\vec{w}_1,\vec{w}_2,\dots,\vec{w}_n)$ for $V$
$$f(\vec{v}_j)~=~\sum\limits^n_{i=1}a_{ij}\vec{v_i}~,~f(\vec{w}_j)~=~
\sum\limits^n_{i=1}b_{ij}\vec{w_i} \in V~.$$
The change of basis matrix $P=(p_{ij})=_{\mathcal A}[\id_V]_{\mathcal B} \in {\sf Mat}(n;F)$ is invertible, with
$$\vec{w}_j~=~\sum\limits^n_{i=1}p_{ij}\vec{v}_i \in V~.$$
We have the identity
$$B~=~P^{-1}AP \in {\sf Mat}(n;F)~,$$
so $A,B$ are conjugate.\\
(3)  {\bf Key observation}: the characteristic polynomials of conjugate $A,B \in {\sf Mat}(n,R)$ are
the same
$$\begin{array}{ll}
\chi_B(x)&=~{\sf det}(B-xI_n)~=~{\sf det}(P^{-1}AP-xI_n)\\[1ex]
&=~{\sf det}(P^{-1}(A-xI_n)P)=~{\sf det}(P)^{-1}{\sf det}(A-xI_n){\sf det}(P)\\[1ex]
&=~{\sf det}(A-xI_n)~=~\chi_A(x) \in R[x]~.
\end{array}$$
(4) In view of (2) and (3) we can define the characteristic polynomial of an endomorphism
$f:V \to V$ of an $n$-dimensional vector space over a field $F$ to be
$$\chi_f(x)~=~\chi_A(x) \in F[x]$$
with $A=_{\mathcal A}[f]_{\mathcal A} \in {\sf Mat}(n;R)$ the matrix of $f$ with respect to
{\it any} basis ${\mathcal A}$ for $V$. Thanks to \hyperref[evalchar]{Theorem \ref{evalchar}} the eigenvalues of $f$ are exactly the roots of $\chi_f$, the characteristic polynomial of $f$.
\end{rem}

\begin{exercise} Prove that for a field $F$  two matrices $A,B \in {\sf Mat}(n;F)$ are conjugate if and only
if there exists an endomorphism $f:F^n \to F^n$ and bases  ${\mathcal A}$, ${\mathcal B}$ for $F^n$ such that
$$A~=~_{\mathcal A}[f]_{\mathcal A}~,~B~=~_{\mathcal B}[f]_{\mathcal B} \in {\sf Mat}(n;F)~.$$
\end{exercise}




\begin{rem} \label{charend2}
Let $f:V \to V$ be an endomorphism of an $n$-dimensional vector space $V$ over a field $F$. Suppose given an $m$-dimensional subspace $W \subseteq V$ such that $f(W) \subseteq W$, so that there are defined endomorphisms of the subspace and the quotient space
$$\begin{array}{l}
g~:~W\to W~;~ \vec{w} \mapsto f(\vec{w})~,\\[1ex]
h~:~V/W \to V/W~;~W+\vec{v} \mapsto W+f(\vec{v})~.
\end{array}$$
Any ordered basis ${\mathcal A}=(\vec{w}_1,\vec{w}_2,\dots,\vec{w}_m)$ for $W$ can be extended to an ordered basis for $V$
$${\mathcal B}~=~(\vec{w}_1,\vec{w}_2,\dots,\vec{w}_m,\vec{v}_{m+1},\vec{v}_{m+2}\dots,\vec{v}_n)~.$$
The images of the $\vec{v}_j$'s under the canonical projection ${\sf can}:V \to V/W$
 are then an ordered basis for $V/W$
 $${\mathcal C}~=~({\sf can}({v}_{m+1}),{\sf can}(v_{m+2}),\dots,{\sf can}(\vec{v}_n))~.$$
 Let $a_{ij},b_{jk},c_{ik} \in F$ be the coefficients in the linear combinations
$$f(\vec{w}_j)~=~\sum\limits^m_{i=1} a_{ij}\vec{w}_i \in W~,~
f(\vec{v}_k)~=~~\sum\limits^n_{j=m+1} b_{jk}\vec{v}_j+\sum\limits^m_{i=1} c_{ik}\vec{w}_i \in V$$
and define the linear map
$$e~:~V/W \to W~;~W+\vec{v}_k \mapsto \sum\limits^m_{i=1} c_{ik}\vec{w}_i~.$$
The $n \times n$ matrix of $f$ with respect to the basis $\mathcal A$ is
$$~_{\mathcal B}[f]_{\mathcal B}~=~\begin{pmatrix}
~_{\mathcal A}[g]_{\mathcal A}&~_{\mathcal A}[e]_{\mathcal C}\\
0 &~_{\mathcal C}[h]_{\mathcal C}\end{pmatrix}~=~
\begin{pmatrix} a_{ij} & c_{ik} \\ 0 & b_{jk} \end{pmatrix}$$
 blocked as an $\begin{pmatrix}
m \times m & m \times (n-m)\\
(n-m) \times m & (n-m) \times (n-m)
\end{pmatrix}$  matrix. Applying \hyperref[blockdet]{Exercise \ref{blockdet}} to the $n \times n$
matrix with entries in $F[x]$
$$ ~_{\mathcal B}[f]_{\mathcal B}-xI_n~=~\begin{pmatrix}
~_{\mathcal A}[g]_{\mathcal A}-xI_m&~_{\mathcal A}[e]_{\mathcal C}\\
0 &~_{\mathcal C}[h]_{\mathcal C}-xI_{n-m}\end{pmatrix}~=~
\begin{pmatrix} a_{ij} -xI_m & c_{ik} \\ 0 & b_{jk}-xI_{n-m} \end{pmatrix}$$
we have that the characteristic polynomial of $f$ is the product of the characteristic polynomials of $g$ and $h$
$$\begin{array}{ll}
\chi_f(x)&=~{\sf det}( ~_{\mathcal B}[f]_{\mathcal B}-xI_n)\\[1ex]
&=~{\sf det}( ~_{\mathcal A}[g]_{\mathcal A}-xI_m){\sf det}( ~_{\mathcal C}[h]_{\mathcal C}-xI_{n-m})\\[1ex]
&=~\chi_g(x)\chi_h(x) \in F[x]~.
\end{array}$$
\end{rem}

\begin{proof}[Proof of Theorem \ref{evalsexist}]
\hyperref[evalsexist]{Theorem \ref{evalsexist}} states that every endomorphism of a non-zero vector space over an algebraically closed field $F$ has an eigenvalue. To see why this is true, you only need to notice that the characteristic polynomial $\chi_f \in F[x]$ is not constant. Then the \hyperref[algclsdef]{definition of algebraically closed} kicks in to say that since $F$ is algebraically closed $\chi_f$ must have at least one root in $F$, say $\lambda\in F$. But then \hyperref[charend]{Remark \ref{charend} (4)} demonstrates that $\lambda$ is an eigenvalue of $f$.
\end{proof}

\section{Triangularisable, Diagonalisable, and the Cayley-Hamilton Theorem}
Remember that in Exercise $2$ of the 2nd Workshop you proved that $A\in {\sf Mat}(n;F)$ satisfies a polynomial equation: there exists $P\in F[x]$ such that $P(A) = 0$. I will now tell you what that polynomial is, and I will show you that it is even possible to replace $F$ with $R$, an arbitrary commutative ring.

\begin{proposition} [Triangularisability] \label{triendo} Let $f:V\to V$ be an endomorphism of a finite dimensional $F$-vector space $V$. The following two statements are equivalent:
\begin{enumerate}
\item The vector space $V$ has an ordered basis $\mathcal{B}=(\vec{v}_1,\vec{v}_2,\dots,\vec{v}_n)$ such that
$$\begin{array}{l}
f(\vec{v}_1)~=~a_{11}\vec{v}_1~,\\[1ex]
f(\vec{v}_2)~=~a_{12}\vec{v}_1+a_{22}\vec{v}_2~,\\[1ex]
\hskip12mm \vdots \\[1ex]
f(\vec{v}_n)~=~a_{1n}\vec{v}_1+a_{2n}\vec{v}_2+\dots+a_{nn}\vec{v}_n \in V
\end{array}
$$
(so that the first basis vector $\vec{v}_1$ is an eigenvector, with eigenvalue $a_{11}$) or
equivalently such that the $n \times n$ matrix   ${}_{\mathcal{B}}[f]_{\mathcal{B}}=(a_{ij})$ representing $f$ with respect to $\mathcal{B}$ is upper triangular
$$A~=~\begin{pmatrix} a_{11} & a_{12} & a_{13} & \dots & a_{1n}\\
0 & a_{22} & a_{23} & \dots & a_{2n}\\
0 & 0 & a_{33} & \dots & a_{3n}\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 &  \dots & a_{nn} \end{pmatrix}~.
$$
When this happens, I will say that $f$ is {\bf triangularisable}.
\item The characteristic polynomial $\chi_f(x)$ of $f$ decomposes into linear factors in $F[x]$.
\end{enumerate}
\end{proposition}
\begin{proof}
1 $\to$ 2: This is clear from \hyperref[tridet]{Example \ref{tridet}} which describes the determinant of an upper triangular matrix: if ${}_{\mathcal{B}}[f]_{\mathcal{B}}=(a_{ij})$ is upper triangular with diagonal entries $a_{ii}=\lambda_i$ then $\chi_f(x) = (\lambda_1 -x)\cdots (\lambda_n -x)$.

2 $\to$ 1: I'm going to prove this by induction on $n = \dim(V)$.

\noindent
{\it The case $n=1$}: the matrix is $(1\times 1)$ with respect to any basis, so is upper triangular!

\noindent
{\it Induction hypothesis}: Assume $n>1$ and that the result is true for endomorphisms of $(n-1)$-dimensional vector spaces.

I will now prove the result for $n$. By hypothesis there is a root $a_{11}\in F$ of the polynomial $\chi_f(x) \in F[x]$, and so \hyperref[evalchar]{Theorem \ref{evalchar}} and
\hyperref[charend2]{Remark \ref{charend2}}
show that $a_{11}$ is an eigenvalue of $f$, with an eigenvector $\vec{v}_1
\neq \vec{0} \in V$ such that $f(\vec{v}_1)=a_{11}\vec{v}_1 \in V$. The 1-dimensional subspace $W = \{ \mu \vec{v}_1|\mu \in F\} \subseteq V$ is such that $f(W) \subseteq W$, so we can proceed as in \hyperref[charend]{Remark \ref{charend}}
(2) to define endomorphisms $g:W \to W$, $h:V/W \to V/W$. It follows from
$$\chi_f(x)~=~\chi_g(x)\chi_h(x)~=~(a_{11}-x)\chi_h(x) \in F[x]$$
and the hypothesis that $\chi_f(x)$ decomposes into linear factors in $F[x]$ that $\chi_h(x)$
also decomposes into linear factors in $F[x]$, namely all the linear factors
of $\chi_f(x)$ except for one $(a_{11}-x)$. (We are using
 \hyperref[euclid]{Theorem \ref{euclid}} here, the division algorithm for polynomials with coefficients in the field $F$). The subspace $W \subseteq V$ has basis ${\mathcal A}=(\vec{v}_1)$ such that $_{\mathcal A}[g]_{\mathcal A}=(a_{11})$. The quotient space $V/W$ is $(n-1)$-dimensional, and by the inductive hypothesis there is an ordered basis ${\mathcal D}=(\vec{u}_2,\vec{u}_3,\dots,\vec{u}_n)$ for $V/W$ such that the $(n-1)\times (n-1)$-matrix
$_{\mathcal D}[h]_{\mathcal D}$ is upper triangular, say
$$_{\mathcal D}[h]_{\mathcal D}~=~\begin{pmatrix} a_{22} & a_{23} & \dots &a_{2 n}\\
0 & a_{33} & \dots &a_{3 n}\\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \dots &a_{nn}\end{pmatrix}~.$$
For each $\vec{u}_j \in V/W$ $(2 \leqslant j \leqslant n)$ choose a vector
$\vec{v}_j \in V$ with ${\sf can}(\vec{v}_j)=\vec{u}_j$. It follows from
$$h(\vec{u}_j)-\sum\limits^n_{i=2}a_{ij}\vec{u}_i ~=~\vec{0}_{V/W} \in V/W$$
that
$${\sf can}(f(\vec{v}_j)-\sum\limits^n_{i=2}a_{ij}\vec{v}_i)~=~h(\vec{u}_j)-\sum\limits^n_{i=2}a_{ij}\vec{u}_i
~=~\vec{0}_{V/W}~=~W+\vec{0}_V \in V/W~,$$
so that $f(\vec{v}_j)-\sum\limits^n_{i=2}a_{ij}\vec{v}_i \in W \subseteq V$, say
$$f(\vec{v}_j)-\sum\limits^n_{i=2}a_{ij}\vec{v}_i~=~a_{1j} \vec{v}_1~(2 \leqslant j \leqslant n) \in W~.$$
Putting all this together gives
$$f(\vec{v}_j)~=~\sum\limits^n_{i=1}a_{ij}\vec{v}_i \in V~(1 \leqslant j \leqslant n)$$
with $a_{ij}=0 \in F$ for $i>j$. The ordered  basis ${\mathcal B}=(\vec{v}_1,\vec{v}_2,\dots,\vec{v}_n)$ for $V$ is such that the $n \times n$-matrix
$$_{\mathcal B}[f]_{\mathcal B}~=~\begin{pmatrix}
~_{\mathcal A}[g]_{\mathcal A}&~_{\mathcal A}[e]_{\mathcal D}\\
0 &~_{\mathcal D}[h]_{\mathcal D}\end{pmatrix}~=~
\begin{pmatrix} a_{11} & a_{12} & a_{13} & \dots & a_{1n}\\
0 & a_{22} & a_{23} & \dots & a_{2n}\\
0 & 0 & a_{33} & \dots & a_{3n}\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 &  \dots & a_{nn} \end{pmatrix}~.
$$
is upper triangular, with $e:V/W \to W;\vec{u}_j \mapsto a_{1j}\vec{v}_1$.
\end{proof}

\begin{rem}
(1) An endomorphism $A:F^n \to F^n$ is triangularisable if and only if $A=(a_{ij})$ is conjugate
to an upper triangular matrix $B=(b_{ij})$ ($b_{ij}=0$ for $i>j$), with $P^{-1}AP=B$ for
an invertible matrix $P$.\\
(2) Combining \hyperref[triendo]{Proposition \ref{triendo}} with \hyperref[decpoly]{Theorem \ref{decpoly}} shows that any endomorphism of a finite dimensional $\mathbb{C}$-vector space is triangularisable. On the other hand there are endomorphisms of $\mathbb{R}$-vector spaces that are non triangularisable, for instance the endomorphism of $\mathbb{R}^2$ given by rotation anticlockwise by $\theta$ radians is represented in the standard basis by the matrix $$ R:=\begin{pmatrix} \cos \theta & -\sin \theta \\ \sin \theta & \cos \theta \end{pmatrix}$$ and therefore has characteristic polynomial $\chi_R (x) = x^2 - 2x \cos \theta + 1$. You can check that this has real roots if and only if $\cos \theta = \pm 1$, that is if and only if $\theta$ is an integral multiple of $\pi$. So, by \hyperref[triendo]{Proposition \ref{triendo}} this endomorphism is not triangularisable unless $\theta = n\pi$ for some $n\in \mathbb{Z}$.\\
(3) An endomorphism $f:V \to V$ of an $n$-dimensional $F$-vector space $V$ is triangularisable if and only if there is a sequence of subspaces
$$V_0~=~\{0\} \subset V_1 \subset V_2 \subset \dots \subset V_n~=~V$$
such that $V_i$ is $i$-dimensional and $f(V_i) \subseteq V_i$. In the proof of
 \hyperref[triendo]{Proposition \ref{triendo}}  $V_i$ is the subspace of $V$ spanned by
$ \vec{v}_1,\vec{v}_2,\dots,\vec{v}_i$. Each quotient space $V_i/V_{i-1}$ is 1-dimensional with the endomorphism
$$f_i~:~V_i/V_{i-1} \to V_i/V_{i-1}~;~V_{i-1}+\vec{v} \mapsto V_{i-1}+f(\vec{v})~=~V_{i-1}+\lambda_i\vec{v}$$
given by scalar multiplication by the $i$th root $\lambda_i\in F$ of the characteristic polynomial
$$\chi_f(x)~=~(\lambda_1-x)(\lambda_2-x) \dots (\lambda_n-x) \in F[x]~.$$
(4) We have already seen the value of triangular matrices (not just square ones) in Gaussian elimination. Triangular matrices play an important role in numerical  analysis. For example, if $A=(a_{ij})\in {\sf Mat}(n;F)$ is an upper triangular $n \times n$ matrix
which is invertible (so that each $a_{ii} \neq 0 \in F$), then a system of simultaneous
linear equations
$$\sum\limits^n_{j=i}a_{ij}x_j~=~b_i \in F~(i=1,2,\dots,n)$$
can be solved for $x_n,x_{n-1},\dots,x_1 \in F$ using only division by the non-zero diagonal entries $a_{ii}$ and addition and subtraction
$$\begin{array}{l}
x_n~=~b_n/a_{nn}~,\\[1ex]
x_{n-1}~=~(b_{n-1}-a_{n-1\,n}x_n)/a_{n-1\, n-1}~,\\[1ex]
\vdots\\[1ex]
x_1~=~(b_1-\sum\limits^n_{j=2}a_{1j}x_j)/a_{11} \in F~.
\end{array}$$
\end{rem}
\begin{ex}\label{triex}
Let $f: \mathbb{R}^3 \to \mathbb{R}^3$ be represented in the standard basis by the matrix $$ [f] = \begin{pmatrix} 14 & 8 & 3 \\  \hlight{-17} & -9 & -3 \\ \hlight{1} & \hlight{0} & 0\end{pmatrix}$$ I've highlighted the entries that show that the matrix is not in upper triangular form yet. Then, using your pen and paper, or Wolfram Alpha, $\chi_f(x) = -x^3 + 5x^2 -7x +3  =  (1-x)^2(3-x)$ so that the eigenvalues of $f$ are $1$ and $3$ and \hyperref[triendo]{Proposition \ref{triendo}} implies that $f$ is triangularisable. To find a basis for $\mathbb{R}^3$ that produces an upper triangular matrix representing $f$, I study the eigenvalue $3$:
$$ [f] - 3I_3 = \begin{pmatrix} 11 & 8 & 3 \\ -17 & -12 & -3 \\ 1 & 0 & -3 \end{pmatrix}$$ Let $W$ be the image of this matrix. By looking at the columns of the matrix I see that $W$ has a basis $\mathcal{A} =  \{ \vec{w}_1 = (2,-3,0)^{\sf T}, \vec{w}_2 = (1,-1,-1)^{\sf T}\}$. According to the proof of \hyperref[triendo]{Proposition \ref{triendo}} I should now consider $f|_W$: $$ f(\vec{w}_1) = \begin{pmatrix} 14 & 8 & 3 \\ -17 & -9 & -3 \\ 1 & 0 & 0\end{pmatrix} \begin{pmatrix} 2 \\ -3 \\ 0 \end{pmatrix} =  \begin{pmatrix} 4 \\ -7 \\ 2\end{pmatrix} =  3\vec{w}_1 -2 \vec{w}_2$$   and $$ f(\vec{w}_2) = \begin{pmatrix} 14 & 8 & 3 \\ -17 & -9 & -3 \\ 1 & 0 & 0\end{pmatrix} \begin{pmatrix} 1 \\ -1 \\ -1 \end{pmatrix} =  \begin{pmatrix} 3 \\ -5 \\ 1\end{pmatrix} =  2\vec{w}_1 - \vec{w}_2$$ So, if I extend the basis $\mathcal{A}$ of $W$ to a basis $\mathcal{B}$ of $V$, say $\mathcal{B} = \{ \vec{w}_1, \vec{w}_2 , \vec{e}_1 \}$, I use $f(\vec{e}_1) = 6\vec{w}_1 - \vec{w}_2 + 3\vec{e}_1$ to find $${}_{\mathcal{B}}[f]_{\mathcal{B}} = \left( \begin{array}{cc|c}  3 & 2 & 6 \\ \hlight{-2} & - 1 & -1 \\  \hline 0 & 0 & 3 \end{array}\right)= \left( \begin{array}{c|c}  {}_{\mathcal{A}}[f]_{\mathcal{A}} & \ast \rule[-1ex]{0pt}{2ex} \\ \hline 0 & 3 \end{array}\right) $$ The characteristic polynomial of $f|_W$ is $\chi_{f|_W} (x) = x^2 - 2x+ 1 = (1-x)^2$. I now study the eigenvalue $1$: $${}_{\mathcal{A}}[f|_W]_{\mathcal{A}} - 1I_2 = \begin{pmatrix} 2 & 2 \\ -2 & -2 \end{pmatrix} $$ The image of this mapping, which I'll call $U$, is $\langle (1,-1)^T = \vec{w}_1 - \vec{w}_2 \rangle$. Again, according to the proof of \hyperref[triendo]{Proposition \ref{triendo}}, I should consider $f|_U$: $$f(\vec{w}_1 - \vec{w}_2) = \begin{pmatrix} 14 & 8 & 3 \\ -17 & -9 & -3 \\ 1 & 0 & 0\end{pmatrix} \begin{pmatrix} 1 \\ -2 \\ 1 \end{pmatrix}  = \begin{pmatrix} 1 \\ -2 \\ 1\end{pmatrix} = \vec{w}_1 - \vec{w}_2$$ So I should pick a basis whose first element is $\vec{w}_1 - \vec{w}_2$ is a basis of $U$, whose second element extends this to a basis of $W$, say $\vec{w}_1$, and whose third element extends this to a basis of $V$, say $\vec{e}_1$. Now I calculate the matrix representing $f$ with respect to the basis $\mathcal{B}' = \{ \vec{w}_1 - \vec{w}_2, \vec{w}_1, \vec{e}_1 \} = \{ (1,-2,1)^{\sf T}, (2,-3,0)^{\sf T}, (1,0,0)^{\sf T} \}$ to be:
$${}_{\mathcal{B}'}[f]_{\mathcal{B}'} = \begin{pmatrix} 1 & 2 & 1 \\ 0& 1 & 5 \\ 0 & 0 & 3 \end{pmatrix}$$ As \hyperref[triendo]{Proposition \ref{triendo}} shows it must be, this matrix is upper triangular.
\end{ex}

\begin{rem} Combining \hyperref[nilpex]{Exercise \ref{nilpex}} with \hyperref[triendo]{Proposition \ref{triendo}} shows that a matrix $A\in {\sf Mat}(n;F)$ is nilpotent if and only if $\chi_A(x) = (-x)^n$.
\end{rem}

\begin{definition} An endomorphism $f:V\to V$ of an $F$-vector space $V$ is {\bf diagonalisable} if and only if there exists a basis of $V$ consisting of eigenvectors of $f$. If $V$ is finite dimensional then this is the same as saying that there exists an ordered basis $\mathcal{B} = \{ \vec{v}_1, \ldots , \vec{v}_n \}$ such that corresponding matrix representing $f$ is diagonal, that is ${}_{\mathcal{B}}[f]_{\mathcal{B}} = {\sf diag}(\lambda_1, \ldots , \lambda_n)$. In this case, of course, $f(\vec{v}_i ) = \lambda_i \vec{v}_i$.

A square matrix $A\in {\sf Mat}(n;F)$ is {\bf diagonalisable} if and only if the corresponding linear mapping $F^n \to F^n$ given by left multiplication by $A$ is diagonalisable. Thanks to \hyperref[cobcor]{Corollary \ref{cobcor}} this just means that $A$ is conjugate to
a diagonal matrix,   there exists an invertible matrix $P\in {\sf GL}(n;F)$ such that $P^{-1}AP = {\sf diag}(\lambda_1, \ldots , \lambda_n)$. In this case the columns of $P$ are the vectors of a basis of $F^n$ consisting of eigenvectors of $A$ with eigenvalues $\lambda_1, \ldots , \lambda_n$.
\end{definition}

\begin{ex}
A nilpotent matrix is diagonalisable if and only if it is the zero matrix. (Think about it!)
\end{ex}

\begin{ex}
Let $A$ be the real matrix $$A = \begin{pmatrix} 7 & 2 \\ -18 & -6 \end{pmatrix}$$ I calculate that $\chi_A(x) = (7-x)(-6 -x) + 36  = x^2 - x - 6 = (x-3)(x+2)$ so that the eigenvalues of $A$ are $3$ and $-2$. To calculate eigenvectors with eigenvalue $3$ I calculate $$A - 3I_n = \begin{pmatrix} 4 & 2 \\ -18 & -9 \end{pmatrix}$$ and observe then that $(A- 3I_n){\vec v} = \vec{0}$ if and only if $\vec{v} = \alpha (1, -2)^{\sf T}$ for some $\alpha \in \mathbb{R}$, so that $E(3, A) = \{ \alpha (1,-2)^{\sf T} : \alpha \in \mathbb{R} \}$. Similarly, I calculate that $E(-2, A) = \{ \alpha (2, -9)^{\sf T}: \alpha \in \mathbb{R} \}$. It follows that if I take $P = \begin{pmatrix} 1 & 2 \\ -2 & -9 \end{pmatrix}$ then $P^{-1}AP = {\rm diag}(3,-2)$, which can be easily checked.
\end{ex}

\begin{lemma}[Linear independence of Eigenvectors] \label{diageval} Let $f:V\to V$ be an endomorphism of a vector space $V$ and let $\vec{v}_1, \ldots , \vec{v}_n$ be eigenvectors of $f$ with pairwise different eigenvalues $\lambda_1, \ldots , \lambda_n$. Then the vectors $\vec{v}_1, \ldots , \vec{v}_n$ are linearly independent.
\end{lemma}

\begin{proof}
Suppose that $\alpha_1 \vec{v}_1 + \cdots + \alpha_n\vec{v}_n = \vec{0}$. Let's see what happens when I apply the endomorphism $(f-\lambda_2 \id_V) \circ \cdots \circ (f-\lambda_n \id_V)$ of $V$ to $\vec{v}_i$:
$$(f-\lambda_2 \id_V) \circ \cdots \circ (f-\lambda_n \id_V) (\vec{v}_i) = \prod_{j=2}^n (\lambda_i -\lambda_j) \vec{v}_i = \begin{cases} \prod_{j=2}^n (\lambda_1 -\lambda_j) \vec{v}_1 \quad & i=1 \\ \vec{0} & i\neq 1 \end{cases}.$$ From this observation and the equation $\alpha_1 \vec{v}_1 + \cdots + \alpha_n\vec{v}_n = \vec{0}$, I deduce that $\alpha_1  \prod_{j=2}^n (\lambda_1 -\lambda_j) \vec{v}_1 = \vec{0}$. Since $\lambda_1 \neq \lambda_j$ for all $j>1$, it follows that $\alpha_1=0$. A similar argument shows also that $\alpha_2 = \cdots = \alpha_n = 0$. In other words, the vectors are linearly independent.
\end{proof}

This means that if $f:V\to V$ is an endomorphism of a finite dimensional vector space $V$ whose characteristic polynomial decomposes into linear factors $\chi_f(x) = \prod_{j=1}^n (\lambda_j -x)$ with pairwise different roots, then $f$ is a diagonalisable.

\begin{theorem}[The Cayley-Hamilton Theorem] \label{CHthm} Let $A\in {\sf Mat}(n;R)$ be a square matrix with entries in a commutative ring $R$. Then evaluating its characteristic polynomial $\chi_A(x) \in R[x]$ at the matrix $A$ gives zero.
\end{theorem}

\begin{ex} Let $A = \begin{pmatrix} 14 & 8 & 3 \\  {-17} & -9 & -3 \\ {1} & {0} & 0\end{pmatrix}$, the matrix appearing in \hyperref[triex]{Example \ref{triex}}. Its characteristic polynomial is $\chi_A(x) = -x^3 + 5x^2 -7x + 3$. Now I calculate \begin{eqnarray*} - A^3 + 5 A^2 - 7A + 3I &=& - \begin{pmatrix} 220 & 144 & 69 \\ -321 & -209 & - 99 \\ 63 & 40 & 18 \end{pmatrix} +5\begin{pmatrix} 63 & 40 & 18 \\ -88 & -55 & -24 \\ 14 & 8 & 3 \end{pmatrix} \\ && \qquad -7 \begin{pmatrix} 14 & 8 & 3 \\  {-17} & -9 & -3 \\ {1} & {0} & 0\end{pmatrix}  + 3\begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix} \\ & =& \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}\end{eqnarray*} Just as the Cayley-Hamilton Theorem predicts.
\end{ex}

\begin{ex} The polynomial extension ring $F[x]$ of a field $F$ is an (infinite-dimensional) $F$-vector space.
For a degree $n$ polynomial
$$P(x)~=~a_0+a_1x+a_2x^2 + \dots +a_nx^n \in F[x]~(a_n \neq 0 \in F)$$
the principal ideal generated by $P(x)$ is a subspace
$$(P(x))~=~\{P(x)Q(x)\,\vert\,Q(x) \in F[x]\} \subset  F[x]$$
(which is also infinite-dimensional).
The quotient space  $V=F[x]/(P(x))$ is an $n$-dimensional $F$-vector space with basis
$${\mathcal B}~=~(\vec{v}_0,\vec{v}_1,\dots,\vec{v}_{n-1})~=~(1,x,x^2,\dots,x^{n-1})~.$$
The endomorphism
$$f~=~\hbox{multiplication by $x$}~:~V \to V~;~x^i \mapsto x^{i+1}$$
is such that
$$\begin{array}{l}
f(\vec{v}_i)~=x^{i+1}~=~\vec{v}_{i+1}~(0 \leqslant i \leqslant n-2)~,\\[1ex]
f(\vec{v}_{n-1})~=~x^n~=~-(a_n)^{-1}(\sum\limits^{n-1}_{i=0}a_ix^i)~=~-(a_n)^{-1}(\sum\limits^{n-1}_{i=0}a_i\vec{v}_i) \in V~,
\end{array}$$
so it has $n \times n$ matrix
$$A~=~~_{\mathcal B}[f]_{\mathcal B}~=~\begin{pmatrix}
 0 & 0 & 0 & \dots & -c_0 \\
 1 & 0 & 0 & \dots & -c_1 \\
 0 & 1 & 0 & \dots & -c_2 \\
 \vdots & \vdots & \vdots & \ddots & \vdots \\
 0 & 0 & 0 & \dots & -c_{n-1}
 \end{pmatrix}~{\rm with}~c_i~=~(a_n)^{-1}a_i \in F~(0 \leqslant i \leqslant n-1)~.$$
Exercise 6 in Homework  8 is to calculate the characteristic polynomial $\chi_f(x)=\chi_A(x)\in F[x]$.  After you have done this verify the Cayley-Hamilton Theorem
for $f$ !
\end{ex}

\begin{proof}[Intuitive Proof] The first thing to observe is that if $A$ is a diagonal matrix then it is straightforward to check that the theorem holds. For suppose that $A = {\sf diag} (\lambda_1, \ldots , \lambda_n)$. Then $\chi_A(x) = \prod_{j=1}^n (\lambda_j - x)$. I must show then that $\prod_{j=1}^n ( \lambda_j I_n - A ) = 0$. But the matrix $\lambda_j I_n - A$ is a diagonal matrix whose $j$-th entry is zero. Therefore multiplying all $n$ of these diagonal matrices together produces $$ \begin{pmatrix} 0 & & &  \\ & \lambda_2 -\lambda_1 & &  \\ & & \ddots &  \\ &&& \lambda_n-\lambda_1  \end{pmatrix} \cdot \begin{pmatrix} \lambda_1 - \lambda_2 & & &  \\ & 0 & &  \\ & & \ddots &  \\ &&& \lambda_n - \lambda_2  \end{pmatrix}  \cdots = \begin{pmatrix} 0 & & &  \\ & 0 & &  \\ & & \ddots &  \\ &&&0  \end{pmatrix} $$ So in this diagonal case the theorem holds.

Now suppose that $A\in {\sf Mat}(n;F)$ where $F$ is an algebraically closed field. Then $P:= \chi_A(x)\in F[x]$ decomposes into linear factors $P = \prod_{j=1}^n (\lambda_j - x)$ and so has $n$ roots. It is more likely than not that the roots $\{ \lambda_j\}_{1\leqslant j \leqslant n}$ are pairwise different, and in that case it follows from \hyperref[diageval]{Lemma \ref{diageval}} that the matrix $A$ is a diagonalisable. Hence I can find $S\in {\sf GL}(n;F)$ such that $S^{-1}AS = {\sf diag}(\lambda_1, \ldots , \lambda_n)$. Observing that $$(S^{-1}AS)^n = \underbrace{(S^{-1}AS)(S^{-1}AS) \cdots (S^{-1}AS)}_{n\text{ times}} = S^{-1} A^n S$$ because all the $SS^{-1}$ products cancel, I see that $P(S^{-1}AS) = S^{-1}P(A) S$. Therefore $P(A) = 0$ if and only if $P(S^{-1}AS) = 0$, so that I need only check that $P({\sf diag}(\lambda_1, \ldots , \lambda_n)) = 0$. But that's what I did in the first paragraph.
\end{proof}

\begin{rem}This proof won't cut the mustard in general for a couple of reasons. First I have written ``more likely than not" and that is not a mathematical statement: one can make this precise using \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath11120.htm}{Algebraic Geometry}, but that's for your future. Second, in order to decompose the characteristic polynomial, this proof required me to use an algebraically closed field $F$ rather than a commutative ring. So even if I had just wanted to consider matrices over fields I would have struggled! I could certainly have considered an $(n\times n)$-real matrix as a complex matrix and deduced the result for real matrices from the result for complex matrices, but what if the field had been a finite field $\mathbb{F}_p$? This argument would only have worked if I knew that $\mathbb{F}_p$ could sit inside an algebraically closed, just as $\mathbb{R}$ sits inside $\mathbb{C}$. This is true, but it is proved in \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath10050.htm}{Jewels of Algebra}, so again that's in your future.
\end{rem}

\begin{proof}[Formally complete, unintuitive proof] Let $S = R[x]$, the ring of polynomials with coefficients in $R$. Given $A\in {\sf Mat}(n;R)$ let $B = A - xI_n \in {\sf Mat}(n; S)$. Let ${\sf adj}(B)$ be the adjugate of $B$, as defined in \hyperref[defadjugate]{Definition \ref{defadjugate}}. By \hyperref[Cramer]{Cramer's Rule} \begin{equation} \label{CHadj} B \cdot {\sf adj}(B) = ({\sf det}(A - xI_n))I_n = \chi_A(x) I_n\end{equation}
The adjugate ${\sf adj}(B)\in {\sf Mat}(n;S)$ and so it is an $(n\times n)$-matrix with entries in $S=R[x]$. I can consider such a matrix equally well as an element of the ring ${\sf Mat}(n;R)[x]$ of polynomials with coefficients with entries in ${\sf Mat}(n;R)$. For instance $$\begin{pmatrix} 1+ x + 3x^2 & - x \\ x^{10} &  2+x\end{pmatrix} =  \begin{pmatrix} 1 & 0 \\ 0 & 2 \end{pmatrix} + x\begin{pmatrix}  1 & -1 \\ 0 & 1 \end{pmatrix} + x^2 \begin{pmatrix} 3 & 0 \\ 0 & 0 \end{pmatrix} + x^{10}\begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix} $$
In this way ${\sf adj}(B) = \sum_{i\geqslant 0} x^i C_i$ where each $C_i \in {\sf Mat}(n;R)$. \hyperref[CHadj]{Equation \eqref{CHadj}} then gives an equality of polynomials with coefficients in ${\sf Mat}(n;R)$ \begin{eqnarray*} \chi_A(x) I_n &=& (A - xI_n) \cdot {\sf adj}(B)  \\ & = & (A- xI_n) \cdot \left(\sum_{i\geqslant 0} x^i C_i\right) \\ & = & \sum_{i\geqslant 0} x^i AC_i - \sum_{i\geqslant 0} x^{i+1} C_i \\ & = & AC_0 + \sum_{i\geqslant 1} x^i(AC_i - C_{i-1})\end{eqnarray*}
If I write $\chi_A(x) = x^n + x^{n-1} c_{n-1} + \cdots + xc_1 + c_0$ with each $c_i\in R$, then I get  a sequence of $n+1$ equalities by comparing coefficients of the various powers of $x$:
$$ AC_0 = c_0I_n; \quad AC_1 - C_0 = c_1I_n; \quad \ldots \quad ; \quad AC_{n-1} - C_{n-2} = c_{n-1}I_n; \quad AC_{n} - C_{n-1} = I_n$$
Multiplying the $i$-th one of these equalities by $A^i$ produces $A^i(AC_i - C_{i-1}) = A^i c_i$ which is the $i$-th term in the evaluation of $\chi_A(x)$ at the matrix $A$. I deduce that \begin{eqnarray*}
A^n + A^{n-1} c_{n-1} + \cdots Ac_1 + c_0 & = & A^n(AC_n - C_{n+1}) + A^{n-1}(AC_{n-1} - C_{n-2})  \\ && \qquad + A^{n-2}(AC_{n-2} - C_{n-3}) + \cdots + A(AC_1 - C_0) + AC_0 \\&=& A^{n+1}C_n \end{eqnarray*}
So the theorem will be complete if I can show that $A^{n+1}C_n = 0$. In fact $C_n = 0$ for the following reason. Recall that $C_n$ is the coefficient of $x^n$ in ${\sf adj}(A-xI_n)$. But the entries of ${\sf adj}(A-xI_n)$ are by definition the different cofactors of the matrix $A-xI_n$. These cofactors are obtained as the determinant of $(A-xI_n)\langle i,j\rangle$, the matrices gotten from $A-xI_n$ by deleting the $i$-th row and $j$-th column. But $(A-xI_n)\langle i,j\rangle$ is an $((n-1)\times (n-1))$-matrix, so its determinant is a sum of products of $n-1$ entries of $(A-xI_n)\langle i,j\rangle$. Each of these entries has degree at most $1$ as a polynomial in $x$, and so each of the products in the determinant has degree at most $n-1$, from which I deduce that the degree of each cofactor is at most $n-1$, and hence $C_n = 0$.
 \end{proof}

 Well, I did warn you. In any case you now see that Cayley-Hamilton is true and that this proof, even if you were only considering a matrix with entries in a field $R = F$, required considering matrices and determinants of matrices with entries in the ring $R[t]$ which is not a field. So, at last, rings have become useful! In fact, an intuitive and formally correct proof for this theorem exists, but really requires the development of some serious commutative ring theory, currently beyond the scope of the undergraduate syllabus in Edinburgh.




\section{Google's PageRank Algorithm}

What is the internet? {\it It is a library like no other}: it is big, with, I've heard, over 25 billion documents. Contrast this with \href{http://en.wikipedia.org/wiki/Edinburgh_University_Library}{Edinburgh University Library} which, I've heard, has around 200,000 books in its Special Collection. {\it It is a library like no other}: anyone can add a document at any time, telling nobody. Contrast this with Edinburgh University Library.  {\it It is a library like no other}: the documents change frequently, with, I've heard, 40\% of webpages changing their data each week. Contrast this with Edinburgh University Library.

What about the data? Search engines typically have an army of spiders deployed all over the web, retrieving pages, indexing the words in each document, and then scuttling back to a depot somewhere to store the information. Whenever you input into a search engine a phrase such as ``honours algebra", the engine uses this information to determine all the documents on the web containing the words ``honours" and the word "algebra"  (on Google 368,000 hits for ``honours algebra" without inverted commas; 3,980 for `` "honours algebra" "). Add to this that 95\% of the text in webpages is composed from 10,000 words and you see the problem: how can a ranking of the importance of the pages that fit the search criteria be made, placing the most important pages first?

How does this happen? Google's PageRank algorithm assesses the importance of webpage in a purely automated fashion, without human interference. One incredibly useful difference between a traditional library and the internet, or at the very least a resource that is not used by librarians, is all the cross-referencing that goes on between different webpages via \href{http://en.wikipedia.org/wiki/Hyperlink}{hyperlinks}. It is this information that Google uses to rank its pages, not the content of the pages themselves.
\medskip

\noindent
{\bf Graphs and Matrices} If I focus on hyperlinks as connections between webpages then I should think of the web as a graph, \href{http://en.wikipedia.org/wiki/Discrete_mathematics#Graph_theory}{in the sense of discrete mathematics}. So each document on the web is a vertex of the graph and there is an arrow from the vertex $i$ to vertex $j$ if there is a hyperlink on page $i$ going to page $j$. So this a graph with perhaps 25 billion nodes!

Any graph can be encoded in a square matrix $L$ with real entries:
 $$\ell_{ij} = \begin{cases} 1 \quad & \text{if page $j$ has a link to page $i$} \\
 0 & \text{if not}.
 \end{cases}
 $$
For the web, $L\in {\sf Mat}(n; \mathbb{R})$ where $n$ is approximately a $25\times 10^9$.
\begin{ex} \label{exgraph}
Here is a smaller example:
$$
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,
                    semithick]
  \tikzstyle{every state}=[fill=red,draw=none,text=white]

  \node[state] (A)                    {$A$};
  \node[state]         (B) [below of=A] {$B$};
   \node[state]         (C) [right of=A] {$C$};
   \node[state]         (D) [below of=C] {$D$};
   \node[state]         (E) [right of=C] {$E$};
   \node[state]         (F) [below of=E] {$F$};
   \node[state]         (G) [right of=E] {$G$};
  \node[state]         (H) [below of=G] {$H$};

  \path (A) edge              node {} (C)
  (A) edge              node {} (B)
  (C) edge              node {} (B)
  (B) edge [bend left]             node {} (D)
  (D) edge  [bend left]            node {} (B)
  (D) edge              node {} (E)
  (C) edge              node {} (E)
  (D) edge              node {} (F)
  (E) edge              node {} (F)
   (E) edge              node {} (H)
   (E) edge node {} (G)
  (F) edge  [bend left]            node {} (H)
  (H) edge [bend left]             node {} (F)
    (G) edge [bend left]             node {} (E)
  (G) edge  [bend left]            node {} (H)
  (H) edge [bend left]             node {} (G)
  (G) edge   [bend right]           node {} (A);
\end{tikzpicture}
$$
The associated matrix $L$ is $$\begin{pmatrix} 0 & 0 & 0  & 0 & 0 & 0  & 1 &  0 \\ 1 & 0 & 1 & 1 & 0 & 0 & 0 & 0 \\ 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 1 & 1 & 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 & 1 & 0 & 0 & 1 \\ 0 & 0 & 0 & 0 & 1 & 0 & 0 &1 \\ 0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 \end{pmatrix}$$
\end{ex}

\begin{rem}
Note that most entries are $0$. In fact, I've heard, an average webpage links to 10 other webpages, so an average column of the $25$ billion by $25$ billion matrix representing the web will have only $10$ non-zero entries.
\end{rem}
What's this to do with importance? Well I think of any link pointing to page $i$ as a recommendation for it, and so the total number of recommendations that page $i$ gets is its row sum $\ell_{i1} + \cdots + \ell_{in}$. This is a first approximation to a measure of the importance of page $i$:
$$v_i = \ell_{i1} + \cdots + \ell_{in}$$
There is a concise mathematical way to write this $$\vec{v} = L \vec{u}$$ where $\vec{u}\in \mathbb{R}^n$ is the vector each of whose components is $1$. In the \hyperref[exgraph]{example above} there would be a ranking of importance: (B = E = F = H $>$ G $>$ A = C = D) with values $(3;2;1)$.

\begin{exercise}
The information of a directed graph $\Gamma$ as above can be recorded by a matrix: ${\Gamma}\mapsto L({\Gamma})$. Here $L({\Gamma})\in {\sf Mat}(n, \mathbb{R})$ where $n$ is the number of vertices of ${\Gamma}$ where $L({\Gamma})_{ij}$ is the number of edges from vertex $j$ to vertex $i$. Show that: the $(i,j)$ entry of $L({\Gamma})^k$ is the number of distinct paths of length $k$ going from vertex $j$ to vertex $i$ in the graph ${\Gamma}$.
\end{exercise}

\noindent
{\bf ``I don't want to belong to any club that will accept me as a member"} It's obvious that the above measure of importance is not very sophisticated. Suppose you are looking for a restaurant to go to: Person A gives 25 recommendations; Person B gives only 2. Which recommendations do you value more? Presumably, all other things being equal, one of the 2 selected by Person B.

To implement this into mathematics I should therefore not attach a value $1$ to each link from page $j$ to page $i$, but rather the number $1/t_j$ where $t_j = \sum_{k=1}^n \ell_{kj}$ is the total number of links from page $j$. This produces a new matrix $M\in {\sf Mat}(n; \mathbb{R})$ where $$m_{ij} =  \begin{cases} 1/t_j \quad & \text{if page $j$ has a link to page $i$} \\
 0 & \text{if not}. \end{cases}
$$

\begin{ex}{Example \hyperref[exgraph]{continued}} I continue with the \hyperref[exgraph]{example above}, but now I indicate on each edge the value I give to that edge, as determined by the number of outgoing links from its starting vertex.
$$
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                    thick]
  \tikzstyle{every state}=[fill=red,draw=none,text=white]

  \node[state] (A)                    {$A$};
  \node[state]         (B) [below of=A] {$B$};
   \node[state]         (C) [right of=A] {$C$};
   \node[state]         (D) [below of=C] {$D$};
   \node[state]         (E) [right of=C] {$E$};
   \node[state]         (F) [below of=E] {$F$};
   \node[state]         (G) [right of=E] {$G$};
  \node[state]         (H) [below of=G] {$H$};

  \path[every node/.style={font=\sffamily\footnotesize}]
  (A) edge              node [right] {$\frac{1}{2}$} (C)
   edge  node [right] {$\frac{1}{2}$} (B)
  (C) edge              node {$\frac{1}{2}$} (B)
  (B) edge [bend left]             node {$1$} (D)
  (D) edge  [bend left]            node {$\frac{1}{3}$} (B)
  (D) edge              node {$\frac{1}{3}$} (E)
  (C) edge              node {$\frac{1}{2}$} (E)
  (D) edge              node {$\frac{1}{3}$} (F)
  (E) edge              node {$\frac{1}{3}$} (F)
   (E) edge              node {$\frac{1}{3}$} (H)
   (E) edge node {$\frac{1}{3}$} (G)
  (F) edge  [bend left]            node {$1$} (H)
  (H) edge [bend left]             node {$\frac{1}{2}$} (F)
    (G) edge [bend left]             node {$\frac{1}{3}$} (E)
  (G) edge  [bend left]            node {$\frac{1}{3}$} (H)
  (H) edge [bend left]             node [right] {$\frac{1}{2}$} (G)
  (G) edge   [bend right]           node {$\frac{1}{3}$} (A);
\end{tikzpicture}
$$
So now the matrix $M$ for this example is $$\begin{pmatrix} 0 & 0 & 0 & 0 & 0 & 0 & 1/3 & 0 \\ 1/2 & 0 & 1/2& 1/3 & 0 & 0 & 0 & 0 \\ 1/2 & 0 & 0 & 0 & 0 & 0 & 0& 0 \\ 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 1/2 & 1/3 & 0 & 0 & 1/3 & 0  \\ 0 & 0 & 0 & 1/3 & 1/3 & 0 & 0 & 1/2 \\ 0 & 0 & 0 & 0 & 1/3 & 0 & 0 & 1/2 \\ 0 & 0 & 0 & 0 & 1/3 & 1 & 1/3 & 0  \end{pmatrix}
$$
\end{ex}
There is a problem with this definition to ``weight" the edges of the graph: what if a page has no outgoing links at all? This is common enough -- think for instance of PDFs online -- and such a page is called a {\bf dangling node}. I'll get round this by pretending then that such a page is linked to every  node and then calculate the matrix $M$ as suggested.

\begin{ex} In this example vertex $4$ is a dangling node:
$$
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=3cm,
                    thick]
  \tikzstyle{every state}=[fill=red,draw=none,text=white]

  \node[state] (A)                    {$4$};
  \node[state]         (B) [above left of=A] {$1$};
  \node[state] (D) [right of=B]  {$2$};
   \node[state]         (C) [right of=D] {$3$};

  \path[every node/.style={font=\sffamily\footnotesize}]
  (B) edge              node {} (A)
  (D) edge          node {} (B)
  (D) edge          node {} (C)
  (C) edge              node {} (A);
\end{tikzpicture}
$$
The new matrix $M$ associated to this would be $$\begin{pmatrix}0 & 1/2 & 0 & 1/4 \\ 0 & 0 & 0 & 1/4 \\ 0 & 1/2 & 0 & 1/4 \\ 1 & 0 & 1 & 1/4  \end{pmatrix}$$
\end{ex}
So importance is now measured by $$v_i = \frac{\ell_{i1}}{t_1} + \cdots + \frac{\ell_{in}}{t_n}$$ Written in concise mathematical form this states $$\vec{v} = M \vec{u}$$ where $\vec{u}$ is still the vector of ones. Now in \hyperref[exgraph]{Example \ref{exgraph}} the ranking is more refined than before, becoming: (H $>$ B $>$  E = F $>$  D $>$ G $>$ C$>$ A) with values $(5/3; 4/3; 7/6; 1; 5/6; 1/2; 1/3)$.
\medskip

\noindent
{\bf ``Oh, you're a member. I didn't know. Well, maybe I'll join"} When I discussed Person A and Person B giving restaurant recommendations, I wrote ``all other things being equal". Well, of course they're not. What if Person A was a food critic, and like me Person B grew up on a strict Scottish diet of potatoes? Wouldn't you want to take Person A's recommendation a little more seriously than before? In other words, what should happen is that each recommendation made should reflect the importance of the recommender. But since $v_j$ is supposed to be the importance of page $j$, this would mean that I should multiply the previous value $\ell_{ij}/t_j$ of a recommendation from page $j$ to page $i$ by $v_j$. This sounds circular to me, at least until I write out its mathematical formulation: $$v_i  = \frac{\ell_{i1}}{t_1} v_1 + \cdots + \frac{\ell_{in}}{t_n}v_n$$
% $$
% \includegraphics[scale=0.5]{eureka.pdf}
% $$
Written in concise mathematical form this states $${\vec v} = M {\vec v}$$ In other words, the vector that measures importance should be an eigenvector of $M$ with eigenvalue $1$.

It's just like I always said: mathematics rules the world! Well actually, Google rules the world {\it and} mathematics rules Google; but sadly ``rules the world" is not a transitive relation.

In the two running examples, there are unique (up to scalar) eigenvectors with eigenvalue $1$: $$\frac{1}{400}(24, 27, 12, 27, 39, 81, 72, 118)^{\sf T} \text{ and } \frac{1}{16}(3, 2, 3, 8)^{\sf T}$$ So in \hyperref[exgraph]{Example \ref{exgraph}} the order of importance is (H$>$F$>$G$>$ E$>$ B = D $>$A $>$ C).
\medskip

\phantomsection
\label{prospective}
As prospective mathematicians you will automatically ask yourself the following questions:
\begin{itemize}
\item Does $M$ always have an eigenvalue $1$?
\item How many eigenvectors of $M$ with eigenvalue $1$ are there? In other words, what is $\dim \{ \vec{v}: M \vec{v } = \vec{v}\}$?
\item Is it true that there is always an eigenvector of $M$ whose entries are non-negative; even positive? \item How can I calculate such an eigenvector given that I may want to work with square matrices of size $25\times 10^9$?
\end{itemize}

Now, I need to do mathematics. This is because most of the answers to the above questions are not the answers I'd want them to be. But at least the first question has a positive answer, provided you notice something staring you in the face.

\begin{definition} A matrix $M$ whose entries are non-negative and such that the sum of the entries of each column equals $1$ is a {\bf Markov matrix} or a {\bf stochastic matrix}.
\end{definition}

\begin{lemma} Suppose that $M\in {\sf Mat}(n; \mathbb{R})$ is a Markov matrix. Then $\lambda = 1$ is an eigenvalue of $M$. \end{lemma}
\begin{proof}
The sum of the entries of each column of $M - I_n$ is $0$. Therefore if I add all the row vectors of the matrix together I must get the zero vector. This means that there is a linear dependence amongst the rows, which in turn means that ${\sf det}(M - I_n) = 0$ so that $\chi_M(1) = 0$, as required.
\end{proof}



\begin{ex}\label{exgraph3}
I take the following variation on \hyperref[exgraph]{Example \ref{exgraph}}, the only difference being that I remove the arrow from $G$ to $A$:
$$
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,
                    semithick]
  \tikzstyle{every state}=[fill=red,draw=none,text=white]

  \node[state] (A)                    {$A$};
  \node[state]         (B) [below of=A] {$B$};
   \node[state]         (C) [right of=A] {$C$};
   \node[state]         (D) [below of=C] {$D$};
   \node[state]         (E) [right of=C] {$E$};
   \node[state]         (F) [below of=E] {$F$};
   \node[state]         (G) [right of=E] {$G$};
  \node[state]         (H) [below of=G] {$H$};

  \path (A) edge              node {} (C)
  (A) edge              node {} (B)
  (C) edge              node {} (B)
  (B) edge [bend left]             node {} (D)
  (D) edge  [bend left]            node {} (B)
  (D) edge              node {} (E)
  (C) edge              node {} (E)
  (D) edge              node {} (F)
  (E) edge              node {} (F)
   (E) edge              node {} (H)
   (E) edge node {} (G)
  (F) edge  [bend left]            node {} (H)
  (H) edge [bend left]             node {} (F)
    (G) edge [bend left]             node {} (E)
  (G) edge  [bend left]            node {} (H)
  (H) edge [bend left]             node {} (G);
\end{tikzpicture}
$$
Its associated Markov matrix is:
$$\begin{pmatrix} 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 1/2 & 0 & 1/2& 1/3 & 0 & 0 & 0 & 0 \\ 1/2 & 0 & 0 & 0 & 0 & 0 & 0& 0 \\ 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 1/2 & 1/3 & 0 & 0 & 1/2 & 0  \\ 0 & 0 & 0 & 1/3 & 1/3 & 0 & 0 & 1/2 \\ 0 & 0 & 0 & 0 & 1/3 & 0 & 0 & 1/2 \\ 0 & 0 & 0 & 0 & 1/3 & 1 & 1/2 & 0  \end{pmatrix}
$$
It still has a unique (up to scalar) eigenvector with eigenvalue $1$, which is $\frac{1}{25}(0,0,0,0,3,6,6,10)^{\sf T}$. This is, of course, not what I want because it says that A, B, C and $D$ have no importance whatsoever.
\end{ex}
To understand what has happened in the above example, think of the Markov matrix $M$ as describing a random walk over the web in which I move from page to page as follows. On a given page, I choose at random one of the links from that page and then follow that link. For example, if there are six links, I roll a die to choose which link to follow. If I reach a dead end, meaning there are no links out, I choose a page at random from the entire web and move to it. Now imagine lots and lots of web surfers moving around the web in this way: $m_{ij}$ gives the expected fraction of those surfers on page $j$ who will then move to page $i$. A solution $\vec{v}$ to the equation $\vec{v} = M\vec{v}$ can be then regarded as describing an equilibrium or steady state distribution of surfers on the various pages.

Now what would you expect a steady state of surfers to look like in the above example? Note there are no dangling nodes, so there's no trouble with ending up on a page with no links out. Surf randomly, for some time! You expect you'll get to one of E, F, G or H at some point. But once you're on one of those pages you'll never get back to A, B, C  or D. So the steady state has to expect that all surfers are on pages E, F, G or H and nowhere else. That's what the eigenvector demonstrates! The four zeros at the start appear because once I reach E, F, G or H I can never get back to A, B, C or D; and I have to reach E, F, G or H sometime if I'm moving around randomly.

\begin{ex}\label{exgraph4}
I now take the following more brutal variation on \hyperref[exgraph]{Example \ref{exgraph}}, splitting the graph into two separate pieces, but reversing the arrow between A and B:
$$
\begin{tikzpicture}[->,>=stealth',shorten >=1pt,auto,node distance=2.8cm,
                    semithick]
  \tikzstyle{every state}=[fill=red,draw=none,text=white]

  \node[state] (A)                    {$A$};
  \node[state]         (B) [below of=A] {$B$};
   \node[state]         (C) [right of=A] {$C$};
   \node[state]         (D) [below of=C] {$D$};
   \node[state]         (E) [right of=C] {$E$};
   \node[state]         (F) [below of=E] {$F$};
   \node[state]         (G) [right of=E] {$G$};
  \node[state]         (H) [below of=G] {$H$};

  \path (A) edge              node {} (C)
  (B) edge              node {} (A)
  (C) edge              node {} (B)
  (B) edge [bend left]             node {} (D)
  (D) edge  [bend left]            node {} (B)
  (E) edge              node {} (F)
   (E) edge              node {} (H)
   (E) edge node {} (G)
  (F) edge  [bend left]            node {} (H)
  (H) edge [bend left]             node {} (F)
    (G) edge [bend left]             node {} (E)
  (G) edge  [bend left]            node {} (H)
  (H) edge [bend left]             node {} (G);
\end{tikzpicture}
$$
Its associated Markov matrix is:
$$\begin{pmatrix} 0 & 1/2 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 1& 1 & 0 & 0 & 0 & 0 \\ 1 & 0 & 0 & 0 & 0 & 0 & 0& 0 \\ 0 & 1/2 & 0 & 0 & 0 & 0 & 0 & 0 \\ 0 & 0 & 0 & 0 & 0 & 0 & 1/2 & 0  \\ 0 & 0 & 0 & 0 & 1/3 & 0 & 0 & 1/2 \\ 0 & 0 & 0 & 0 & 1/3 & 0 & 0 & 1/2 \\ 0 & 0 & 0 & 0 & 1/3 & 1 & 1/2 & 0  \end{pmatrix}
$$
This now has a two-dimensional eigenspace $\{ \vec{v} : M \vec{v} = \vec{v} \}$ for eigenvalue $1$ with basis $$\frac{1}{25}(0,0,0,0,3,6,6,10)^{\sf T} \text{ and } \frac{1}{5}(1,2,1,1,0,0,0,0)^{\sf T}$$
\end{ex}
Obviously, a similar but different problem has arisen: this time there are parts of the web that are just not talking to each other and so I can't make a comparison between A, B, C, D and E, F, G, H.
\medskip

{\bf Very smart idea} Going back to thinking of $M$ as a random walk over the web, one way to solve the problem that some parts of the web might get forgotten completely, as in \hyperref[exgraph3]{Example \ref{exgraph3}}, or that some parts do not communicate with each other at all, as in \hyperref[exgraph4]{Example \ref{exgraph4}}, Sergey Brin and Larry Page -- the founders of Google -- introduced the key idea of teleportation. To understand it, imagine that you have a biased coin so that the probability of heads is $\alpha$ with $0\leqslant \alpha \leqslant 1$. Typically $\alpha$ is $1/2$, but that's not what I want here. If I'm on page $j$ I toss the coin. If page $j$ is a dangling node or if the coin comes up tails, then I pick a page at random from the whole web and teleport to it; otherwise I do what I always did, follow a link at random. In mathematics, I replace the Markov matrix $M$ with the new matrix $${\sf Google}(\alpha) = \alpha M + (1-\alpha)T$$ where $T$ is the teleportation matrix each of whose entries is $\frac{1}{n}$. Notice that whatever I choose for $\alpha$, ${\sf Google}(\alpha)$ is still a Markov matrix, so it will still have $1$ as an eigenvalue.  If I chose $\alpha = 1$ then ${\sf Google}(\alpha) = M$, so nothing new. If I chose $\alpha = 0$ then ${\sf Google}(\alpha) = T$ and so I'd forget all about the hyperlink structure and be moving around the web at random. The obviously good thing about choosing $\alpha <1$ is that the matrix ${\sf Google}(\alpha)$ has all entries positive.

\begin{ex}
In \hyperref[exgraph3]{Example \ref{exgraph3}} an example of the new Markov matrix with $\alpha = 17/20$  is: $$ \begin{pmatrix}  3/160 & 3/160 & 3/160 & 3/160 & 3/160 & 3/160 & 3/160 & 3/160 \\ 71/160 & 3/160 & 71/160 & 29/96 & 3/160 & 3/160 & 3/160 & 3/160 \\ 71/160 & 3/160 & 3/160 & 3/160 & 3/160 & 3/160 & 3/160& 3/160 \\ 3/160 & 139/160  & 3/160 & 3/160 & 3/160 & 3/160 & 3/160 & 3/160 \\ 3/160 & 3/160 & 71/160 & 29/96 & 3/160 & 3/160 & 71/160 & 3/160  \\ 3/160 & 3/160 & 3/160 & 29/96 & 29/96 & 3/160 & 3/160 & 71/160 \\ 3/160 & 3/160 & 3/160 & 3/160 & 29/96 & 3/160 & 3/160 & 71/160 \\ 3/160 & 3/160 & 3/160 & 3/160 & 29/96 & 139/160 & 71/160 & 3/160 \end{pmatrix}$$ It looks quite complicated, but it has a unique (up to scaling) eigenvector with eigenvalue $1$ which is approximately:
$$
(0.019, 0.057, 0.027, 0.067, 0.128, 0.206, 0.187,  0.309)^{\sf T}
$$
This orders the vertices H $>$ F $>$ G $>$ E $>$ D $>$ B $>$ C $>$ A.
\end{ex}

\begin{theorem}[Perron, 1907] \label{perron}
If $M\in {\sf Mat}(n; \mathbb{R})$ is a Markov matrix all of whose entries are positive, then the eigenspace $E(1, M)$ is one dimensional.  There exists a unique basis vector $\vec{v} \in E(1, M)$ all of whose entries are positive real numbers, $v_i >0$ for all $i$, and such that the sum of its entries is $1$, $\sum_{i=1}^n v_i = 1$.
\end{theorem}
I've presented here a cannibalisation of the {\bf Perron-Frobenius Theorem} which has a weaker hypothesis on the initial matrix $N$ and which deduces more consequences than those I present here. [If you look closely at the proof, you'll see that I already prove a little bit more than the statement of the Theorem: what extra do I do?] The proof of the full Perron-Frobenius theorem is not really harder than what follows, but it does involve more definitions. The Theorem is important in all sorts of places: graph theory, statistical mechanics, commodity pricing, power control in wireless networks, population growth (think haggii). Really anywhere that iterative processes involving positive matrices appear.

\begin{proof} I will split the proof into several steps. But before doing so I need to introduce notation for $\vec{v}, \vec{w} \in \mathbb{R}^n$:
\begin{eqnarray*} \vec{v}> \vec{w} &\Leftrightarrow & v_i > w_i \text{ for all }1\leqslant i \leqslant n \\
 \vec{v}\geqslant \vec{w} &\Leftrightarrow & v_i \geqslant w_i \text{ for all }1\leqslant i \leqslant n \end{eqnarray*}
Given $\vec{v}\in \mathbb{R}^n$, I will write $|\vec{v}|$ for $\sum_{i=1}^n v_i$.

\begin{proof}[Step 1] {\it I claim that if $\vec{v} \geqslant \vec{w}$ and $\vec{v}\neq \vec{w}$ then $M\vec{v} > M\vec{w}$. }
By definition of matrix multiplication, the $i$-th entry of $M\vec{v} - M\vec{w}$ is $\sum_{j=1}^n M_{ij} (v_j - w_j)$. Each term in the sum is a product of $M_{ij}>0$ and $v_j-w_j \geqslant 0$ and so non-negative. At least one of the terms is positive since by hypothesis there is some $j$ such that $v_j-w_j >0$. It follows that  $\sum_{j=1}^n M_{ij} (v_j - w_j)>0$ for each $i$. Therefore $M\vec{v} > M\vec{w}$.
\end{proof}

\begin{proof}[Step 2] {\it There exists an eigenvector $\vec{v} \geqslant 0$ for $M$.}
For $\vec{x} \geqslant 0$ and $\vec{x} \neq 0$ let $$R(\vec{x}) = {\rm max}\{ c\in \mathbb{R} : M \vec{x} \geqslant c \vec{x} \}= \min_{1\leqslant i \leqslant n, x_i \neq 0} \frac{(M\vec{x})_i}{x_i}$$

If $ M\vec{x} \geqslant c \vec{x} $ then by Step 1 $M(M\vec{x}) \geqslant cM\vec{x}$ with equality if and only if $M\vec{x} = c \vec{x}$. This means that \begin{equation} \label{equsein} R(M\vec{x}) \geqslant R(\vec{x}) \end{equation} and that the inequality is strict unless $M\vec{x} = c\vec{x}$.

Let $$C = \{ \vec{x} : \vec{x} \geqslant 0 \text{ and } |\vec{x}| = 1 \}$$ This is a  {\bf compact set} as you will see using the \href{http://en.wikipedia.org/wiki/Heine-Borel_theorem}{Heine--Borel Theorem} from \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath10068.htm}{Honours Analysis}. You will also see in \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath10068.htm}{Honours Analysis} that a continuous real-valued function on a compact set always achieves a maximum, a higher dimensional version of the Extreme Value Theorem from \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath08064.htm}{Fundamentals of Pure Mathematics}. Thus, since $R$ is a continuous function on $C$, it achieves a maximum on $C$. This means there exists $\vec{v} \in C$ such that $$R(\vec{v}) \geqslant R(\vec{x}) \text{ for all }\vec{x} \in C$$

By definition $R( \alpha \vec{x} ) = R(\vec{x})$ for any $\alpha > 0$ and any non-zero $\vec{x}\geqslant \vec{0}$. Then, as $|M\vec{x}| > 0$ by Step 1, I see $\frac{M\vec{x}}{|M\vec{x}|} \in C$. So in particular I deduce $$R(M\vec{v}) = R\left(\frac{M\vec{v}}{|M\vec{v}|}\right) \leqslant R(\vec{v}).$$ Combining this with equation \eqref{equsein} shows that $M\vec{v} = c \vec{v}$ for some $c \in \mathbb{R}$, as required.
\end{proof}

\begin{proof}[Step 3] {\it I claim that if $\vec{v}\geqslant \vec{0}$ is an eigenvector of $M$ then its eigenvalue must be $1$. } Let $\vec{u}^{\sf T} = (1, 1, \ldots , 1)$ be the row vector of length $n$ all of whose entries are $1$. Since $M$ is a Markov matrix the sum of the entries of each column is $1$, and so $\vec{u}^{\sf T} M = \vec{u}^{\sf T}$. By assumption $M \vec{v} = \lambda \vec{v}$ for some $\lambda$ and so $$\lambda \vec{u}^{\sf T}\vec{v} = \vec{u}^{\sf T}(M\vec{v}) = (\vec{u}^{\sf T}M)\vec{v} = \vec{u}^{\sf T}\vec{v}$$ Since $\vec{u}^{\sf T}\vec{v} = \sum_{i=1}^n u_iv_i = \sum_{i=1}^n v_i > 0 $ it follows that $\lambda =1$.
\end{proof}

\begin{proof}[Step 4]{\it If $\vec{v} \geqslant \vec{0}$ is an eigenvalue of $M$, then $\vec{v} > \vec{0}$.}
This is a consequence of Steps 1 and 3 because $\vec{v} = M\vec{v} > M\vec{0} = \vec{0}.$
\end{proof}

\begin{proof}[Step 5] {\it If $\vec{v} \geqslant \vec{0}$ is an eigenvector of $M$, then $E(1, M) = \langle \vec{v} \rangle.$ In other words the eigenspace of eigenvalue $1$ is one dimensional.}
By Step 3 $\vec{v}\in E(1, M)$. Suppose that $\vec{x} \in E(1,M)$ is non-zero. By multiplying by $-1$ if necessary, I can assume that there exists at least one positive entry $x_j$ in $\vec{x}$. Obviously $\vec{v} + \alpha \vec{x} \in E(1,M)$ for all $\alpha \in \mathbb{R}$. If I set $\alpha = {\rm min} \{ -v_i / x_i : x_i >0 \}$ then $\vec{v} + \alpha \vec{x} \geqslant 0$. But then at least one entry of $\vec{v} + \alpha \vec{x}$ is zero: this contradicts Step 4 unless $\vec{v} + \alpha \vec{x} = \vec{0}$. Hence $\vec{x}$ is a scalar multiple of $\vec{v}$, as required.
\end{proof}

This completes the proof of the theorem. Steps 2 and 4 shows the existence of the positive eigenvalue, and Step 5 shows that it is a basis for $E(1,M)$.
\end{proof}

This now answers the first three questions you asked as \hyperref[prospective]{prospective mathematicians}, at least for the matrix ${\sf Google}(\alpha)$ with $0\leqslant \alpha <1$.

The last question is a little more practical in nature and the sort of question that is important in \href{http://www.drps.ed.ac.uk/11-12/dpt/cxmath10059.htm}{Numerical Linear Algebra and Applications}. The basic point is that if $\vec{w} \geqslant \vec{0}$ with $|\vec{w}| =1$ then $$\lim_{k\to \infty}M^k \vec{w} = \vec{v}$$ So calculating powers of the Markov matrix ${\sf Google}(\alpha)$ cleverly -- remembering it's origin in the sparse matrix $M$ that represented hyperlinks between webpages -- allows one to approximate the eigenvector $\vec{v}$ that is required for PageRank. The smaller the number $\alpha$, the faster the convergence of the above limit and so the fewer multiplications that have to be made to find a good approximation of $\vec{v}$. But remember, choosing $\alpha$ closer to $1$ gives a better page ranking. Google, I've read, uses $\alpha = 17/20$.


\chapter{Inner Product Spaces}
Inner product spaces are a halfway-house between the Euclidean geometry of $\mathbb{R}^2$ and $\mathbb{R}^3$ and the mathematics of Hilbert spaces, objects that are particularly important in analysis and in quantum mechanics. As you saw in the proof of the \hyperref[perron]{Perron Theorem} special things happen in linear algebra when the general is replaced by the specific: there it was combining linear algebra and the ordering on real numbers, that is the notion of $x>y$. Inner product spaces are similar in that this is no longer the study of general vector spaces, but ones with just enough extra structure for the notion of angle to make sense: inner product spaces are a natural extension of Euclidean geometry. In fact, this fits very well with a huge theme of twentieth century mathematics in which usual spatial ``geometry" is generalised: one approach comes from \href{http://en.wikipedia.org/wiki/Erlangen_program}{Klein's ``Erlangen Program"} which understands geometry through group theory; another is via \href{http://en.wikipedia.org/wiki/Riemannian_geometry}{Riemannian Geometry} which is, for instance, what Einstein required for General Relativity. Inner product spaces are tied up with Erlangen program via the associated group of orthogonal symmetries. What is incredible however, at least to me, is that inner product spaces are useful far beyond this, especially when they are infinite dimensional. In that case, if they satisfy an additional hypothesis called completeness, they are \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath10046.htm}{Hilbert Spaces}, an incredibly interesting topic that spans algebra, analysis and is half of the language of quantum theory (the other half being groups and their representations). You really do want to study it next year. But before you do, it helps to have gripped the finite dimensional case first. So that means you really do want absorb what is about to happen!

\section{Inner Product Spaces: Definitions}
\begin{definition} Let $V$ be a vector space over $\mathbb{R}$. An {\bf inner product} on $V$ is a mapping $$(- , -) : V\times V \to \mathbb{R}$$ that satisfies the following for all $\vec{x}, \vec{y}, \vec{z} \in V$ and $\lambda, \mu \in \mathbb{R}$:
\begin{enumerate}
\item $
(\lambda \vec{x} + \mu \vec{y}, \vec{z}) = \lambda (\vec{x}, \vec{z}) + \mu (\vec{y}, \vec{z}) $
\item $
(\vec{x} , \vec{y} ) = (\vec{y}, \vec{x}) $
\item $
(\vec{x}, \vec{x}) \geqslant 0, \text{ with equality if and only if } \vec{x} = \vec{0}$
\end{enumerate}
A {\bf real inner product space} is a real vector space endowed with an inner product.
\end{definition}

\begin{ex} \label{standardreal}
I can endow $V = \mathbb{R}^n$ with the {\bf standard inner product} $$(\vec{v}, \vec{w}) = v_1w_1 + v_2w_2 + \cdots + v_nw_n$$ You have already seen this, usually written as $\vec{v} \cdot \vec{w}$ and called the dot product. If I was so inclined, I could also write it as via matrix multiplication $(\vec{v}, \vec{w}) = \vec{v}^T \circ \vec{w}$, where I implicitly use the obvious identification ${\sf Mat}(1; \mathbb{R}) = \mathbb{R}$.
\end{ex}

Recall that in \hyperref[bilindef]{Definition \ref{bilindef}} I introduced the notion of a symmetric bilinear form. You will see that Conditions (1) and (2) on the inner product are precisely the same as asserting that $(-,-) : V\times V \to \mathbb{R}$ is a symmetric bilinear form. Linearity in the second entry of $(-,-)$ follows from linearity in the first entry by using the symmetry condition to swap ``first" and ``second". Condition (3) says that the symmetric bilinear form is {\bf positive definite}.

\begin{exercise} \label{intreal} Confirm the following claims:
\begin{enumerate}
\item On $\mathbb{R}^2$ define $(\vec{x}, \vec{y}) = x_1 y_1 + 4x_2y_2$ where $\vec{x} = (x_1, x_2)^{\sf T}$ and $\vec{y} = (y_1, y_2)^{\sf T}$. This is an inner product.
\item On $\mathbb{R}^2$ define $(\vec{x}, \vec{y}) = 2x_1 y_1 + x_1y_2 + x_2y_1 + x_2y_2$ where $\vec{x} = (x_1, x_2)^{\sf T}$ and $\vec{y} = (y_1, y_2)^{\sf T}$. This is an inner product.
\item On $\mathbb{R}^2$ define $(\vec{x}, \vec{y}) = x_1 y_1 + 2x_1y_2 + 2x_2y_1 + x_2y_2$ where $\vec{x} = (x_1, x_2)^{\sf T}$ and $\vec{y} = (y_1, y_2)^{\sf T}$. This is not an inner product.
\item Fix real numbers $a<b$. For $P, Q \in \mathbb{R}[X]_{< n}$ define $$(P,Q) = \int_a^b P(X)Q(X) dX$$ This is an inner product.
\end{enumerate}
\end{exercise}

It is easy to extend the definition of an inner product space from real vector spaces to complex vector spaces. Unfortunately, I don't have a really convincing intuitive picture for it, unlike geometry for real spaces. The only justification I can give is a posteriori. It turns out that the combination of the algebraic closure of the complex numbers with the positivity of an inner product ends up proving powerful theorems, and theorems which even have consequences for real vector spaces.

\begin{definition}
Let $V$ be a vector space over $\mathbb{C}$. An {\bf inner product} on $V$ is a mapping $$(- , -) : V\times V \to \mathbb{C}$$ that satisfies the following for all $\vec{x}, \vec{y}, \vec{z} \in V$ and $\lambda, \mu \in \mathbb{C}$:
\begin{enumerate}
\item $
(\lambda \vec{x} + \mu \vec{y}, \vec{z}) = \lambda (\vec{x}, \vec{z}) + \mu (\vec{y}, \vec{z}) $
\item $
(\vec{x} , \vec{y} ) = \overline{(\vec{y}, \vec{x})} $
\item $
(\vec{x}, \vec{x}) \geqslant 0, \text{ with equality if and only if } \vec{x} = \vec{0}$
\end{enumerate}
Here $\overline{z}$ denotes the complex conjugate of $z$. A {\bf complex inner product space} is a complex vector space endowed with an inner product.
\end{definition}

\begin{ex} \label{standardcomp}
I can endow $V = \mathbb{C}^n$ with the {\bf standard inner product} $$(\vec{v}, \vec{w}) = v_1\overline{w_1} + v_2\overline{w_2} + \cdots + v_n\overline{w_n}$$ If I was so inclined, I could also write it as via matrix multiplication $(\vec{v}, \vec{w}) = \vec{v}^T \circ \overline{\vec{w}}$, where $\overline{\vec{w}} = (\overline{w_1}, \ldots , \overline{w_n})^{\sf T}$ and where I implicitly use the obvious identification ${\sf Mat}(1; \mathbb{C}) = \mathbb{C}$.
\end{ex}

The form $(-, -)$ in a real inner product space is necessarily a symmetric bilinear form. For a complex inner product space this is false: $$(\vec{z}, \lambda \vec{x} + \mu \vec{y}) = \overline{(\lambda \vec{x} + \mu \vec{y}, \vec{z})} =   \overline{\lambda (\vec{x}, \vec{z}) + \mu (\vec{y}, \vec{z})} =  \overline{\lambda} (\vec{z}, \vec{x}) + \overline{\mu} (\vec{z}, \vec{y})$$ In other words, it is not $\mathbb{C}$-linear in the second variable.  I will call a mapping $f:V \to W$ between complex vector spaces {\bf skew-linear} if $f(\vec{v}_1+\vec{v}_2) = f(\vec{v}_1) + f(\vec{v}_2)$ and $f(\lambda\vec{v}_1) = \overline{\lambda}f(\vec{v}_1)$ for all $\vec{v}_1, \vec{v}_2 \in V$ and all $\lambda \in \mathbb{C}$. So a complex inner product is skew-linear in its second variable. Such forms are called {\bf sesquilinear} \footnote{Sesqui is Latin for ``one-and-a-half": for instance this year is the sesquicentennial of the birth of the Canadian mathematician \href{http://en.wikipedia.org/wiki/John_Charles_Fields}{J.C.Fields}, after whom the \href{http://en.wikipedia.org/wiki/Fields_Medal}{Fields Medal} is named.}. When a sesquilinear form satisfies (2) it is {\bf hermitian}, named after the French mathematician Hermite. Condition (3) says that the symmetric bilinear form is {\bf positive definite}.

\begin{exercise} \label{intcom} Confirm the following claims:
\begin{enumerate}
\item On $\mathbb{C}^2$ define $(\vec{z}, \vec{w}) = z_1 \overline{w_1} + 4z_2\overline{w_2}$ where $\vec{z} = (z_1, z_2)^{\sf T}$ and $\vec{w} = (w_1, w_2)^{\sf T}$. This is an inner product.
\item Let $V = C_{\mathbb{C}}[a,b]$ be the vector space of all continuous complex valued functions defined on $[a,b]$ where $a<b$ are real. (In case you don't know, continuity for complex valued functions is defined as the natural extension of the definition of the real valued functions, using the modulus in $\mathbb{C}$ to measure ``nearness".) For $f, g\in V$ define $$(f,g) = \int_a^b f(t)\overline{g(t)} dt$$ (For a complex valued function $\phi$ on an interval $[a,b]$, the integral $\int_a^b \phi(t)dt$ is defined to be $\int_a^b \mathfrak{R}e (\phi(t)) dt + \sqrt{-1} \int_a^b \mathfrak{I}m (\phi(t))dt.$) This is an inner product.
\end{enumerate}
\end{exercise}

For future orientation, I give you some of the names that are associated to what I have just defined. A finite dimensional real inner product space is a {\bf Euclidean vector space} or, with belts-and-braces, a {\bf real Euclidean vector space}. A complex inner product space is a {\bf unitary space} or, because of its connection to Hilbert spaces, a {\bf pre-Hilbert space}. A finite dimensional complex inner product space is a {\bf finite dimensional Hilbert space}.


\begin{definition}
In a real or complex inner product space the {\bf length} or {\bf inner product norm} or {\bf norm} $\|\vec{v}\| \in \mathbb{R}$ of a vector $\vec{v}$ is defined as the non-negative square root $$\|\vec{v}\| = \sqrt{(\vec{v}, \vec{v})}$$ Vectors whose length is $1$ are called {\bf units}. Two vectors $\vec{v}, \vec{w}$ are {\bf orthogonal} and I write $$\vec{v} \perp \vec{w}$$ if and only if $( \vec{v}, \vec{w}) = 0$. I'll also say that $\vec{v}$ and $\vec{w}$ are at right-angles to each other. Sometimes I will use the symbol $\perp$ for general subsets $S,T$ of an inner product space and write $S\perp T$ as a shorthand for $\vec{v} \perp \vec{w}$ for all $\vec{v}\in S$ and $\vec{w}\in T$.
\end{definition}

\begin{exercise} \label{exlength} In an inner product space $V$ show that: $\| \lambda \vec{v} \| = |\lambda | \|\vec{v}\|$ for all $\vec{v}\in V$ and all $\lambda \in \mathbb{R}$ or $\mathbb{C}$, as relevant depending on whether it is a real or complex inner product space.
\end{exercise}

\begin{ex} \label{herecomethegreeks} If two vectors $\vec{v}$ and $\vec{w}$ in an inner product space are at right-angles then Pythagoras' Theorem holds $$\| \vec{v} + \vec{w} \|^2 = \|\vec{v}\|^2 + \|\vec{w}\|^2$$ To see this, follow your linear nose: $$( \vec{v} + \vec{w}, \vec{v} + \vec{w} ) = (\vec{v}, \vec{v}) + (\vec{v}, \vec{w}) + (\vec{w}, \vec{v}) + (\vec{w}, \vec{w}) = (\vec{v}, \vec{v}) + (\vec{w}, \vec{w})$$ See! Just like ordinary Euclidean geometry.
\end{ex}

\begin{definition} A family $(\vec{v}_i)_{i\in I}$ for vectors from an inner product space is an {\bf orthonormal family} if all the vectors $\vec{v}_i$ have length $1$ and if they are pairwise orthogonal to each other, which, using the Kronecker delta symbol defined in \hyperref[kronecker]{Example \ref{kronecker}}, means $$( \vec{v}_i, \vec{v}_j ) = \delta_{ij}$$ An orthonormal family that is a basis is an {\bf orthonormal basis}.
\end{definition}

\begin{ex}
When contemplating SPACE AROUND US I often imagine that the coordinate axes are there. After all the energy I exerted in \hyperref[chap2]{Chapter Two} explaining that it's really not good to have a specific basis in mind all the time, I know that I shouldn't do this, but I can't help it! There they are. One of their most obvious properties is that they are at right-angles. So at least I did something sophisticated: the standard basis $(\vec{e}_1, \vec{e}_2, \vec{e}_3)$ in SPACE AROUND US is an orthonormal basis for the standard inner product defined in \hyperref[standardreal]{Example \ref{standardreal}}.
\end{ex}

\begin{rem}\label{remdec}
There is a very useful, very easy observation to make about orthonormal bases. Suppose that $V$ is an inner product space and that $( \vec{v}_i )_{i\in I}$ is an orthonormal basis. Then I can write any $\vec{w}\in V$ in the form \begin{equation} \label{illuseitnow} \vec{w} = \sum_{i\in I} \lambda_i \vec{v}_i\end{equation} The observation is that it is easy to calculate the $\lambda_i$. As soon as I apply the inner product with $\vec{v}_i$ to the left and right sides of \eqref{illuseitnow} I get that $( \vec{w}, \vec{v}_i) = \lambda_i$ because of the orthonormality of $( \vec{v}_i )_{i\in I}$. In other words \begin{equation} \label{illuseitlater} \vec{w} = \sum_{i\in I} (\vec{w}, \vec{v}_i) \vec{v}_i\end{equation}
\end{rem}

\begin{theorem} \label{orthbasis} Every finite dimensional inner product space has an orthonormal basis.
\end{theorem}

\begin{proof} I'll call the inner product space $V$. It may be a real or a complex vector space, so I will let $F$ denote the relevant field. I will prove the theorem by induction on $\dim_F(V)$.

The theorem is trivial when $\dim (V) = 0$, beginning the induction. Assume that $\dim (V) = n > 0$, so that there exists a non-zero vector $\vec{v} \in V$. Rescaling $$\vec{v}_1 := \frac{1}{\|\vec{v}\|}\vec{v}$$ produces a unit vector by \hyperref[exlength]{Exercise \ref{exlength}}. The linear mapping $$( - , \vec{v}_1) : V\to \RR, \quad \vec{w} \mapsto (\vec{w}, \vec{v}_1) $$ is not zero since it sends $\vec{v}_1$ to $1$. Therefore, by the \hyperref[rnthm]{Rank-Nullity Theorem}, its kernel $U$ has dimension $n-1$. The axioms for an inner product hold for all elements of $U$ since $U$ is a subspace of $V$, and so $U$ is also an inner product space. Therefore by induction $U$ has an orthonormal basis $(\vec{v}_i)_{i=2}^n$. Since $(\vec{u}, \vec{v}_1) = 0$ for any $\vec{u}\in U$ it follows that $(\vec{v}_i)_{i=1}^n$ is an orthonormal basis for $V$.
\end{proof}

\section{Orthogonal Complements and Orthogonal Projections}
The proof of \hyperref[orthbasis]{Theorem \ref{orthbasis}} above illustrates an important technique in inner product spaces. In that proof we took a vector $\vec{v}_1$ and then created a complementary subspace $U$ to $\langle \vec{v}_1 \rangle$ all of whose vectors were at right-angles to $\vec{v}$. This is an example of an orthogonal complement.

\begin{definition} Let $V$ be an inner product space and let $T\subseteq V$ be an arbitrary subset. Define $$T^{\perp} = \{ \vec{v}\in V: \vec{v}\perp \vec{t} \text{ for all } \vec{t}\in T\},$$ calling this set the {\bf orthogonal} to $T$.
\end{definition}

\begin{exercise} Show that: in an inner product space, $V$, $T^{\perp}$ is a subspace for {\it any} $T\subseteq V$. Show too that: $T^{\perp} = \langle T \rangle ^{\perp}.$ \end{exercise}

\begin{proposition} \label{hypcom} Let $V$ be an inner product space and let $U$ be a finite dimensional subspace of $V$. Then $U$ and $U^{\perp}$ are complementary in the sense of \hyperref[internaldirsum]{Definition \ref{internaldirsum}}. In other words $$V = U \oplus U^{\perp}$$
\end{proposition}

\begin{proof}
I am going to use the criteria given in \hyperref[compex]{Exercise \ref{compex}}. I must show that $U \cap U^{\perp} = 0$ and $V = U + U^{\perp}$.

Suppose first that $\vec{v}\in U \cap U^{\perp}$. Then $(\vec{v}, \vec{v}) = 0$. By the axioms for an inner product space, this means that $\vec{v} = \vec{0}$, as required.
Now I'll show that every vector can be written as $$\vec{v} = \vec{p} + \vec{r}$$ with $\vec{p}\in U$ and $\vec{r}\in U^{\perp}$. Thanks to \hyperref[orthbasis]{Theorem \ref{orthbasis}} there exists an orthonormal basis for $U$, say $\vec{v}_1, \ldots , \vec{v}_n$. Following \hyperref[remdec]{Remark \ref{remdec}} it is natural to try $\vec{p} = \sum_{i=1}^n \lambda_i \vec{v}_i$ where $\lambda_i = (\vec{v}, \vec{v}_i)$. If I do this, I'd better take $\vec{r} = \vec{v} - \sum_{i=1}^n \lambda_i \vec{v}_i$. What remains for me to show is that $\vec{r}$ does indeed belong to $U^{\perp}$. For this, I calculate:
$$(\vec{r}, \vec{v}_j) = ( \vec{v}, \vec{v}_j)  - \sum_{i=1}^n \lambda_i (\vec{v}_i, \vec{v}_j) = ( \vec{v}, \vec{v}_j)  - \sum_{i=1}^n \lambda_i \delta_{ij} = ( \vec{v}, \vec{v}_j)  -  \lambda_j = 0.$$ Thus $\vec{r}$ is perpendicular to each $\vec{v}_j$ and hence to $U$, the space spanned by the $\vec{v}_j$'s.
\end{proof}

\begin{definition}\label{orthproj}
Let $U$ be a finite dimensional subspace of an inner product space $V$. The space $U^{\perp}$ is the {\bf orthogonal complement to $U$}. The {\bf orthogonal projection from $V$ onto $U$} is the mapping $$\pi_U : V \to V$$ that sends $\vec{v} = \vec{p} + \vec{r}$ to $\vec{p}$. \end{definition}
In the terminology of \hyperref[projalong]{Exercise \ref{projalong}} $\pi_U$ is the projection of $V$ onto $U$ along $U^{\perp}$. In particular, this means that $\pi_U$ satisfies Properties (1) and (3) below. Property (2) has already been checked in the proof of \hyperref[hypcom]{Proposition \ref{hypcom}}
\begin{proposition} \label{orthogproj} Let $U$ be a finite dimensional subspace of an inner product space $V$ and let $\pi_U$ be the orthogonal projection from $V$ onto $U$.
\begin{enumerate}
\item $\pi_U$ is a linear mapping with ${\rm im} (\pi_U) = U$ and kernel ${\rm ker}(\pi_U) = U^{\perp}$.
\item If $\{ \vec{v}_1, \ldots , \vec{v}_n\}$ is an orthonormal basis of $U$, then $\pi_U$ is given by the following formula for all $\vec{v}\in V$ $$\pi_U(\vec{v}) =  \sum_{i=1}^n  (\vec{v}, \vec{v}_i) \vec{v}_i$$
\item $\pi_U^2 = \pi_U$, that is $\pi_U$ is an idempotent.
\end{enumerate}
\end{proposition}

I am now going to explain a few different uses of the orthogonal projection.

\subsection*{The Cauchy-Schwarz Inequality} I think this is the finest elementary inequality in mathematics. It is easy to prove -- in many ways, in fact, as you'll see in a Workshop -- and it permeates all of modern mathematics. The classical version of the inequality, called Cauchy's Inequality for real numbers, states that $$x_1y_1 + x_2y_2 + \cdots + x_ny_n \leqslant \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2 }\sqrt{y_1^2 + y_2^2 + \cdots y_n^2}$$ A serious contemplation of this identity could lead you to invent real inner product spaces, as expounded in this \href{http://www-stat.wharton.upenn.edu/~steele/Publications/Books/CSMC/CSMC_index.html}{lovely book}. That's not a direction I'll take, but I do encourage you to investigate that book. As far as the notes are concerned, you should recognise the left hand side as the inner product $(\vec{x}, \vec{y})$ and the right hand side as $\| \vec{x}\| \| \vec{y}\|$. {\bf The power of abstraction} strikes again: this inequality generalises. This is where Schwarz enters (and also Bunyakovsky -- it's also sometimes called the Cauchy-Bunyakovsky-Schwarz inequality), as he proved that a similar inequality held for integrals: $$\int_a^b f(t)g(t) dt \leqslant \left( \int_a^b f^2(t)dt \right)^{1/2} \left(\int_a^b g^2(t)dt \right)^{1/2}$$ This reminds you of \hyperref[intreal]{Exercise \ref{intreal}(4)} and \hyperref[intcom]{Exercise \ref{intcom}(2)}, right? Well, it generalises much further, to evert sort of weird or wonderful, important or useless inner product space.

\begin{theorem}[Cauchy-Schwarz Inequality]
Let $\vec{v}, \vec{w}$ be vectors in an inner product space. Then $$| (\vec{v}, \vec{w}) | \leqslant \| \vec{v} \| \|\vec{w}\|$$ with equality if and only $\vec{v}$ and $\vec{w}$ are linearly dependent.
\end{theorem}

\begin{proof} If $\vec{w} = \vec{0}$ then the statement is trivially true, so assume that $\vec{w}\neq \vec{0}$. Let $U = \langle \vec{w} \rangle$ and let $\vec{x} = \vec{v} - \pi_U (\vec{v})$. Then $\vec{x} \perp U$ and so Pythagoras' theorem yields: \begin{equation} \label{pyth} \| \vec{v} \|^2 = \| \vec{x} + \pi_U(\vec{v}) \|^2 = \| \vec{x}\|^2 + \|\pi_U(\vec{v}) \|^2\end{equation}  What is $\pi_U(\vec{v})$? To use \hyperref[orthogproj]{Proposition \ref{orthogproj}(2)} I need an orthonormal basis of $U$: the vector $\vec{w}/ \|\vec{w}\|$ will do. Then I see
$$\pi_U(\vec{v}) =(\vec{v},  \frac{\vec{w}}{ \|\vec{w}\|}) \frac{\vec{w}}{  \|\vec{w}\| } = \frac{(\vec{v}, \vec{w})}{ \|\vec{w}\|^2} \vec{w}$$ It follows that $$\|\pi_U(\vec{v}) \|^2 = ( \frac{(\vec{v}, \vec{w})}{ \|\vec{w}\|^2 } \vec{w}, \frac{(\vec{v}, \vec{w})}{ \|\vec{w}\|^2} \vec{w}) = \frac{|(\vec{v}, \vec{w})|^2}{ \|\vec{w}\|^4} (\vec{w}, \vec{w}) = \frac{|(\vec{v}, \vec{w})|^2}{ \|\vec{w}\|^2}$$
So \eqref{pyth} leads to: $$ \|\vec{v}\|^2 = \| \vec{x} \|^2 +  \frac{|(\vec{v}, \vec{w})|^2}{ \|\vec{w}\|^2} \geqslant  \frac{|(\vec{v}, \vec{w})|^2}{ \|\vec{w}\|^2}$$
with equality if and only if $\vec{x} = \vec{0}$. Multiplying through by $\|\vec{w}\|^2$ and taking square roots gives the Cauchy-Schwarz inequality; equality occurs precisely when $\vec{x} = \vec{v} -  \pi_U (\vec{v}) = \vec{0}$, that is when $\vec{v} \in U$ and hence, since $U = \langle \vec{w} \rangle$, when $\vec{v}$ is linearly dependent on $\vec{w}$.
\end{proof}

$$
\begin{tikzpicture}
  \draw[gray, very thin, step=.5cm] (-4.4,-2.4) grid (1.4,1.4);
 \draw[->, red, very thick] (-3.5,-1.5) --  (-3.0,-1.5);
 \draw[->,  dashed, red, very thick] (-3.0,-1.5)  -- (0, -1.5);
 \draw[->, red, very thick] (-3.5,-1.5) --  (0, 1.0);
 \draw[->, red, dashed, very thick] (0,-1.5) --  (0,1);
 \draw (-2,-0.1) node {$\vec{v}$};
  \draw (-3,-1.7) node {$\vec{w}$};
   \draw (0,-1.7) node {$\pi_U(\vec{v})$};
    \draw (0.2,-0.5) node {$\vec{x}$};
\end{tikzpicture}
$$
\begin{center} {\footnotesize Above is an illustration of the critical equality \eqref{pyth} in the proof of the Cauchy-Schwarz inequality, where I have taken the standard inner product on $\mathbb{R}^2$,  $\vec{v} = (7,5)^{\sf T}$ and $\vec{w} = (1,0)^{\sf T}$. I find $\pi_U(\vec{v}) = (7,0)^{\sf T}$ and $\vec{x} = (0,5)^{\sf T}$.} \end{center}
\medskip

As a simple application of this result -- showing that more basic Euclidean geometry appears -- I can now define the angle $\theta$ between two non-zero vectors $\vec{v}$ and $\vec{w}$ in a real inner product space: the Cauchy-Schwarz inequality implies that $-1 \leqslant (\vec{v}, \vec{w}) /\|\vec{v}\| \|\vec{w}\| \leqslant 1$ and so there exists a unique $0\leqslant \theta \leqslant \pi$ such that $$\cos \theta = \frac{(\vec{v}, \vec{w}) }{\|\vec{v}\| \|\vec{w}\|}.$$ This generalises the simple vector algebra fact in $\mathbb{R}^2$ that you probably wrote in school as:
$${\bf a}\cdot {\bf b} = |{\bf a}| |{\bf b}| \cos \theta$$
where $\theta$ is the angle between ${\bf a}, {\bf b}\in \mathbb{R}^2$.

I'll just point out the following corollary for form's sake. It's not critical for the course but it is critical for mathematics: the items listed are exactly the axioms for a {\bf normed vector space} which is the heart of \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath11097.htm}{functional analysis}.
\begin{corollary} \label{normcor}
The norm $\| \cdot \|$ on an inner product space $V$ satisfies, for any $\vec{v}, \vec{w}\in V$ and scalar $\lambda$:
\begin{enumerate}
\item $\| \vec{v} \| \geqslant 0$ with equality if and only if $\vec{v} = \vec{0}$.
\item $\| \lambda \vec{v} \| = |\lambda | \| \vec{v} \|$
\item $\| \vec{v}  + \vec{w} \| \leqslant \| \vec{v} \| + \| \vec{w} \|$, the {\bf triangle inequality}
\end{enumerate}
\end{corollary}
\begin{proof}
(1) is an axiom for an inner product space; (2) is \hyperref[exlength]{Exercise \ref{exlength}}; (3) is a consequence of Cauchy-Schwarz as follows. First, expanding out the brackets as in \hyperref[herecomethegreeks]{Example \ref{herecomethegreeks}} $$\| \vec{v} + \vec{w}\|^2 = \|\vec{v}\|^2 + (\vec{v}, \vec{w}) + (\vec{w}, \vec{v}) + \| \vec{w}\|^2 =  \|\vec{v}\|^2 + 2 \mathfrak{R}e(\vec{v}, \vec{w}) + \| \vec{w}\|^2.$$ Then observe (by the usual triangle inequality in the plane if the underlying field is $\mathbb{C}$ and) by the Cauchy-Schwarz inequality that $\mathfrak{R}e(\vec{v}, \vec{w}) \leqslant | (\vec{v}, \vec{w})) | \leqslant \|\vec{v}\| \| \vec{w}\|$. Therefore $$\| \vec{v} + \vec{w}\|^2 \leqslant  |\vec{v}\|^2 + 2\|\vec{v}\| \| \vec{w}\| + \| \vec{w}\|^2 = ( \|\vec{v}\| + \|\vec{w}\|)^2$$ Taking square roots gives the result.
\end{proof}
\subsection*{Gram-Schmidt Process} In \hyperref[orthbasis]{Theorem \ref{orthbasis}} I showed you that every finite dimensional inner product space has an orthonormal basis. I'll now show you how to find such a basis by massaging any existing basis.

\begin{theorem} \label{GS} Let $\vec{v}_1, \ldots , \vec{v}_k $ be a linearly independent vectors in an inner product space $V$. Then there exists an orthonormal family $\vec{w}_1, \ldots , \vec{w}_k$ with the property that for all $1\leqslant i \leqslant k$ $$\vec{w}_i \in \mathbb{R}_{>0} \vec{v}_i + \langle \vec{v}_{i-1}, \ldots , \vec{v}_1 \rangle$$
\end{theorem}
\begin{proof}
Following \hyperref[orthproj]{Definition \ref{orthproj}}, I can decompose any $\vec{v}_i$ as $\vec{v}_i = \vec{p}_i + \vec{r}_i$ where $\vec{p}_i$ is the orthogonal projection of $\vec{v}_i$ onto the subspace $\langle \vec{v}_{i-1}, \ldots , \vec{v}_1 \rangle$ and where $\vec{r}_i$ belongs to the orthogonal complement of this subspace. The vectors $\vec{w}_i = \vec{r}_i / \|\vec{r}_i\|$ are then an orthonormal family with the displayed property.
\end{proof}

\begin{exercise} Prove that there is a unique orthonormal family whose elements satisfy the property displayed in the statement of \hyperref[GS]{Theorem \ref{GS}}.
\end{exercise}

The proof of \hyperref[GS]{Theorem \ref{GS}} actually gives an algorithm for constructing an orthonormal family from an arbitrary ordered linearly independent subset of an inner product space. If the linearly independent subset happened to be a basis, then this process produces an orthonormal basis. Of course, it is best illustrated through an example, but let me first explain in words what is going to happen. I'll start out with an arbitrary linearly independent ordered subset $\vec{v}_1, \vec{v}_2, \ldots $ of an inner product space. I'll take the first element $\vec{v}_1$ and normalise it so that it has length $1$. That will be the first element $\vec{w}_1$ of the new orthonormal family I'm going to produce. Now I take the second vector $\vec{v}_2$ from the subset. I want to adjust it to be at right-angles to $\vec{w}_1$. To do this I subtract from it the orthogonal projection of it onto the space $\langle \vec{w}_1 \rangle$. This gives me the vector at right-angles that I wanted, and I normalise it so that it has length $1$. This will be $\vec{w}_2$, the second element of the orthonormal family. Now I take the third vector $\vec{v}_3$ from the subset. I want to adjust it to be at right-angles to the first two vectors $\vec{w}_1, \vec{w}_2$. To do this I subtract from it the orthogonal projection of it onto the space $\langle \vec{w}_1, \vec{w}_2 \rangle$. This gives the vector I wanted, and I normalise it so that it has length $1$. This will be $\vec{w}_3$. I repeat this until I've dealt with all the vectors $\vec{v}_1, \vec{v}_2, \ldots$. Observe that each step I have $\langle \vec{w}_{i-1}, \ldots , \vec{w}_1 \rangle \subseteq \langle \vec{v}_{i-1}, \ldots, \vec{v}_1 \rangle$ and because the dimension of both sides is the same, this is actually an equality.

\begin{ex}
Consider $\mathbb{R}^4$ with the standard inner product. Let $V$ be the subspace of $\mathbb{R}^4$ with basis $\{ \vec{v}_1, \vec{v}_2, \vec{v}_3 \}$ where $$\vec{v}_1 = (1,1,0,0)^{\sf T}, \vec{v}_2 = (1,0,1,1)^{\sf T}, \vec{v}_3 = (1,0,0,1)^{\sf T}$$
I want to construct an orthonormal basis $\{ \vec{w}_1, \vec{w}_2, \vec{w}_3\}$ of $V$ from the basis $\{ \vec{v}_1, \vec{v}_2, \vec{v}_3 \}$ using the proof of the theorem.

To unclutter notation, I won't write $\sf T$ to transpose row vectors; that is always there implicitly, and I don't think it is going to cause you any confusion. Let $\vec{w}_1 = \vec{v}_1/ ||\vec{v}_1 || = \frac{1}{\sqrt{2}}(1,1,0,0)$. Now find a vector $$\vec{w}_2' = \vec{v}_2 - \lambda \vec{v}_1 = (1,0,1,1) - {\lambda} (1,1,0,0) = (1 - {\lambda}, - {\lambda}, 1,1)$$ and choose $\lambda$ so that $\vec{w}'_2 \perp \vec{w}_1$. For this, I need $$( \vec{w}'_2, \vec{w}_1) = 1- {\lambda} - {\lambda} = 0$$ so that $\lambda = \frac{1}{{2}}$. This gives $\vec{w}'_2 = (\frac{1}{2}, -\frac{1}{2}, 1, 1)$. Normalising this for unit length gives $\vec{w}_2 = \vec{w}'_2/\|\vec{w}_2'\| = \frac{1}{\sqrt{10}}(1,-1, 2,2)$. Now put \begin{eqnarray*} \vec{w}_3' = \vec{v}_3 - \lambda_1 \vec{w}_1 - \lambda_2 \vec{w}_2 &=& (1,0,0,1) - \frac{\lambda_1}{\sqrt{2}} (1,1,0,0) - \frac{\lambda_2}{\sqrt{10}} (1,-1,2,2) \\ & =& (1 - \frac{\lambda_1}{\sqrt{2}} - \frac{\lambda_2}{\sqrt{10}}, -\frac{\lambda_1}{\sqrt{2}}+ \frac{\lambda_2}{\sqrt{10}}, - 2\frac{\lambda_2}{\sqrt{10}}, 1 - 2\frac{\lambda_2}{\sqrt{10}})\end{eqnarray*} and choose $\lambda_1, \lambda_2$ so that $\vec{w}_3'\perp \vec{w}_1, \vec{w}_2$. For $\vec{w}_3'\perp \vec{w}_1$, I need $$( \vec{w}_3', \vec{w}_1 ) = \frac{1}{\sqrt{2}}\left(((1,0,0,1),(1,1,0,0)) - \frac{\lambda_1}{\sqrt{2}} ((1,1,0,0), (1,1,0,0))\right) = 0$$ That is $1- \sqrt{2}\lambda_1 = 0$, so $\lambda_1 = 1/\sqrt{2}$. (Note that I have used the fact that $\vec{w}_1\perp \vec{w}_2$ when calculating here; the effect is that I get an equation with $\lambda_1$ alone.) For $\vec{w}_3'\perp \vec{w}_2$, I need
$$(\vec{w}_3', \vec{w}_2) = \frac{1}{\sqrt{10}} \left(( (1,0,0,1) , (1,-1,2,2) ) - \frac{\lambda_2}{\sqrt{10}} ( (1,-1,2,2), (1,-1,2,2) ) \right) = 0$$
That is $3 - \lambda_2\sqrt{10}=0$, so $\lambda_2 = 3/\sqrt{10}$. Hence $$\vec{w}'_3 = (1,0,0,1) - \frac{1}{2}(1,1,0,0) - \frac{3}{10}(1,-1,2,2) = \frac{1}{5}(1,-1,-3,2)$$ Normalising this for length one gives $\vec{w}_3 = \frac{1}{\sqrt{15}}(1,-1,-3,2)$. So the orthonormal basis produced by the Gram-Schmidt process is $$\vec{w}_1 =  \frac{1}{\sqrt{2}}(1,1,0,0),\quad \vec{w}_2 =  \frac{1}{\sqrt{10}}(1,-1, 2,2), \quad \vec{w}_3 = \frac{1}{\sqrt{15}}(1,-1,-3,2)$$ You can double-check easily that these vectors have length one and are orthogonal to one another.

Just for a little extra pleasure, let's take a random vector in $V$, say $\vec{v} = \vec{v}_1 + \vec{v}_2 - \vec{v}_3 = (1,1,1,0)$. This can be written as $\alpha_1 \vec{w}_1 + \alpha_2 \vec{w}_2 + \alpha_3\vec{w}_3$ where the coefficients $\alpha_i$ are given by $$\alpha_1 = (\vec{v} , \vec{w}_1) = \frac{2}{\sqrt{2}}= \sqrt{2}, \quad \alpha_2 = (\vec{v}, \vec{w}_2) = \frac{2}{\sqrt{10}} = \frac{\sqrt{10}}{5}, \quad \alpha_3 = (\vec{v}, \vec{w}_3) = \frac{-3}{\sqrt{15}} = - \frac{\sqrt{15}}{5}$$
\end{ex}

\section{Adjoints and Self-Adjoints}
Do you remember my question in lectures about the dictionary between linear mapping and matrices: what does the transpose of a matrix mean for a linear mapping? I'm going to show you something very closely related by introducing the adjoint of a linear mapping on $V$. You'll already have come across this if you are taking \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath10066.htm}{Honours Differential Equations}. To connect to that, I think of differential equations in two parts: a space of functions I want to use (periodic, complex valued, square-integrable, whatever) together with differential equations (such as $- \frac{d}{dx}( p(x) \frac{d}{dx}) + q(x)$) that I consider as mappings on the space of functions. The functions used often produce inner product spaces similar to \hyperref[intreal]{Exercise \ref{intreal}(4)} and \hyperref[intcom]{Exercise \ref{intcom}(3)}, and studying eigenvalues and eigenfunctions related to the differential equation is closely related to what I do here. For me, the most crucial is the case of a self-adjoint operator, which is at the heart of Sturm-Liouville Theory. A second place where this theory is critical is in \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath11053.htm}{Introduction to Lie Groups} where the symmetry involved in adjoints are probed systematically.

\begin{definition} Let $V$ be an inner product space. Then two endomorphism $T, S: V\to V$ are called {\bf adjoint} to one another if the following holds for all $\vec{v}, \vec{w} \in V$: $$( T\vec{v}, \vec{w}) = (\vec{v}, S\vec{w})$$ In this case I will write $S = T^{\ast}$ and call $S$ the {\bf adjoint} of $T$.
\end{definition}

\begin{rem}
Any endomorphism has at most one adjoint. This is because if both $S$ and $S'$ are adjoint to $T$ then $(\vec{v}, S\vec{w} - S' \vec{w}) = 0$ for all $\vec{v}, \vec{w} \in V$, so the positivity axiom for an inner product space immediately implies that $S\vec{w} = S'\vec{w}$ for all $\vec{w}$.
\end{rem}

\begin{exercise}
Show that: If $T^{\ast}$ is the adjoint of $T$, then $T^{\ast}$ has an adjoint and it is $(T^{\ast})^{\ast} = T$.
\end{exercise}

I don't have a particularly persuasive intuitive reason why I must study adjoints: it's an obvious and nice definition, but there is not much more that I can write at this moment. Sigh! Probably the best thing I can say is \href{http://golem.ph.utexas.edu/category/2013/08/linear_operators_done_right.html}{taking the adjoint of an endomorphism is analogous to taking the conjugate of a complex number} and leave it at that. Maybe you're smitten by complex conjugation, maybe not. So in fact probably the best thing I can do is present an example.

\begin{ex} \label{adjtrn} Suppose $V = \mathbb{R}^n$ or $\mathbb{C}^n$ with the standard inner product and suppose the endomorphism $T$ of $V$ is given by matrix multiplication $A\circ$ where $A\in {\sf Mat}(n;\mathbb{R})$ or $A\in {\sf Mat}(n; \mathbb{C})$.

Recall from \hyperref[standardreal]{Definition \ref{standardreal}} that in $\mathbb{R}^n$, $(\vec{v}, \vec{w}) = \vec{v}^{\sf T} \circ \vec{w}$. Hence $$(A\circ \vec{v},  \vec{w}) = (A\circ\vec{v})^{\sf T}\circ \vec{w} = \vec{v}^{\sf T} \circ A^{\sf T} \circ \vec{w} = \vec{v}^{\sf T} \circ (A^{\sf T}\circ \vec{w} )= (\vec{v}, A^{\sf T}\circ\vec{w})$$ Therefore the adjoint of multiplication by $A$ in $\mathbb{R}^n$ is multiplication by $A^{\sf T}$. At last, some meaning for the transpose!

In $\mathbb{C}^n$ \hyperref[standardcomp]{Definition \ref{standardcomp}} tells me that $(\vec{v}, \vec{w}) = \vec{v}^{\sf T} \circ \overline{\vec{w}}$, so I find $$(A\circ\vec{v}, \vec{w}) = ( \vec{v}, \overline{A}^{\sf T}\circ\vec{w})$$ where $\overline{A}^{\sf T}$ denotes the {\bf conjugate transpose} of $A$, the matrix obtained from $A$ by first conjugating each entry and then transposing the resulting matrix. Therefore the adjoint of multiplication by $A$ in $\mathbb{C}^n$ is multiplication by $\overline{A}^{\sf T}$.
\end{ex}

\begin{theorem} \label{adjoint} Let $V$  be a finite dimensional inner product space. Let $T:V\to V$ be an endomorphism. Then $T^\ast$ exists. That is, there exists a unique linear mapping $T^\ast : V\to V$ such that for all $\vec{v}, \vec{w}\in V$ $$( T\vec{v}, \vec{w}) = (\vec{v}, T^{\ast}\vec{w})$$
\end{theorem}

\begin{proof}
I will begin by defining $T^{\ast}\vec{w}$ for an arbitrary $\vec{w}\in V$. I'll use again the notation that $F$ stands for either $\mathbb{R}$ or $\mathbb{C}$ depending on whether $V$ is real or complex inner product space. Fix $\vec{w}\in V$ and consider the mapping $$\phi = (T(-), \vec{w}): V\to F$$ This is linear since it is the composition of the two linear mappings $T$ and $(-, \vec{w})$.

I claim there is a unique vector $\vec{u}\in V$ such that $\phi(\vec{v}) = (\vec{v}, \vec{u})$ for all $\vec{v}\in V$. To see that $\vec{u}$ exists, take by \hyperref[orthbasis]{Theorem \ref{orthbasis}} any orthonormal basis of $V$, say $\vec{e}_1, \ldots , \vec{e}_n$. By \eqref{illuseitlater} $\vec{v} = \sum_{i=1}^n (\vec{v}, \vec{e}_i) \vec{e}_i$. By the linearity of $\phi$ and the sesquilinearity of the $(-,-)$ I get $$\phi(\vec{v}) = \sum_{i=1}^n (\vec{v}, \vec{e}_i) \phi(\vec{e}_i) = (\vec{v}, \sum_{i=1}^n \overline{\phi(\vec{e}_i)} \vec{e}_i)$$ Hence $\vec{u} =  \sum_{i=1}^n \overline{\phi(\vec{e}_i)} \vec{e}_i$ does the trick for existence. The uniqueness is quicker: if $\vec{u}$ and $\vec{u}'$ both produce $\phi$, then $$(\vec{v}, \vec{u} - \vec{u}') =  (\vec{v}, \vec{u}) - (\vec{v},  \vec{u}') = \phi(\vec{v}) - \phi(\vec{v}) = 0$$ for all $\vec{v} \in V$. By positivity then, $\vec{u}-\vec{u}' = 0$, i.e. $\vec{u} = \vec{u}'$.

This defines $T^{\ast}$: for each $\vec{w}\in V$ I find the corresponding $\vec{u}$ from above and that's $T^{\ast} \vec{w}$. This is a well-defined and unique mapping $T^{\ast} : V\to V$ that satisfies the property $$( T\vec{v}, \vec{w}) = (\vec{v}, T^{\ast}\vec{w})$$ for all $\vec{v}, \vec{w}\in V$. It remains for me to prove that $T^{\ast}$ is linear. Let $\vec{w}_1, \vec{w}_2 \in V$ and $\lambda \in F$. Then disentangling the definition of $T^{\ast}$ gives me for any $\vec{v}\in V$
$$
(\vec{v}, T^{\ast}(\vec{w}_1 + \vec{w}_2)) = (T\vec{v}, \vec{w}_1 + \vec{w_2}) = (T\vec{v}, \vec{w}_1) + (T\vec{v}, \vec{w}_2) = (\vec{v}, T^{\ast}\vec{w}_1) + (\vec{v}, T^{\ast}\vec{w}_2) = (\vec{v}, T^{\ast}\vec{w}_1 + T^{\ast}\vec{w}_2)$$
and
$$(\vec{v}, T^{\ast}(\lambda \vec{w}_1)) = (T\vec{v}, \lambda \vec{w}_1) = \overline{\lambda}(T\vec{v}, \vec{w}_1) = \overline{\lambda}(\vec{v}, T^{\ast}\vec{w}_1) = (\vec{v}, \lambda T^{\ast}\vec{w}_1)$$
It follows from the uniqueness claim of the paragraph above that $T^{\ast}(\vec{w}_1 + \vec{w}_2) = T^{\ast}\vec{w}_1 + T^{\ast}\vec{w}_2$ and $T^{\ast}(\lambda \vec{w}_1) = \lambda T^{\ast}\vec{w}_1$. That's what I wanted to prove!
\end{proof}

\begin{definition} An endomorphism of an inner product space $T:V\to V$ is  {\bf self-adjoint} if it equals its own adjoint, that is if $T^{\ast} = T$. \end{definition}

If you liked the idea that \href{http://golem.ph.utexas.edu/category/2013/08/linear_operators_done_right.html}{taking the adjoint of an endomorphism is analogous to taking the conjugate of a complex number}, then you'll buy in to self-adjoint operators being analogous to real numbers. Given \hyperref[reality]{Theorem \ref{reality}} below, this analogy starts to seem realistic.

\begin{ex} By \hyperref[adjtrn]{Example \ref{adjtrn}}, a real $(n\times n)$-matrix $A$ describes a self-adjoint mapping on the standard inner product space $\mathbb{R}^n$ precisely when $A$ is symmetric, that is when $A^{\sf T} = A$. A complex $(n\times n)$-matrix $A$ describes a self-adjoint mapping on the standard inner product space $\mathbb{C}^n$ precisely when $A = \overline{A}^{\sf T}$ holds. Such matrices are called {\bf hermitian}.
\end{ex}

\begin{theorem}
\label{reality}
Let $T:V \to V$ be a self-adjoint linear mapping on an inner product space $V$.
\begin{enumerate}
\item Every eigenvalue of $T$ is real.
\item If $\lambda$ and $\mu$ are distinct eigenvalues of $T$ with corresponding eigenvectors $\vec{v}$ and $\vec{w}$, then $(\vec{v}, \vec{w}) = 0$.
\item $T$ has an eigenvalue.
\end{enumerate}
\end{theorem}
\begin{proof}
(1) This first statement has content only for a complex inner product space. But let $\vec{v}$ be an eigenvector of $T$ with eigenvalue $\lambda$. Then $$\lambda  (\vec{v}, \vec{v}) = (\lambda \vec{v}, \vec{v}) = (T\vec{v}, \vec{v}) = (\vec{v}, T\vec{v}) = ( \vec{v}, \lambda\vec{v}) = \overline{\lambda} (\vec{v}, \vec{v})$$ Since $\vec{v} \neq \vec{0}$, I have $(\vec{v}, \vec{v})>0$ and so $\lambda = \overline{\lambda}$.

(2) Both $\lambda$ and $\mu$ must be real by (1). Therefore $$\lambda  (\vec{v}, \vec{w})  = (T\vec{v}, \vec{w}) = (\vec{v}, T\vec{w}) = \mu (\vec{v}, \vec{w})$$ Since $\lambda \neq \mu$ this is only possible if $(\vec{v}, \vec{w}) = 0$.

(3) In the case of a complex inner product space this is simply a consequence of \hyperref[evalsexist]{Theorem \ref{evalsexist}} since $\mathbb{C}$ is algebraically closed. The real case is much more interesting. If you are so-minded you can find an algebraic proof in \href{http://catalogue.lib.ed.ac.uk/vwebv/holdingsInfo?searchId=4905&recCount=10&recPointer=0&bibId=1771135}{Lemma 7.12 of Linear Algebra Done Right}. But I find that proof doesn't tell me anything except the statement is true, so I prefer the following proof, which may remind you of the proof of \hyperref[perron]{Perron's Theorem}.

Assume that $V$ is a finite dimensional real inner product space. Define the following real-valued function $R: V\setminus \{ \vec{0} \} \to\mathbb{R}$ $$\vec{v}\mapsto R(\vec{v}) = \frac{(T\vec{v}, \vec{v})}{(\vec{v}, \vec{v})}$$ It's called the {\bf Raleigh Quotient}. Restricting this function to the unit sphere $\{ \vec{v} : \| \vec{v} \| =1 \}$ allows me to apply the \href{http://en.wikipedia.org/wiki/Heine-Borel_theorem}{Heine--Borel Theorem} from \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath10068.htm}{Honours Analysis} to deduce that there exists a point on the unit sphere, $\vec{v}_+$, where $R$ achieves its maximum. Because the function $R$ has the property that $R(\lambda \vec{v}) = R(\vec{v})$ for any $\lambda \in \mathbb{R}_{>0}$ this means that the function $R$ actually achieves its maximum on all of $V\setminus \{ \vec{0} \}$ at $\vec{v}_+$. Now take any vector $\vec{w} \in V$ and observe that for any really small $t\in \mathbb{R}$ the one-variable function $t\mapsto R_{\vec{w}}(t) = R(\vec{v}_+ + t\vec{w})$ is well-defined. Written out this is $$R_{\vec{w}}(t) = \frac{ (T(\vec{v}_+ + t\vec{w}), \vec{v}_+ + t\vec{w})}{(\vec{v}_+ + t\vec{w},\vec{v}_+ + t\vec{w})}$$ The derivative of this function with respect to $t$ has got to vanish at $t=0$ because $\vec{v}_+$ is a maximum. By the quotient rule I can write out $R_{\vec{w}}'(0)$ explicitly for all $\vec{w}\in V$:
$$ R_{\vec{w}}'(0) = \frac{(T\vec{w}, \vec{v}_+) + (T\vec{v}_+, \vec{w})}{(\vec{v}_+, \vec{v}_+)} - \frac{2 (T\vec{v}_+, \vec{v}_+)(\vec{v}_+ , \vec{w})}{(\vec{v}_+, \vec{v}_+)^2}$$
If I choose any $\vec{w} \perp \vec{v}_+$ I then deduce that $$\frac{(T\vec{w}, \vec{v}_+) + (T\vec{v}_+, \vec{w})}{(\vec{v}_+, \vec{v}_+)} =0 $$ and so, by using that $T$ is self-adjoint and $(\vec{v}_+, \vec{v}_+) \neq 0$, that $\vec{w} \perp T\vec{v}_+$. In other words, $T\vec{v}_+ \in (\langle \vec{v}_+\rangle^{\perp})^{\perp}$. By \hyperref[hypcom]{Proposition \ref{hypcom}} $(\langle \vec{v}_+\rangle^{\perp})^{\perp} = \langle \vec{v}_+ \rangle$ and so  $T\vec{v}_+ \in \mathbb{R}\vec{v}_+$, as required.
\end{proof}

\begin{rem}\label{ellipses}
(1) Why do I like this proof? It's completely geometric! Let me consider $\mathbb{R}^2$ with its standard inner product. To illustrate the theorem I'll take $T$ to be represented by the symmetric matrix $$T = \begin{pmatrix} 5 & -6 \\ -6 & 13 \end{pmatrix}$$ I chose this more-or-less at random; I need it to be symmetric for $T$ to be self-adjoint and that's all.  The proof above then tells me to maximise the Rayleigh quotient $$R(\vec{v}) = \frac{(T\vec{v}, \vec{v})}{(\vec{v}, \vec{v})}$$ over the set of points $\|\vec{v} \|=1$. Because of the invariance of $R(\vec{v})$ under dilation, maximizing the numerator of $R(\vec{v})$ while keeping the denominator fixed is the same as minimizing the denominator while keeping the numerator fixed.  So I have to minimize $\|\vec{v}\|$ over the set of points $(T\vec{v}, \vec{v}) = 1$. If I write $\vec{v} = (x,y)^{\sf T}$ then I see that $$(T\vec{v}, \vec{v}) = ((5x-6y, -6x + 13y)^{\sf T}, (x,y)^{\sf T}) = 5x^2 - 12xy + 13y^2$$ Here is a plot of $5x^2 - 12xy + 13y^2 = 1$:
% $$
% \includegraphics[scale=0.5]{ellipse.pdf}
% $$
Now think back to school when you studied \href{http://en.wikipedia.org/wiki/Ellipse}{ellipses}: remember that any ellipse is symmetric about two axes at right angles to one another, called the major and the minor axes. Here the major axis is a line roughly in the northeast-southwest direction and the minor axis in the northwest-southeast direction. Minimizing $\|\vec{v}\|$ on this curve chooses the two points on the curve closest to the origin; the line through those points is precisely what determines the minor axis. Of course, I could applied exactly the same argument as in the proof of \hyperref[reality]{Theorem \ref{reality}} {\it except} for looking for a minimum for $R(\vec{v})$ instead of a maximum. This would then have produced a different eigenvector, this time determining the points on the curve furthest from the origin, i.e. the major axis. That the major and minor axes are at right angles to each other then just becomes a manifestation of Part (2) of
 \hyperref[reality]{Theorem \ref{reality}}. All that understanding just from thinking what $(T\vec{v}, \vec{v})$ actually is!\\
(2) Let $(V,(-,-))=(\RR^n,\bullet)$ be the standard $n$-dimensional inner product space, defined by the dot product
$$(x_1,x_2,\dots,x_n) \bullet (y_1,y_2,\dots,y_n)~=~\sum\limits^n_{i=1}x_iy_i \in \RR~.$$
Given a symmetric $n \times n$ matrix $A=(a_{ij}) \in {\sf Mat}(n;\RR)$ let $T:\RR^n \to \RR^n$ be the self-adjoint endomorphism with matrix $A$ defined by
$$T(x_1,x_2,\dots,x_n)~=~(\sum\limits^n_{j=1}a_{1j}x_j,\sum\limits^n_{j=1}a_{2j}x_j,\dots,\sum\limits^n_{j=1}a_{nj}x_j) \in \RR^n~.$$
Consider the Rayleigh quotient function as in the proof of \hyperref[reality]{Theorem \ref{reality}}
$$R~:~\RR^n\backslash \{\vec{0}\} \to \RR~;~\vec{x}=(x_1,x_2,\dots,x_n) \mapsto
R(\vec{x})~=~
\dfrac{T\vec{x} \bullet\vec{x}}{\vec{x} \bullet \vec{x}}~=~
\dfrac{\sum\limits^n_{i=1}\sum\limits^n_{j=1}a_{ij}x_ix_j}{\sum\limits^n_{k=1}(x_k)^2}~.$$
In view of \hyperref[spectralmatrices]{the Spectral Theorem for Real Symmetric Matrices \ref{spectralmatrices}} below, the symmetric matrix $A$ is diagonalizable, and $\RR^n$ has an orthonormal basis ${\mathcal B}=(\vec{v}_1,\vec{v}_2,\dots,\vec{v}_n)$ of eigenvectors of $T$ (= eigenvectors of $A$) such that
$$T(\vec{v}_i)~=~\lambda_i \vec{v}_i \in \RR^n$$
with the eigenvalues such that $\lambda_1 \geqslant \lambda_2 \geqslant \dots \geqslant \lambda_n$. With respect to this basis $\mathcal{B}$ the Rayleigh quotient becomes
$$R(x_1,x_2,\dots,x_n)~=~\dfrac{\sum\limits^n_{j=1}\lambda_j(x_j)^2}{\sum\limits^n_{k=1}(x_k)^2} \in \RR$$
with maximum value $\lambda_1$ and minimum value $\lambda_n$. In particular, if $n=2$
then the subset
$$E~=~\{(x_1,x_2) \in \RR^2\,\vert\,\lambda_1 (x_1)^2+\lambda_2(x_2)^2=1\} \subset \RR^2$$
is a 	`conic section'. (\href{http://en.wikipedia.org/wiki/Conic_section}{Conic sections} have been studied for 2,000 years, since before Archimedes). As in the example in (1) if $\lambda_1 \geqslant \lambda_2 >0$ $E$ is an ellipse
with maximum (resp. minimum) distance from the origin $1/\sqrt{\lambda}_1$ (resp. $1/\sqrt{\lambda}_2$). See \hyperref[schade]{Remark \ref{schade}} below for a bit more about this.
\hfill\qed
\end{rem}

Upwards to one of the highlights of the course, your degree, your intellectual life:
\medskip

\begin{center}Simple Cosmic Beauty\end{center}
\medskip

\noindent
At this moment the theorem below might only appear as a good-looking result: general hypothesis; simple statement; natural outcome. And I hope you are meeting such handsome theorems each week at University. You are, aren't you? The Spectral Theorem, however, and its correct infinite dimensional extension to \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath10046.htm}{Hilbert Spaces}, is ubiquitous. Is that apparent to you? Maybe not. Except you may at least be surprised by the result that all real symmetric matrices are diagonalisable. Oh, and you might be taking \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath10066.htm}{Honours Differential Equations} and know that it is the foundation of Sturm-Liouville Theory. And if you have taken a course in quantum mechanics you may know that observables are self-adjoint operators on the space of states, a Hilbert space, and that eigenvalues are then possible measurement outcomes. And I know you've just seen that self-adjoint operators control parts of classical geometry. But that would be just the beginning! \footnote{And in any case ``The Spectral Theorem" is a cool name}

\begin{theorem}[The Spectral Theorem for Self-Adjoint Endomorphisms] \label{spectral} Let $V$ be a finite dimensional inner product space and let $T:V\to V$ be a self-adjoint linear mapping. Then $V$ has an orthonormal basis consisting of eigenvectors of $T$.
\end{theorem}

\begin{proof}
I'll induct on $\dim V$. If $\dim V$ is either $0$ or $1$ the result clearly holds. Now assume that $\dim V >1$ and that the result holds on inner product spaces of smaller dimension.

Let $\lambda$ be any eigenvalue of $T$. By \hyperref[reality]{Theorem \ref{reality}} this exists and is real. Let $\vec{u}$ be an eigenvector with eigenvalue $\lambda$, normalized so that $\vec{u}$ has unit length. Let $U = \langle \vec{u} \rangle$. Suppose that $\vec{v}\in U^{\perp}$. Then because $T$ is self-adjoint and $\vec{u}$ is an eigenvector, I have $$ (\vec{u}, T\vec{v}) = (T\vec{u}, \vec{v}) = (\lambda \vec{u}, \vec{v}) = \lambda (\vec{u}, \vec{v}) = 0$$ Therefore $T(U^{\perp}) \subseteq U^{\perp}$. Thus $T$ restricted to $U^{\perp}$ defines a linear mapping $T|_{U^{\perp}}: U^{\perp} \to U^{\perp}$. This linear mapping is self-adjoint because $T$ is, and so the induction hypothesis applies to give me an orthonormal basis of $U^{\perp}$ consisting of eigenvectors of $T|_{U^{\perp}}$. Combined with $\vec{u}$, this gives me the orthonormal basis of $V$ that I was seeking.
\end{proof}

\begin{ex}
First of all I'll show you this theorem in action in the real case. Let $T$ be the symmetric matrix appearing in \hyperref[ellipses]{Remark \ref{ellipses}} $$T = \begin{pmatrix} 5 & -6 \\ -6 & 13 \end{pmatrix}$$ Its characteristic polynomial is $x^2 - 18x + 29$ (I told you it was more-or-less random!) The roots of this are $9 \pm 2\sqrt{13}$. So I have to find null vectors for $$T - (9+2\sqrt{13})I_2 = \begin{pmatrix} -4 - 2\sqrt{13} & -6 \\ -6 & 4 - 2\sqrt{13} \end{pmatrix} \text{ and } T - (9-2\sqrt{13})I_2 = \begin{pmatrix} -4 + 2\sqrt{13} & -6 \\ -6 & 4 + 2\sqrt{13} \end{pmatrix} $$ For the first, I find $(6, -4-2\sqrt{13})^{\sf T}$; for the second, $(6, -4+2\sqrt{13})^{\sf T}$. Neither of these is a unit vector, so I have to normalize. This produces $$\vec{u}_1 = \frac{1}{2\sqrt{26 + 4\sqrt{13}}}(6, -4-2\sqrt{13})^{\sf T} \text{ and } \vec{u}_2 = \frac{1}{2\sqrt{26 - 4\sqrt{13}}} (6, -4+2\sqrt{13})^{\sf T}$$ Observe explicitly that $\vec{u}_1 \perp \vec{u}_2$. If I get the computer to calculate a decimal expansion of these vectors I find $(0.47,-0.88)$ and $(0.88,0.47)$. If you look back at the picture of the ellipse in \hyperref[ellipses]{Remark \ref{ellipses}}, you'll see that these tie up with the directions of the minor and major axes respectively.

If I wrote out what I've just done in terms of matrices it is $P^{-1}TP = D$ where $D$ is the diagonal matrix consisting of the eigenvalues of $T$ and $P$ is the matrix whose columns are the corresponding eigenvectors of $T$. Explicitly: $$ D = {\rm diag}(9 + 2\sqrt{13}, 9 - 2\sqrt{13}), \quad P = \begin{pmatrix} \frac{6}{2\sqrt{26 + 4\sqrt{13}}} & \frac{6}{2\sqrt{26 - 4\sqrt{13}}} \\ \frac{-4 -2\sqrt{13}}{2\sqrt{26 + 4\sqrt{13}}} & \frac{-4+2\sqrt{13}}{2\sqrt{26 - 4\sqrt{13}}} \end{pmatrix} $$
Remarkably, when I calculate $P^{-1}$ I find $$  P^{-1} = \begin{pmatrix} \frac{6}{2\sqrt{26 + 4\sqrt{13}}} & \frac{-4 -2\sqrt{13}}{2\sqrt{26 + 4\sqrt{13}}}  \\  \frac{6}{2\sqrt{26 - 4\sqrt{13}}}& \frac{-4+2\sqrt{13}}{2\sqrt{26 - 4\sqrt{13}}} \end{pmatrix} $$  That is, $P^{-1} = P^{\sf T}$.
\end{ex}

\begin{definition}
Show that: an {\bf orthogonal matrix} is an $(n\times n)$-matrix $P$ with real entries such that $P^{\sf T}P = I_n$. In other words, an orthogonal matrix is a square matrix $P$ with real entries such that $P^{-1} = P^{\sf T}$.
\end{definition}

\begin{exercise}\label{orthmat}
The condition that $P^{\sf T}P = I_n$ is equivalent to the columns of $P$ forming an orthonormal basis for $\mathbb{R}^n$ with its standard inner product.
\end{exercise}

\begin{exercise}
Show that: the set $\{ P\in {\sf Mat}(n;\mathbb{R}) : P^{\sf T}P = I_n \}$ is a group. It is called the {\bf orthogonal group}, $O(n)$.
\end{exercise}

\begin{corollary}[The Spectral Theorem for Real Symmetric Matrices]\label{spectralmatrices} Let $A$ be a real $(n\times n)$-symmetric matrix. Then there is an $(n\times n)$-orthogonal matrix $P$ such that $$P^{\sf T}AP = P^{-1}AP = {\rm diag} (\lambda_1, \ldots , \lambda_n)$$ where $\lambda_1, \ldots , \lambda_n$ are the (necessarily real) eigenvalues of $A$, repeated according to their multiplicity as roots of the characteristic polynomial of $A$.
\end{corollary}

\begin{proof} This follows from combining the \hyperref[spectral]{Spectral Theorem} applied to $\mathbb{R}^n$ with its standard inner product with \hyperref[orthmat]{Exercise \ref{orthmat}}.
\end{proof}

\begin{ex} Now I'll show you the \hyperref[spectral]{Spectral Theorem} in action in the complex case. Let $T$ be the hermitian matrix $$T = \frac{1}{2} \begin{pmatrix} 5  & 1 & -1 + i \\ 1 & {5}  & 1 - i \\ -1 - i & 1+i & 4 \end{pmatrix} $$ A calculation gives the characteristic polynomial of $T$ to be $- x^3 + 7x^2 - 15x + 9 = (3-x)^2(1-x)$.  I then calculate the eigenspace for $E(3,T)$ to be spanned by $(1,1,0)^{\sf T}$ and $(2,0,-1-i)^{\sf T}$. I calculate the eigenspace $E(1,T)$ to be spanned by $( 1,-1,1+i)^{\sf T}$. So, I'm almost done: $E(1,T)\perp E(3,T)$. But I still don't have an orthonormal basis of eigenvectors. For this I need to find an orthonormal basis of $E(3,T)$ first and also to normalise the basis vector of $E(1,T)$ I've given. I use the \hyperref[GS]{Gram-Schmidt Process} to deal with $E(3,T)$, starting with the basis $(1,1,0)^{\sf T}$ and $(2,0,-1-i)^{\sf T}$. This gives me an orthonormal basis of eigenvectors: $$\vec{u}_1 = \frac{1}{\sqrt{2}}(1,1,0)^{\sf T}, \quad \vec{u}_2 = \frac{1}{2}(1,-1,-1-i)^{\sf T}, \quad \vec{u}_3 = \frac{1}{2}(1,-1,1+i)^{\sf T}$$

If I wrote out what I've just done in terms of matrices it is $P^{-1}TP = D$ where $D$ is the diagonal matrix consisting of the eigenvalues of $T$ and $P$ is the matrix whose columns are the corresponding eigenvectors of $T$. Explicitly: $$ D = {\rm diag}(3,3,1), \quad P = \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{2} & \frac{1}{2} \\ \frac{1}{\sqrt{2}} & -\frac{1}{2} & -\frac{1}{2} \\ 0 & -\frac{1+i}{2} & \frac{ 1+i}{2}\end{pmatrix} $$
Remarkably, when I calculate $P^{-1}$ I find $$  P^{-1} = \begin{pmatrix} \frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} & 0 \\ \frac{1}{2} & - \frac{1}{{2}} & -\frac{1-i}{2} \\ \frac{1}{2} & -\frac{1}{2} & \frac{ 1-i}{2} \end{pmatrix} $$  That is, $P^{-1} = \overline{P}^{\sf T}$, the conjugate transpose.
\end{ex}

\begin{definition}
An {\bf unitary matrix} is an $(n\times n)$-matrix $P$ with complex entries such that $\overline{P}^{\sf T}P = I_n$. In other words, a unitary matrix is a square matrix $P$ with complex entries such that $P^{-1} = \overline{P}^{\sf T}$.
\end{definition}

\begin{exercise}\label{orthmat2}
Show that: the condition that $\overline{P}^{\sf T}P = I_n$ is equivalent to the columns of $P$ forming an orthonormal basis for $\mathbb{C}^n$ with its standard inner product.
\end{exercise}

\begin{exercise}
Show that: the set $\{ P\in {\sf Mat}(n;\mathbb{C}) : \overline{P}^{\sf T}P = I_n \}$ is a group. It is called the {\bf unitary group}, $U(n)$.
\end{exercise}

\begin{corollary}[The Spectral Theorem for Hermitian Matrices] Let $A$ be a $(n\times n)$-hermitian matrix. Then there is an $(n\times n)$-unitary matrix $P$ such that $$\overline{P}^{\sf T}AP = P^{-1}AP = {\rm diag} (\lambda_1, \ldots , \lambda_n)$$ where $\lambda_1, \ldots , \lambda_n$ are the (necessarily real) eigenvalues of $A$, repeated according to their multiplicity as roots of the characteristic polynomial of $A$.
\end{corollary}

\begin{proof} This follows from combining the \hyperref[spectral]{Spectral Theorem} applied to $\mathbb{C}^n$ with its standard inner product with \hyperref[orthmat2]{Exercise \ref{orthmat2}}.
\end{proof}

\begin{rem} \label{schade}
\begin{enumerate}
\item
Orthogonal and unitary groups are some of the most important groups in mathematics. They are the cornerstones \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath11053.htm}{Introduction to Lie Groups} and their properties determine the shape of that topic. You already know many examples of orthogonal matrices: those representing reflections and rotations in $\mathbb{R}^n$ with respect to the standard basis. The orthogonal matrices, together with ``translations" $\tau_{\vec{w}}: \vec{x} \mapsto \vec{x}+\vec{w}$ determine the group that defines Euclidean geometry from the point of view of \href{http://en.wikipedia.org/wiki/Erlangen_program}{Klein's ``Erlangen Program"}. I'd love to tell you more, but it's really the topic for a different course in geometry.
\item Given a real symmetric matrix $A= (a_{ij})\in {\sf Mat}(n; \mathbb{R})$, I can define the following {\bf quadratic form} on $\mathbb{R}^n$, using the standard inner product: $$\vec{v} \mapsto Q(\vec{v}) = (A\vec{v}, \vec{v})$$ You already saw an $n=2$ case of this \hyperref[ellipses]{Remark \ref{ellipses}}. Such quadratic forms appear throughout the mathematics, and are particularly important in \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath10027.htm}{Algebraic Topology}, \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath10088.htm}{Differentiable Manifolds} and \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath11138.htm}{General Relativity} amongst other subjects. A simple consequence of the \hyperref[spectral]{Spectral Theorem} is \href{http://en.wikipedia.org/wiki/Sylvester'27s_law_of_inertia}{Sylvester's Law of Inertia} which states that you can make a change of basis in $\mathbb{R}^n$, sending $\vec{v}$ to $\vec{x}$, so that the quadratic form looks like:
$$Q(\vec{v}) = \sum_{i=1}^n a_i x_i^2$$ where  each $a_i \in \{ -1,0,1\}$ and such that the numbers of $1$'s, $0$'s and $-1$'s is independent of the change of basis. For $n=2$ this produces the tetraptych of ellipses, hyperbolae, parabolae and straight lines that you may have met before.
\item There is one final topic related to orthogonal projection that it would have been good to go into detail about: nearest point to a subspace and the \href{http://en.wikipedia.org/wiki/Least_squares}{least squares} approximation. Schade in German, dommage in French, but in any case a pity!
\end{enumerate}
\end{rem}

\chapter{Jordan Normal Form}\label{JNF}

\section{Motivation}

\noindent {\bf Differential Equations} If you are taking the course \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath10066.htm}{Honours Differential Equations} you have seen the exponential mapping for complex square matrices. It is defined by the series: \begin{eqnarray*} {\rm exp}: {\sf Mat}(n; \mathbb{C}) &\to & {\sf Mat}(n; \mathbb{C}) \\ A & \mapsto & \sum_{k=0}^{\infty} \frac{1}{k!} A^k \end{eqnarray*} This mapping plays a central role in describing the solutions to linear differential equations with constant coefficients. More precisely, if $A\in {\sf Mat}(n; \mathbb{C})$ is a square matrix and $\vec{c}\in \mathbb{C}^n$ a column vector, then there exists exactly one differentiable mapping $\gamma: \mathbb{R} \to \mathbb{C}^n$ with initial value $\gamma (0) = \vec{c}$ and which satisfies $\dot{\gamma}(t) = A\gamma(t)$ for all $t\in \mathbb{R}$: it is the mapping $$\gamma(t) = {\rm exp}(tA)\vec{c}$$ This fact is motivation to get the best possible understanding of ${\rm exp} (A)$.

The formula ${\rm exp}(PAP^{-1}) = P ({\rm exp}( A))P^{-1}$ for invertible $P$ follows straight from the definition of ${\rm exp}$. Another elementary property is that the formula $ {\rm exp}(A+B) = {\rm exp}(A){\rm exp}(B)$ holds for square matrices $A$ and $B$ that commute. This is not hard to check formally:
\begin{eqnarray*} {\rm exp}(A+B) &=& \sum_{k=0}^{\infty} \frac{1}{k!} (A+B)^k \\ & = & \sum_{k=0}^{\infty} \frac{1}{k!} \sum_{i+j=k} \frac{k!}{i!j!}A^iB^j \\ & = & \sum_{k=0}^{\infty} \sum_{i+j = k} \frac{1}{i!}A^i \, \frac{1}{j!} B^j \\ &=& {\rm exp}(A) {\rm exp}(B)
\end{eqnarray*}

In \hyperref[JNFtheorem]{Theorem \ref{JNFtheorem}} I will prove the Jordan Normal Form Theorem, which has as part of its proof the consequence that each complex square matrix can be decomposed uniquely as a sum $A = D+N$ with $D$ diagonalisable and $N$ nilpotent and $DN = ND$. In particular this means that there exists an invertible matrix $P$ with $PDP^{-1} = {\rm diag}(\lambda_1, \ldots , \lambda_n)$. It then follows from the observations I made in the previous paragraph that: \begin{eqnarray*} {\rm exp} (A) & = & {\rm exp} (D) {\rm exp} (N) \\ & = & P^{-1} {\rm exp}({\rm diag}(\lambda_1, \ldots , \lambda_n) ) P {\rm exp}N \\ & = & P^{-1} {\rm diag}(e^{\lambda_1}, \ldots, e^{\lambda_n})P {\rm exp}(N)\end{eqnarray*} and $$ {\rm exp} (tA) = P^{-1} {\rm diag}(e^{t\lambda_1}, \ldots, e^{t\lambda_n})P {\rm exp}(tN)$$ That only leaves the expansion for ${\rm exp}(tN)$, which turns out to be something that can be worked with. Thus the Jordan Normal Form plus a little more calculation -- of the sort that you meet in \href{http://www.drps.ed.ac.uk/14-15/dpt/cxmath10066.htm}{Honours Differential Equations} -- produces a very explicit solution of the differential equation $\dot{\gamma}(t) = A \gamma(t).$
\medskip

\noindent {\bf Algebra} This is supposed to be an Algebra Course, so I will also mention explicitly the natural motivation that follows from the material in \hyperref[chap2]{Chapter \ref{chap2}}. Given a finite dimensional vector space $V$ and an endomorphism $f: V\to V$, a choice of an ordered basis $\mathcal{B}$ for $V$ determines a matrix ${}_{\mathcal{B}}[f]_{\mathcal{B}}$ representing $f$ with respect to $\mathcal{B}$. Another choice produces another matrix, but it will be conjugate to ${}_{\mathcal{B}}[f]_{\mathcal{B}}$. So this fact is motivation to find an ordered basis that simplifies the representing matrix as much as possible, or equivalently to find the simplest possible matrix that is conjugate to a given matrix. This is exactly what the Jordan Normal Form does! It is actually a special case of the \href{http://en.wikipedia.org/wiki/Structure_theorem_for_finitely_generated_modules_over_a_principal_ideal_domain}{Structure Theorem for Finitely Generated Modules over a Principal Ideal Domain}. This is a lovely classification theorem that, amongst other things, unites a description of all finite abelian groups with a description of square matrices up to conjugacy. Examples of Principal Ideal Domains are the rings $\mathbb{Z}$ and $F[X]$: $\mathbb{Z}$-modules are abelian groups; typically $F[X]$-modules have the form described in \hyperref[forlater]{Exercise \ref{forlater}}.

\begin{rem} You should compare the Jordan Normal Form with the Smith Normal Form. The Smith Normal Form looks far simpler: the reason for this is that it finds the simplest possible matrix ${}_{\mathcal{A}}[f]_{\mathcal{B}}$ where $\mathcal{A}$ and $\mathcal{B}$ are ordered bases of $V$ which may or may not be equal, whereas the Jordan Normal Form allows only for the case $\mathcal{A} = \mathcal{B}$.
\end{rem}

\section{Statement of the Jordan Normal Form and Strategy of Proof}
From now on $F$ will be an algebraically closed field, such as the field of complex numbers $F=\CC$. Since I shall be discussing eigenvalues and eigenvectors for arbitrary matrices with entries in $F$, this is crucial!


Since $F$ is algebraically closed every polynomial $F[x]$ splits as a product of linear factors. It follows from \hyperref[triendo]{the Triangularisability Proposition \ref{triendo}} that every endomorphism $f:V \to V$ of an $n$-dimensional $F$-vector space is triangularisable, i.e. there exists a basis $(\vec{v}_1,\vec{v}_2,\dots,\vec{v}_n)$ such that
$$\begin{array}{l}
f(\vec{v}_1)~=~a_{11}\vec{v}_1~,\\[1ex]
f(\vec{v}_2)~=~a_{12}\vec{v}_1+a_{22}\vec{v}_2~,\\[1ex]
\hskip12mm \vdots \\[1ex]
f(\vec{v}_n)~=~a_{1n}\vec{v}_1+a_{2n}\vec{v}_2+\dots+a_{nn}\vec{v}_n \in V
\end{array}$$
with $\chi_f(x)=(a_{11}-x)(a_{22}-x)\dots (a_{nn}-x) \in F[x]$.
Roughly speaking, the Jordan Normal Form improves this by providing such a basis for $V$ with the additional property that 
$$a_{ij}~=~\begin{cases} 
0~{\rm or}~1&{\rm if}~i=j-1\\[1ex]
0 &{\rm  if}~ i<j-1
\end{cases}$$
so that
$$\begin{array}{l}
f(\vec{v}_1)~=~a_{11}\vec{v}_1~,\\[1ex]
f(\vec{v}_2)~=~a_{12}\vec{v}_1+a_{22}\vec{v}_2~,\\[1ex]
\hskip12mm \vdots \\[1ex]
f(\vec{v}_n)~=~a_{{n-1}n}\vec{v}_{n-1}+a_{nn}\vec{v}_n \in V
\end{array}$$

\begin{definition} \label{JNblock} Given an integer $r\geqslant 1$ define an $(r\times r)$-matrix $J(r)$, called the {\bf nilpotent Jordan block of size $r$}, by the rule $J(r)_{ij} = 1 $ for $j = i+1$ and $J(r)_{ij} = 0$ otherwise.
 $$ J(r)  ~=~  \begin{pmatrix} 
0 & 1 & 0 &  \cdots & 0 &  0 \\ 
 0 & 0 & 1 & \cdots & 0 & 0\\ 
 0 & 0 & 0 & \cdots & 0 & 0\\
 \vdots & \vdots & \vdots & \ddots & \vdots &\vdots  \\
  0 & 0 & 0& \cdots & 0 & 1 \\ 0 & 0 & 0 & \cdots & 0 &0
   \end{pmatrix}~.$$
 In particular $J(1)$ is $(1\times 1)$-matrix whose only entry is zero.

 Given an integer $r\geqslant 1$ and a scalar $\lambda \in F$ define an $(r\times r)$-matrix $J(r, \lambda)$, called the {\bf Jordan block of size $r$ and eigenvalue $\lambda$}, by the rule 
 $$J(r, \lambda) ~=~ \lambda I_r+ J(r)~=~D+N$$
 with $\lambda I_r={\rm diag}(\lambda,\lambda,\dots,\lambda)=D$ diagonal and $J(r)=N$  nilpotent
 $$ J(r, \lambda )  ~=~  \begin{pmatrix} 
 \lambda & 1 & 0 &  \cdots & 0 &  0 \\ 
 0 & \lambda & 1 & \cdots & 0 & 0\\ 
 0 & 0 & \lambda & \cdots & 0 & 0\\
 \vdots & \vdots & \vdots & \ddots & \vdots &\vdots  \\
  0 & 0 & 0& \cdots & \lambda & 1 \\ 0 & 0 & 0 & \cdots & 0 & \lambda \end{pmatrix}$$
such that $DN=ND$.
\end{definition}

If $V$ is an $r$-dimensional $F$-vector space with basis $\mathcal{B}=(\vec{v}_1,\vec{v}_2,\dots,\vec{v}_r)$ and $\lambda \in F$
is a scalar, the endomorphism  $f:V \to V$ defined by
$$\begin{array}{l}
f(\vec{v}_1)~=~\lambda\vec{v}_1~,\\[1ex]
f(\vec{v}_2)~=~\vec{v}_1+\lambda \vec{v}_2~,\\[1ex]
\hskip12mm \vdots \\[1ex]
f(\vec{v}_r)~=~\vec{v}_{r-1}+\lambda \vec{v}_r \in V
\end{array}$$
has matrix $_{\mathcal B}[f]_{\mathcal B}=J(r,\lambda)$. The endomorphism
$$e~=~f-\lambda \id_V~:~V \to V~;~\vec{v} \mapsto f(\vec{v}) - \lambda\vec{v}~,~e(\vec{v}_i)~=~\vec{v}_{i-1}$$
has nilpotent matrix $_{\mathcal B}[e]_{\mathcal B}=J(r)$.
The characteristic polynomial of $f$ is $\chi_f(x)=(\lambda-x)^r \in F[x]$. It is significant that $e^r=0$ (Cayley-Hamilton!) but $e^j \neq 0$ for $j=1,2,\dots,r-1$, with 
$$V_j~=~{\rm ker}(e^j)=\langle \vec{v}_1,\vec{v}_2,\dots,\vec{v}_j\rangle \subseteq V~(1 \leqslant j \leqslant r)$$ 
a $j$-dimensional subspace such that $f(V_j) \subseteq V_j$. In particular,  the $\lambda$-eigenspace $E(\lambda,f)=V_1=\langle \vec{v}_1 \rangle$ is 1-dimensional. 
\medskip

Here is the algebraic apex of the course.



\begin{theorem}[Jordan Normal Form]\label{JNFtheorem}
Let $F$ be an algebraically closed field. Let $V$ be a finite dimensional vector space and let $\phi: V\to V$ be an endomorphism of $V$ with characteristic polynomial 
$$\chi_\phi(x)~=~(\lambda_1-x)^{a_1}(\lambda_2-x)^{a_2} \dots 
(\lambda_s-x)^{a_s} \in F[x] ~(a_i \geqslant 1,\sum\limits^s_{i=1}a_i=n)$$
for distinct $\lambda_1,\lambda_2,\dots,\lambda_s \in F$. Then  there exists an ordered basis $\mathcal{B}$ of $V$ such that the matrix of $\phi$ with respect to the basis $\mathcal{B}$ is block diagonal with Jordan blocks on the diagonal $${}_{\mathcal{B}}[\phi]_{\mathcal{B}} = {\rm diag} (J(r_{11}, \lambda_1), \ldots , J(r_{1m_1},\lambda_1),
J(r_{21},\lambda_2),\ldots,  J(r_{s m_s},\lambda_s))$$
with $r_{11},\dots,r_{1m_1},r_{21},\dots,r_{sm_s} \geqslant 1$ such that
$$a_i~=~r_{i1}+r_{i2}+ \dots + r_{im_i}~(1 \leqslant i \leqslant s)~.$$
\end{theorem}
This is a complicated theorem to prove. It involves quite a lot of abstract calculation and it is {\it very} algebraic. To help to make it easier to follow, I outline here the strategy of the proof. In the next section I  give the proof and in the section after that I illustrate the proof with an example that involves everything that was used.
\medskip

{\it Step 1:} The first step is to decompose the vector space $V$ into a direct sum $V = \oplus_{i=1}^s V_i$ according to the factorization of the characteristic polynomial as a product of linear factors
$$\chi_\phi(x)~=~(\lambda_1-x)^{a_1}(\lambda_2-x)^{a_2}\dots 
(\lambda_s-x)^{a_s} \in F[x]$$
for distinct scalars $\lambda_1,\lambda_2,\dots,\lambda_s \in F$,   where for each $i$:
\begin{itemize} 
\item $V_i={\rm ker}((\phi-\lambda_i\id_V)^{a_i}:V \to V)\subseteq V$, and
\item $\phi (V_i) \subseteq V_i$, and
\item $(\phi - \lambda_i\id_{V_i})^{m_i}$ is zero on $V_i$ for $m_i$ large enough.
\end{itemize}
This behaviour is an example of a general phenomenon from module theory called the \href{http://en.wikipedia.org/wiki/Krull-Schmidt_theorem}{Krull-Remak-Schmidt Decomposition}.
\medskip

{\it Step 2:} The outcome of the first step is to focus attention on the individual spaces $V_i$ instead of $V$. These spaces have the advantage that a power of the endomorphism $(\phi - \lambda_i\id_{V_i}): V_i \to V_i$ is zero. In other words $\psi:= \phi - \lambda_i\id_{V_i}$ is a nilpotent linear mapping on $V_i$. I already showed you this situation in \hyperref[nilpex]{Exercise \ref{nilpex}}. The proof will study a finite dimensional vector space $W$ together with a nilpotent endomorphism $\psi: W\to W$. I will show that there is an ordered basis of $W$, written $\{ \vec{v}_{11}, \vec{v}_{21}, \vec{v}_{31} , \ldots, \vec{v}_{12}, \vec{v}_{22}, \vec{v}_{32}, \ldots \}$ such that the matrix of $\psi$ with respect to this basis is block diagonal with nilpotent Jordan blocks of various sizes along the diagonal.

In my head, I picture such a basis together with $\psi$ as follows:
$$
\begin{tikzpicture}
  \draw[black, very thin, step=1cm] (-4.0,-2.0) grid (1.0,0);
\draw[black, very thin, step=1cm] (-4,0) grid (-1,1);
\draw[black, very thin, step=1cm] (-4,1) grid (-2,2);
\draw[black, very thin, step=1cm] (-4,2) grid (-3,3);
\draw(-1.1,-1.5) node {$\vec{v}_{51} \!\mapsto \! \vec{v}_{41} \mapsto \!\vec{v}_{31}\! \mapsto \vec{v}_{21}\! \mapsto \! \vec{v}_{11}\! \mapsto \! \vec{0}$};
\draw(-1.1, -0.5) node {$\vec{v}_{52} \!\mapsto \! \vec{v}_{42} \mapsto \!\vec{v}_{32}\! \mapsto \vec{v}_{22}\! \mapsto \! \vec{v}_{12}\! \mapsto \! \vec{0}$};
\draw(-2.1, 0.5) node {$\vec{v}_{33}\! \mapsto \vec{v}_{23}\! \mapsto \! \vec{v}_{13}\! \mapsto \! \vec{0}$};
\draw(-2.6, 1.5) node {$\vec{v}_{24}\! \mapsto \! \vec{v}_{14}\! \mapsto \! \vec{0}$};
\draw(-3.1, 2.5) node {$\vec{v}_{15} \mapsto \vec{0}$};
\end{tikzpicture}
$$
This picture would describe an example where $\dim W = 16$ because each of the 16 boxes represents one basis vector; the mapping $\psi$ moves from left to right through the boxes, vanishing when it reaches the outer edge of the diagram. In the example the matrix would have the form ${\rm diag}(J(5), J(5), J(3),J(2), J(1))$.
The vector space $W$ has ordered basis (partitioned  to match the Jordan blocks)
$$\begin{array}{ll}
\mathcal{B}&=~\mathcal{B}_1 \cup \mathcal{B}_2 \cup \mathcal{B}_3 \cup \mathcal{B}_4 \cup \mathcal{B}_5\\[1ex]
&=~(\vec{v}_{11},\vec{v}_{21},\vec{v}_{31},\vec{v}_{41},\vec{v}_{51}) \cup (\vec{v}_{12},\vec{v}_{22},\vec{v}_{32},\vec{v}_{42},\vec{v}_{52}) \cup
(\vec{v}_{13},\vec{v}_{23},\vec{v}_{33}) \cup (\vec{v}_{13},\vec{v}_{23}) \cup (\vec{v}_{15})
\end{array}$$
and $\psi:W \to W$ is given by
$$\begin{array}{l}
\psi(\vec{v}_{11})~=~\vec{0}~,~\psi(\vec{v}_{21})~=~\vec{v}_{11}~,~\psi(\vec{v}_{31})~=~\vec{v}_{21}~,~
\psi(\vec{v}_{41})~=~\vec{v}_{31}~,~\psi(\vec{v}_{51})~=~\vec{v}_{41}~,\\[1ex]
\psi(\vec{v}_{12})~=~\vec{0}~,~\psi(\vec{v}_{22})~=~\vec{v}_{12}~,~\psi(\vec{v}_{32})~=~\vec{v}_{22}~,~
\psi(\vec{v}_{42})~=~\vec{v}_{32}~,~\psi(\vec{v}_{52})~=~\vec{v}_{42}~,\\[1ex]
\psi(\vec{v}_{13})~=~\vec{0}~,~\psi(\vec{v}_{23})~=~\vec{v}_{13}~,~\psi(\vec{v}_{33})~=~\vec{v}_{21}~,\\[1ex]
\psi(\vec{v}_{14})~=~\vec{0}~,~\psi(\vec{v}_{24})~=~\vec{v}_{14}~,\\[1ex]
\psi(\vec{v}_{15})~=~\vec{0}~.
\end{array}$$
Define an increasing sequence of subspaces
$$W_0~=~\{0\} \subset W_1 \subset W_2 \subset W_3 \subset W_4 \subset W_5~=~W$$
with 
$$W_k~=~{\rm ker}(\psi^k:W \to W)~=~\{\vec{w} \in W\,\vert\, \psi^k(\vec{w})=\vec{0}\}~.$$
the kernel of the $k$-fold iteration of $\psi$
$$\psi^k~=~\psi \circ \psi \circ \dots \circ \psi~:~W \to W~.$$ 
Thus $W_k$ is spanned by the basis elements $\vec{v}_{ij} \in {\mathcal B}$ with $\psi^k(\vec{v}_{ij})=\vec{0} \in W$, and
$$\begin{array}{l}
W_1~=~\langle\vec{v}_{11},\vec{v}_{12},\vec{v}_{13},\vec{v}_{14},\vec{v}_{15}\rangle~,\\[1ex]
W_2~=\langle\vec{v}_{24},\vec{v}_{23},\vec{v}_{22},\vec{v}_{21}\rangle \oplus W_1~,\\[1ex]
W_3~=~ \langle\vec{v}_{33},\vec{v}_{32},\vec{v}_{31}\rangle\oplus W_2~,\\[1ex]
W_4~=~\langle\vec{v}_{42},\vec{v}_{41}\rangle\oplus W_3~,\\[1ex]
W_5~=~\langle\vec{v}_{52},\vec{v}_{51}\rangle\oplus W_4~.
\end{array}$$
The subspaces are most easily illustrated by the coloured sequence of boxes in the diagram:

\begin{minipage}[t]{0.18\textwidth}
$$ \begin{tikzpicture}[inner sep=2mm, scale=0.4]
 \draw[black, very thin, step=1cm] (-4.0,-2.0) grid (1.0,0);
\draw[black, very thin, step=1cm] (-4,0) grid (-1,1);
\draw[black, very thin, step=1cm] (-4,1) grid (-2,2);
\draw[black, very thin, step=1cm] (-4,2) grid (-3,3);
\node at (-3.5,-1.5) [rectangle, draw=blue!50, fill=blue!40] {};
\node at (-3.5,-0.5) [rectangle, draw=blue!50, fill=blue!40] {};
\end{tikzpicture}
$$
\end{minipage}
\begin{minipage}[t]{0.18\textwidth}
$$ \begin{tikzpicture}[inner sep=2mm, scale=0.4]
 \draw[black, very thin, step=1cm] (-4.0,-2.0) grid (1.0,0);
\draw[black, very thin, step=1cm] (-4,0) grid (-1,1);
\draw[black, very thin, step=1cm] (-4,1) grid (-2,2);
\draw[black, very thin, step=1cm] (-4,2) grid (-3,3);
\node at (-3.5,-1.5) [rectangle, draw=blue!50, fill=blue!40] {};
\node at (-3.5,-0.5) [rectangle, draw=blue!50, fill=blue!40] {};
\node at (-2.5,-0.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (-2.5,-1.5) [rectangle, draw=blue!50, fill=blue!20] {};
\draw(-3.1, -1.5) node {$ \mapsto$};
\draw(-3.1, -0.5) node {$ \mapsto$};
\end{tikzpicture}
$$
\end{minipage}
\begin{minipage}[t]{0.18\textwidth}
$$ \begin{tikzpicture}[inner sep=2mm, scale=0.4]
 \draw[black, very thin, step=1cm] (-4.0,-2.0) grid (1.0,0);
\draw[black, very thin, step=1cm] (-4,0) grid (-1,1);
\draw[black, very thin, step=1cm] (-4,1) grid (-2,2);
\draw[black, very thin, step=1cm] (-4,2) grid (-3,3);
\node at (-3.5,-1.5) [rectangle, draw=blue!50, fill=blue!40] {};
\node at (-3.5,-0.5) [rectangle, draw=blue!50, fill=blue!40] {};
\node at (-2.5,-0.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (-2.5,-1.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (-1.5,-0.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (-1.5,-1.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (-3.5, 0.5) [rectangle, draw=green!50, fill=green!40] {};
\draw(-3.1, -1.5) node {$ \mapsto$};
\draw(-3.1, -0.5) node {$ \mapsto$};
\draw(-2.1, -1.5) node {$ \mapsto$};
\draw(-2.1, -0.5) node {$ \mapsto$};
\end{tikzpicture}
$$
\end{minipage}
\begin{minipage}[t]{0.18\textwidth}
$$ \begin{tikzpicture}[inner sep=2mm, scale=0.4]
 \draw[black, very thin, step=1cm] (-4.0,-2.0) grid (1.0,0);
\draw[black, very thin, step=1cm] (-4,0) grid (-1,1);
\draw[black, very thin, step=1cm] (-4,1) grid (-2,2);
\draw[black, very thin, step=1cm] (-4,2) grid (-3,3);
\node at (-3.5,-1.5) [rectangle, draw=blue!50, fill=blue!40] {};
\node at (-3.5,-0.5) [rectangle, draw=blue!50, fill=blue!40] {};
\node at (-2.5,-0.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (-2.5,-1.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (-1.5,-0.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (-1.5,-1.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (-0.5,-0.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (-0.5,-1.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (-3.5, 0.5) [rectangle, draw=green!50, fill=green!40] {};
\node at (-2.5, 0.5) [rectangle, draw=green!50, fill=green!20] {};
\node at (-3.5, 1.5) [rectangle, draw=red!50, fill=red!40] {};
\draw(-3.1, -1.5) node {$ \mapsto$};
\draw(-3.1, -0.5) node {$ \mapsto$};
\draw(-2.1, -1.5) node {$ \mapsto$};
\draw(-2.1, -0.5) node {$ \mapsto$};
\draw(-1.1, -1.5) node {$ \mapsto$};
\draw(-1.1, -0.5) node {$ \mapsto$};
\draw(-3.1, 0.5) node {$ \mapsto$};
\end{tikzpicture}
$$
\end{minipage}
\begin{minipage}[t]{0.18\textwidth}
$$ \begin{tikzpicture}[inner sep=2mm, scale=0.4]
 \draw[black, very thin, step=1cm] (-4.0,-2.0) grid (1.0,0);
\draw[black, very thin, step=1cm] (-4,0) grid (-1,1);
\draw[black, very thin, step=1cm] (-4,1) grid (-2,2);
\draw[black, very thin, step=1cm] (-4,2) grid (-3,3);
\node at (-3.5,-1.5) [rectangle, draw=blue!50, fill=blue!40] {};
\node at (-3.5,-0.5) [rectangle, draw=blue!50, fill=blue!40] {};
\node at (-2.5,-0.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (-2.5,-1.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (-1.5,-0.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (-1.5,-1.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (-0.5,-0.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (-0.5,-1.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (0.5,-0.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (0.5,-1.5) [rectangle, draw=blue!50, fill=blue!20] {};
\node at (-3.5, 0.5) [rectangle, draw=green!50, fill=green!40] {};
\node at (-2.5, 0.5) [rectangle, draw=green!50, fill=green!20] {};
\node at (-1.5, 0.5) [rectangle, draw=green!50, fill=green!20] {};
\node at (-3.5, 1.5) [rectangle, draw=red!50, fill=red!40] {};
\node at (-2.5, 1.5) [rectangle, draw=red!50, fill=red!20] {};
\node at (-3.5, 2.5) [rectangle, draw=orange!50, fill=orange!40] {};
\draw(-3.1, -1.5) node {$ \mapsto$};
\draw(-3.1, -0.5) node {$ \mapsto$};
\draw(-2.1, -1.5) node {$ \mapsto$};
\draw(-2.1, -0.5) node {$ \mapsto$};
\draw(-1.1, -1.5) node {$ \mapsto$};
\draw(-1.1, -0.5) node {$ \mapsto$};
\draw(-3.1, 0.5) node {$ \mapsto$};
\draw(-0.1, -0.5) node {$ \mapsto$};
\draw(-0.1, -1.5) node {$ \mapsto$};
\draw(-2.1, 0.5) node {$ \mapsto$};
\draw(-3.1, 1.5) node {$ \mapsto$};
\end{tikzpicture}
$$
\end{minipage}
\newline
\smallskip

\noindent
The first diagram on the left has $W_4$ in white, the next one has $W_3$ in white, the next one $W_2$, the next $W_1$, and the final one $\{0\}$. The coloured boxes produce a basis for the quotient vector spaces 
$$\begin{array}{l}
W/W_4~=~\langle W_4+\vec{v}_{52},W_4+\vec{v}_{51} \rangle~,\\[1ex]
W/W_3~=~\langle W_3+\vec{v}_{52},W_3+\vec{v}_{51},W_3+\vec{v}_{41},W_3+\vec{v}_{42} \rangle~,\\[1ex]
W/W_2~=~\langle W_2+\vec{v}_{52},W_2+\vec{v}_{51},W_2+\vec{v}_{41},W_2+\vec{v}_{42},W_2+ \vec{v}_{33},W_2+\vec{v}_{32},W_2+\vec{v}_{31}\rangle~,\\[1ex]
W/W_1~=~\langle W_1+\vec{v}_{52},W_1+\vec{v}_{51},W_1+\vec{v}_{41},W_1+\vec{v}_{42},W_1+ \vec{v}_{33},W_1+\vec{v}_{32},W_1+\vec{v}_{31},\\[1ex]
\hskip75mm  W_1+\vec{v}_{24},W_1+\vec{v}_{23},W_1+\vec{v}_{22},W_1+\vec{v}_{21}\rangle~.
\end{array}$$
The more darkly coloured boxes (all on the left column) are ``generators" from which all other basis vectors (more lightly coloured with the same colour) are produced using the mapping $\psi$.\medskip

{\it Step 3:} Put Step 1 and Step 2 together.
\medskip


\section{OK, let's go! The proof of Jordan Normal Form}
\noindent {\it Step 1:} Let $\phi : V\to V$ be an endomorphism of the finite dimensional $F$-vector space $V$. Since $F$ is algebraically closed, the characteristic polynomial $\chi_\phi(x)$ decomposes into linear factors by \hyperref[decpoly]{Theorem \ref{decpoly}}. I write it as follows $$\chi_\phi(x) = (-1)^n \prod_{i=1}^s (x- \lambda_i)^{a_i} \in F[x]$$ where each $a_i$ is a positive integer, $\lambda_i \neq \lambda_j$ for $i\neq j$, and the $\lambda_i$ are the eigenvalues of $\phi$. For $1\leqslant j\leqslant s$ define $$P_j(x) = \prod_{\substack{i=1 \\ i\neq j}}^s (x-\lambda_i)^{a_i}$$

\begin{lemma} \label{hcf} There exists polynomials $Q_j(x)\in F[x]$ such that $$\sum_{j=1}^s P_j(x)Q_j(x) = 1.$$
\end{lemma}

\begin{proof} This is an application of the extended Euclidean algorithm for $F[x]$, based on \hyperref[euclid]{Theorem \ref{euclid}}. This algorithm computes the highest common factor of a set of polynomials in terms of the polynomials themselves and some subsidiary polynomials $Q_j(x)$: $$\sum_{j=1}^s P_j(x)Q_j(x) = {\rm h.c.f.}\{P_1(x), \ldots , P_s(x)\}$$ Since the highest common factor of the set of polynomials $\{ P_1(x), P_2(x), \ldots , P_s(x) \}$ is $1$, the lemma follows.
\end{proof}

The extended Euclidean algorithm for $F[x]$ works in exactly the same way as for $\mathbb{Z}$, but using
 \hyperref[euclid]{Theorem \ref{euclid}} here, the division algorithm for polynomials with coefficients in the field $F$.

\begin{definition} The {\bf generalized eigenspace} of $\phi$ with eigenvalue $\lambda_i$, $E^{\sf gen}(\lambda_i, \phi)$, is the following subspace of $V$ $$E^{\sf gen}(\lambda_i, \phi) = \{ \vec{v}\in V\, \vert\, (\phi-\lambda_i\id_V)^{a_i} (\vec{v}) = \vec{0} \}$$ The dimension of $E^{\sf gen}(\lambda_i, \phi)$ is called the {\bf algebraic multiplicity of $\phi$ with eigenvalue $\lambda_i$} while the dimension of the eigenspace $E(\lambda_i, \phi)$ is called the {\bf geometric multiplicity of $\phi$ with eigenvalue $\lambda_i$}.
\end{definition}

\begin{rem} The actual eigenspace is defined by
$$E(\lambda_i , \phi)~ =~ \{ \vec{v}\in V\,\vert\,  (\phi-\lambda_i \id_V) (\vec{v}) = \vec{0}\}~.$$ 
It is immediate that $E(\lambda_i, \phi)\subseteq E^{\sf gen}(\lambda_i, \phi)$. In particular the algebraic multiplicity of any eigenvalue must always be greater than or equal to the corresponding geometric multiplicity.
\end{rem}

\begin{definition} Let $f:X \to X$ be a mapping from a set $X$ to itself. A subset $Y\subseteq X$ is {\bf stable under $f$} precisely when $f(Y) \subseteq Y$, that is if $y \in Y$ then $f(y) \in Y$. 
\end{definition}

\begin{proposition}[The direct sum decomposition] \label{dirsumdec} For each $1\leqslant i \leqslant s$, let 
$${\mathcal B}_i~=~\{ \vec{v}_{ij} \in V \,\vert\, 1\leqslant j \leqslant a_i \}$$
 be a basis of $E^{\sf gen}(\lambda_i, \phi)$, where $a_i$ is the algebraic multiplicity of $\phi$ with eigenvalue $\lambda_i$, such that
 $\sum\limits^s_{i=1}a_i=n$ is the dimension of $V$.
\begin{enumerate}
\item Each $E^{\sf gen}(\lambda_i , \phi)$ is stable under $\phi$.
\item For each $\vec{v} \in V$ there exist unique $\vec{v}_i \in E^{\sf gen}(\lambda_i , \phi)$ such that $\vec{v} = \sum_{i=1}^s \vec{v}_i$.  In other words, there is a direct sum decomposition 
$$V~ =~ \bigoplus_{i=1}^s E^{\sf gen}(\lambda_i, \phi)$$
with $\phi$ restricting to endomorphismsof the summands
$$\phi_i~=~\phi\vert~:~E^{\sf gen}(\lambda_i, \phi)\to E^{\sf gen}(\lambda_i, \phi)~.$$
\item Then 
$$\mathcal{B} ~=~{\mathcal B}_1 \cup {\mathcal B}_2 \cup \dots 
\cup {\mathcal B}_s~=~ \{\vec{v}_{ij}\,\vert\, 1\leqslant i \leqslant s, 1\leqslant j\leqslant a_i\}$$ 
is a basis of $V$. The matrix of the endomorphism $\phi$ with respect to this basis is given by the block diagonal matrix 
$$_{\mathcal B}[\phi]_{\mathcal B}~=~\left(\begin{array}{c | c | c | c}
B_1 & 0 & 0 & 0 \\ \hline
0 & B_2 & 0& 0 \\ \hline 0 & 0 & \ddots & 0 \\ \hline 0 & 0 & 0 & B_s
\end{array}\right) \in {\sf Mat}(n;F)$$ 
with $B_i=_{{\mathcal B}_i}[\phi_i]_{{\mathcal B}_i} \in {\sf Mat}(a_i; F)$.
\end{enumerate}
\end{proposition}

\begin{proof}
(1) Let $\vec{v} \in E^{\sf gen}(\lambda_i ,\phi)$ so that $(\phi-\lambda_i\id_V)^{a_i} (\vec{v}) = \vec{0}$. Then  $$\phi(\phi-\lambda_i \id_V)~ =~ \phi^2 - \lambda_i \phi ~=~ (\phi-\lambda_i \id_V)\phi~:~V \to V~,$$
 so I deduce that for all $\vec{v} \in E^{\sf gen}(\lambda_i,\phi)$
 $$(\phi-\lambda_i\id_V)^{a_i} \phi (\vec{v})~ =~ \phi (\phi-\lambda_i\id_V)^{a_i} (\vec{v})~ = ~\phi (\vec{0})~ =~ \vec{0} \in V~.$$ This shows that $\phi(\vec{v}) \in E^{\sf gen}(\lambda_i, \phi)$ so that $E^{\sf gen}(\lambda_i , \phi)$ is indeed stable under $\phi$.

(2) By \hyperref[hcf]{Lemma \ref{hcf}} I have $1 = \sum_{j=1}^s P_j(x)Q_j(x)$ and so evaluating this at the endomorphism $\phi$ gives \begin{equation} \label{nice!} \id_V = \sum_{j=1}^s P_j(\phi)\circ Q_j(\phi)\end{equation} Therefore, for all $\vec{v}\in V$ I have $$\vec{v} = \sum_{j=1}^s P_j(\phi)\circ Q_j(\phi)(\vec{v})$$ Now I observe that $$(\phi-\lambda_j\id_V)^{a_j} \circ P_j(\phi)\circ Q_j(\phi) (\vec{v}) = \chi_\phi(\phi)\circ Q_j(\phi) (\vec{v}) = 0(\vec{v}) = \vec{0}$$ where I used the \hyperref[CHthm]{Cayley-Hamilton Theorem} for the second equality. Setting
$$\vec{v}_j~ :=~  P_j(\phi)\circ Q_j(\phi) (\vec{v}) \in E^{\sf gen}(\lambda_j, \phi)$$
we have $$\vec{v}~ =~ \sum_{j=1}^s \vec{v}_j~,$$
demonstrating that $V = \sum\limits_{j=1}^s E^{\sf gen}(\lambda_j, \phi)$.

It remains to check uniqueness in this decomposition. So suppose that $\sum_{j=1}^s \vec{v}_i = \sum_{i=1}^s \vec{w}_i$ with $\vec{v}_i, \vec{w}_i \in E^{\sf gen}(\lambda_i, \phi)$ for each $i$. This means that $\sum_{i=1}^s (\vec{v}_i - \vec{w}_i) = \vec{0}$. Given any $\vec{x}_j\in E^{\sf gen}(\lambda_j, \phi)$ I have for $k\neq j$ $$P_k(\phi) (\vec{x}_j) = \prod_{\substack{\ell=1 \\ \ell \neq k}}^s (\phi- \lambda_\ell\id_V )^{a_\ell} (\vec{x}_j) = \vec{0}$$ since $(\phi-\lambda_j\id_V)^{a_j}(\vec{x}_j) = \vec{0}$ and $(\phi-\lambda_j\id_V)^{a_j}$ is a factor of $P_k(\phi)$. So, on applying \eqref{nice!}, I find $$\vec{x}_j = \sum_{k=1}^s P_k(\phi)\circ Q_k(\phi) (\vec{x}_j) = P_j(\phi)\circ Q_j(\phi) (\vec{x}_j)$$
I apply this to the equality $\sum_{i=1}^s (\vec{v}_i - \vec{w}_i) = \vec{0}$. For each $j$ this gives $$\vec{0} = P_j(\phi)Q_j(\phi) \left(\sum_{i=1}^s (\vec{v}_i - \vec{w}_i)\right) = \sum_{i=1}^s P_j(\phi)Q_j(\phi) (\vec{v}_i - \vec{w}_i) = \vec{v}_j - \vec{w}_j.$$ It follows that $\vec{v}_j = \vec{w}_j$ for each $j$, as required.

(3) Since the set $\{ \vec{v}_{ij}: 1\leqslant j \leqslant a_i \}$ is a basis of $E^{\sf gen}(\lambda_i , \phi)$ for each $i$,  it should be clear to you that the union of these bases is a basis of $\oplus_{i=1}^s E^{\sf gen}(\lambda_i, \phi)$. If you're not sure, it is proved in the solution to \hyperref[dimofdirsum]{Exercise \ref{dimofdirsum}}. Since $V = \oplus_{i=1}^s E^{\sf gen}(\lambda_i, \phi)$ by Part (2), that deals with 
the  basis $\mathcal B$ of $V$. What is the matrix with the respect to this basis? (I really need to take an ordered basis: I will take $\vec{v}_{11}, \vec{v}_{12}, \ldots , \vec{v}_{1n_1}, \vec{v}_{21}, \ldots , \vec{v}_{2n_2}, \ldots , \vec{v}_{sn_s}$ as the ordering.) If I calculate the matrix ${}_{\mathcal{B}}[\phi]_{\mathcal{B}}$ by the usual method of \hyperref[abslinmap]{Theorem \ref{abslinmap}}, I see that since $\phi(\vec{v}_{ij}) \in E^{\sf gen}(\lambda_i, \phi)$ by Part (1), $\phi (\vec{v}_{ij})$ can be expressed as a linear combination of the vectors $\vec{v}_{ij}$ where $1\leqslant  j \leqslant a_i$. Therefore the matrix is block diagonal with the $i$-th block having size $(a_i\times a_i)$.
\end{proof}

That completes the first step of the strategy. Each matrix $B_i$ appearing in Part (3) of the \hyperref[dirsumdec]{Theorem \ref{dirsumdec}} represents the restriction of $\phi$ to $E^{\sf gen}(\lambda_i, \phi)$. This endomorphism of $E^{\sf gen}(\lambda_i, \phi)$ is special because it has the property that a power of $\phi-\lambda_i \id_{E^{\sf gen}(\lambda_i, \phi)}$ is zero.

\begin{exercise} Using \hyperref[dirsumdec]{Proposition \ref{dirsumdec}} show that:
\begin{enumerate}
\item each matrix $A\in {\sf Mat}(n;F)$ can be written as $A = D + N$ where $D$ is a diagonalisable matrix and $N$ is a nilpotent matrix and $DN = ND$;
\item the decomposition $A = D + N$ is unique.
\end{enumerate}
This decomposition is called the {\bf Jordan decomposition} of $A$; it plays a basic role in the theory of \href{http://en.wikipedia.org/wiki/Lie_algebra}{Lie algebras}.
\end{exercise}

So now to the next step, studying nilpotent endomorphisms.

\medskip

\noindent
{\it Step 2:}
Let $W$ be a finite dimensional vector space and $\psi: W\to W$ an endomorphism such that some power of $\psi$ is zero, that is $\psi^m = 0 $ for some $m$. This should remind you of \hyperref[nilpex]{Exercise \ref{nilpex}} which was also Homework 3, Exercise 6.

I will fix $m$ to be minimal:  $\psi^m = 0$ but $\psi^{m-1} \neq 0$. For $0 \leqslant i \leqslant m$ define $$W_i = {\rm ker} (\psi^i)$$ If $\vec{w}\in W_i$ then $\psi^{i+1}(\vec{w}) = \psi\circ \psi^i(\vec{w}) = \psi(\vec{0}) = \vec{0}$ so that $\vec{w}\in W_{i+1}$. It follows that $W_i \subseteq W_{i+1}$. Moreover, since $\psi^0 = \id_W$ and $\psi^m = 0$ I also see that $W_0 =  0$ and $W_m = W$. Therefore I get a chain of subspaces $$0 = W_0 \subseteq W_1 \subseteq W_2 \subseteq \cdots \subseteq W_{m-1} \subseteq W_m = W$$

\begin{lemma} \label{tech0} For each $i$, define a linear mapping $$\psi_i : \frac{W_i}{W_{i-1}} \to \frac{W_{i-1}}{W_{i-2}}$$ by $\psi_i ( \vec{w} + W_{i-1}) = \psi(\vec{w}) + W_{i-2}$ for $\vec{w} \in W_i$. Then $\psi_i$ is well-defined and injective.
\end{lemma}
\begin{proof}
Let $\vec{w}, \vec{w}'\in W_i$. First, $\psi(\vec{w}) \in W_{i-1}$ since $\psi^{i-1}(\psi(\vec{w})) = \psi^i (\vec{w}) = \vec{0}$. Second, I check that the mapping is well-defined. If $\vec{w} + W_{i-1} = \vec{w}' + W_{i-1}$ then $\vec{w} - \vec{w}' \in W_{i-1}$. Therefore $\psi^{i-1}(\vec{w}- \vec{w}') = \vec{0}$, and so $\vec{0} = \psi^{i-2} \circ \psi(\vec{w} - \vec{w}') = \psi^{i-2}\circ( \psi(\vec{w}) - \psi(\vec{w}'))$ Therefore, $\psi(\vec{w})  - \psi(\vec{w}') \in W_{i-2}$ so that $\psi(\vec{w}) + W_{i-2} = \psi(\vec{w}') + W_{i-2}$. This confirms that the mapping $\psi_i$ is well-defined.

I now have to prove that $\psi_i$ is injective. If $\psi_i (\vec{w} + W_{i-1}) = \vec{0} + W_{i-2}$ then $\psi(\vec{w}) \in W_{i-2}$ which means that $\vec{0} = \psi^{i-2}(\psi(\vec{w})) = \psi^{i-1}(\vec{w})$ so that $\vec{w} \in W_{i-1}$, or, in other words, that $\vec{w} + W_{i-1} = \vec{0} + W_{i-1}$. This proves that $\ker \psi_i$ is zero and hence that $\psi_i$ is injective.
\end{proof}

This result shows me that if I define $$d_i = \dim \left(\frac{W_i}{W_{i-1}} \right) \qquad 1\leqslant i \leqslant m$$ then $d_1 \geqslant d_{2} \geqslant \cdots \geqslant d_m.$ To refine this and help me to pick a good basis for $W$, I need a little technical lemma.

\begin{lemma}\label{tech1}
Let $f: X \to Y$ be an injective linear mapping between the $F$-vector spaces $X$ and $Y$. If $\{ \vec{x}_1, \ldots , \vec{x}_t\}$ is a linearly independent set in $X$, then $\{ f(\vec{x}_1), \ldots , f(\vec{x}_t) \}$ is a linearly independent set in $Y$.
\end{lemma}
\begin{proof}
As is usual for most of the proofs of linear independence in an abstract setting, you just need to sniff the air and then follow your nose. So let $\alpha_1, \ldots ,\alpha_t\in F$ be scalars. Suppose that $$\alpha_1f(\vec{x}_1) + \cdots + \alpha_t f(\vec{x}_t) =\vec{0}_Y$$ Then the linearity of $f$ allows me to rewrite  this equation as $$f(\alpha_1\vec{x}_1 + \cdots + \alpha_t \vec{x}_t) = \vec{0}_Y$$ Since $f$ is assumed to be injective, this means that $\alpha_1\vec{x}_1 + \cdots + \alpha_t \vec{x}_t = \vec{0}_X$. As the set $\{ \vec{x}_1, \ldots , \vec{x}_t \}$ are linearly independent, this implies that $\alpha_1 = \cdots = \alpha_t = 0$. Thus $\{ f(\vec{x}_1), \ldots , f(\vec{x}_t) \}$ is a linearly independent set.
\end{proof}

I can now develop an algorithm to construct a basis of each $W_i/W_{i-1}$. The algorithm goes as follows:
\begin{itemize}
\item Choose an arbitrary basis for $W_m/W_{m-1}$, say 
$$\{ \vec{v}_{m,1} + W_{m-1}, \vec{v}_{m,2} + W_{m-1} , \ldots , \vec{v}_{m, d_m} + W_{m-1}\}~.$$
\item Since $\psi_m : W_m/W_{m-1} \to W_{m-1}/W_{m-2}$ is injective by \hyperref[tech0]{Lemma \ref{tech0}}, \hyperref[tech1]{Lemma \ref{tech1}} proves that $\{  \psi(\vec{v}_{m,1}) + W_{m-2}, \psi(\vec{v}_{m,2}) + W_{m-2} , \ldots , \psi(\vec{v}_{m,d_m}) + W_{m-2}\}$ is a linearly independent set in $W_{m-1}/W_{m-2}$. Set $\vec{v}_{m-1,i} = \psi(\vec{v}_{m,i})$ for $1\leqslant i \leqslant d_m$.
\item Choose vectors $\{ \vec{v}_{m-1,i}: d_m+1\leqslant i \leqslant d_{m-1}\}$ so that $\{ \vec{v}_{m-1,i} + W_{s-2} : 1\leqslant i \leqslant d_{m-1} \}$ is a basis of $W_{m-1}/W_{m-2}$.
\item Repeat!
\end{itemize}
Let me be explicit about what happens with a repetition. At the $i$-th stage you will have chosen vectors $\vec{v}_{j,k} \text{ for } m+1-i \leqslant j \leqslant m, 1\leqslant k \leqslant d_{j},$ so that $\{ \vec{v}_{j,k} + W_{j-1} : 1\leqslant k \leqslant d_j\}$ is a basis of $W_{j}/W_{j-1}$. This bunch of vectors has the additional property that  $\psi(\vec{v}_{j,k}) = \vec{v}_{j-1,k}$ for $m+1-i < j \leqslant m$. You'll then define $\vec{v}_{m-i, k} = \psi(\vec{v}_{m+1-i, k})$ for $1\leqslant k \leqslant d_{m+1-i}$. By \hyperref[tech0]{Lemma \ref{tech0}} and \hyperref[tech1]{Lemma \ref{tech1}} $\{ \vec{v}_{m-i, k} + W_{m-i-1} : 1\leqslant j \leqslant d_{m+1-i} \}$ is a linearly independent set in $W_{m-i}/W_{m-i-1}$. You now choose $\{ \vec{v}_{m-i, k} : d_{m+1-i}+1\leqslant k \leqslant d_{m-i} \}$ so that $\{ \vec{v}_{m-i, k} + W_{m-i-1}: 1\leqslant k \leqslant d_{m-i} \}$ is a basis of $W_{m-i}/W_{m-i-1}$.

You reach the end of the algorithm when you have completed the $m$-th stage: this produces a basis for $W_1/W_0 = W_1$. Since $W_1 = \ker (\psi)$ all elements of this basis have the property that $\psi (\vec{v}_{1, k}) = \vec{0}$.

\begin{lemma} The set of elements $\{ \vec{v}_{j,k} : 1\leqslant j \leqslant m, 1\leqslant k\leqslant d_j \}$ constructed in the algorithm above is a basis for $W$.
\end{lemma}
\begin{proof}
I check spanning first. I will show a finer statement:

\begin{center} For $1\leqslant i \leqslant m$, the set of elements $\{ \vec{v}_{j,k} : 1\leqslant j \leqslant i, 1\leqslant k \leqslant d_j \}$ spans $W_i$. \end{center}

Of course, a statement like that is set up for a proof by induction. It holds for $i=1$ because $\{ \vec{v}_{1,k} : 1\leqslant k \leqslant d_1\}$ was constructed as a basis for $W_1$, so in particular a spanning set.

Assume that the finer statement holds for a given $i$. Let $\vec{v} \in W_{i+1}$ be an arbitrary element.  Since $\{ \vec{v}_{i+1, k} + W_{i} : 1\leqslant k \leqslant d_{i+1}\}$ is a basis for $W_{i+1}/W_i$, there exist $\alpha_1 , \ldots , \alpha_{d_{i+1}}\in F$ such that $$\vec{v} + W_{i} = \alpha_1\vec{v}_{i+1,1}  + \cdots + \alpha_{d_{i+1}} \vec{v}_{i+1, d_{i+1}} + W_{i}$$ It follows that $$\vec{v} - \alpha_1\vec{v}_{i+1,1}  - \cdots - \alpha_{d_{i+1}} \vec{v}_{i+1, d_{i+1}} \in W_{i}$$ By induction this element can be expressed as a linear combination of vectors from the set $\{ \vec{v}_{j,k} : 1\leqslant j \leqslant i, 1\leqslant k \leqslant d_j \}$, and so $\vec{v}$ can be expressed as a linear combination of element of $\{ \vec{v}_{j,k} : 1\leqslant j \leqslant i+1, 1\leqslant k \leqslant d_j \}$. This confirms the finer statement for $i+1$ and hence completes the induction.

Now I know that the set $\{ \vec{v}_{j,k} : 1\leqslant j \leqslant m, 1\leqslant k\leqslant d_j \}$ spans $W = W_m$ and that it contains $\sum_{j=1}^m d_j$ elements. I'll now explain why $\dim W = \sum_{j=1}^m d_j$. With that fact in my pocket I can apply the \hyperref[cardbasis]{Cardinality Criterion for Bases, Part (2)} to deduce that the set is a basis.

To calculate $\dim (W)$ I use repeatedly the general formula of \hyperref[vsquot]{Exercise \ref{vsquot}}: if $M$ is an $F$-vector space and $N$ a subspace of $M$ then $\dim (M/N) = \dim (M) - \dim (N)$. This gives:
\begin{eqnarray*}
\dim (W) = \dim (W_m) & =& \dim (W_m/W_{m-1}) + \dim (W_{m-1}) \\ & = & \dim(W_m/W_{m-1}) + \dim (W_{m-1}/W_{m-2}) + \dim (W_{m-2}) \\ & \vdots & \\ & = &  \dim(W_m/W_{m-1}) + \dim (W_{m-1}/W_{m-2}) + \cdots + \dim (W_1/W_0) \\ &=& \sum_{j=1}^m d_j. \end{eqnarray*}
\vspace{-0.7cm}

\end{proof}

This lemma gives me a basis of $W$ which I will order via the ordering on subscripts $(j,k) < (j',k')$ if and only if $k<k'$ or $k=k'$ and $j< j'$. So for instance $(3,2) < (1,3)$ and $(1,3)< (2,3)$ so that $\vec{v}_{1,3}$ would appear in the list after $\vec{v}_{3,2}$ but before $\vec{v}_{2,3}$.

\begin{proposition} \label{nilpJN}
Let $\mathcal{B}$ be the ordered basis of $W$ constructed above $( \vec{v}_{jk} : 1\leqslant j \leqslant m, 1\leqslant k \leqslant d_j )$. Then $${}_{\mathcal{B}}[\psi]_{\mathcal{B}} = {\rm diag}( \underbrace{J(m), \ldots , J(m)}_{d_m\text{ times}}, \underbrace{J(m-1), \ldots , J(m-1)}_{d_{m-1}-d_m \text{ times}}, \ldots , \underbrace{J(1), \ldots , J(1)}_{d_1-d_2 \text{ times}})$$ where $J(r)$ denotes the \hyperref[JNblock]{nilpotent Jordan block of size $r$}.
\end{proposition}

\begin{proof}
It follows from the explicit construction of the basis $\mathcal{B}$ that $$\psi (\vec{v}_{i,j}) = \begin{cases} \vec{v}_{i-1, j} \qquad & \text{if }i>1 \\ 0 & \text{otherwise} \end{cases}$$ This tells me that the entries of the $(i,j)$-th column of the matrix ${}_{\mathcal{B}}[\psi]_{\mathcal{B}} $ are all zero if $i=1$ and otherwise are zero everywhere except for a $1$ in the $(i-1,j)$-th row. This is the property that defines the nilpotent Jordan blocks, so I get the description I claimed.
\end{proof}

This completes Step 2 of the proof: for all nilpotent endomorphisms there exists a basis such that the representing matrix can be written as a block diagonal matrix with nilpotent Jordan blocks along the diagonal.

\begin{exercise}
Let $\psi: V\to V$ be a nilpotent endomorphism. Show that: the Jordan Normal Form of $\psi$ is unique up to re-ordering of the nilpotent Jordan blocks. Explicitly, if $\mathcal{A}$ and $\mathcal{B}$ are bases of $V$ such that $${}_{\mathcal{A}}[\psi]_{\mathcal{A}} =   {\rm diag}(J(a_1), \ldots , J(a_s)) \text{ and } {}_{\mathcal{B}}[\psi]_{\mathcal{B}} =   {\rm diag}(J(b_1), \ldots , J(b_{s'}))$$ for some positive integers $a_1, \ldots , a_s$ and $b_1, \ldots b_{s'}$, then the multisets $\{ a_1, \ldots , a_s \}$ and $\{b_1, \ldots , b_{s'}\}$ are equal.
\end{exercise}
\medskip

\noindent
{\it Step 3:} I now apply the outcome of Step 2 to each of the endomorphisms $(\phi- \lambda_i \id_V)$ restricted to $E^{\sf gen}(\lambda_i, \phi)$. This means each such endomorphism can be written as a block diagonal matrix of the form stated in \hyperref[nilpJN]{Proposition \ref{nilpJN}} for a suitable choice of basis. The endomorphism $\lambda_i \id_V$ restricted to $E^{\sf gen}(\lambda_i, \phi)$ is of course $\lambda_i \id_{E^{\sf gen}(\lambda_i, \phi)}$ and so its matrix with respect to the chosen basis is just $\lambda_i I_{a_i}$. Therefore the matrix for $\phi = \lambda_i \id_V +  (\phi- \lambda_i \id_V) $ is just $\lambda_iI_n$ plus the block diagonal matrix found above from \hyperref[nilpJN]{Proposition \ref{nilpJN}}. In other words it is a block diagonal matrix of the form stated in \hyperref[nilpJN]{Proposition \ref{nilpJN}} where I replace each $J(r)$ that appears with $J(r, \lambda_i)$.  This means that each matrix $B_i \in {\sf Mat}(a_i; F)$ appearing in \hyperref[dirsumdec]{Proposition \ref{dirsumdec}} has exactly the form that I'm looking for in the statement of the \hyperref[JNFtheorem]{Jordan Normal Form}.

\begin{exercise}
Let $\psi: V\to V$ be an endomorphism. Show that: the Jordan Normal Form of $\psi$ is unique up to re-ordering of the Jordan blocks. Explicitly, if $\mathcal{A}$ and $\mathcal{B}$ are bases of $V$ such that $${}_{\mathcal{A}}[\psi]_{\mathcal{A}} =   {\rm diag}(J(a_1, \lambda_1), \ldots , J(a_s, \lambda_s)) \text{ and } {}_{\mathcal{B}}[\psi]_{\mathcal{B}} =   {\rm diag}(J(b_1,\mu_1), \ldots , J(b_t, \mu_t))$$ for some positive integers $a_1, \ldots , a_s, b_1, \ldots b_t$ and some scalars $\lambda_1, \ldots, \lambda_s, \mu_1, \ldots , \mu_t\in F$, then the multisets $\{ (a_1, \lambda_1), \ldots , (a_s, \lambda_s) \}$ and $\{(b_1, \mu_1), \ldots , (b_t, \mu_t)\}$ are equal.
\end{exercise}
\section{Example of Jordan Normal Form}

Let $A\in {\sf Mat}(4; \mathbb{C})$ be the following matrix $$A = \begin{pmatrix} 1 & -1 & 0 & -1 \\ 0 & 2 & 0 & 1 \\ -2 & 1 & -1 & 1 \\ 2 & -1 & 2 &0 \end{pmatrix}$$ I calculate that $\chi_A(x) = (x-1)^3 (x+1)$. Therefore $P_1(x)  = (x+1)$, $P_2(x) = (x-1)^3$. To calculate the $Q_j(x)$ I work as follows: :
\begin{eqnarray*} P_2(x) &=& P_1(x) \cdot (x^2-4x+7)-8 \\ \text{so }\qquad 8 &=& P_1(x)\cdot (x^2-4x+7) - P_2(x) \end{eqnarray*} Thus I take $$Q_1(x) = \frac{1}{8}(x^2-4x+7), \, Q_2(x) = -\frac{1}{8}$$

Now let's get to work on the generalised eigenspaces. I am supposed to calculate $P_1(A)Q_1(A)$ and $P_2(A)Q_2(A)$. This is quite tedious, but produces $$P_1(A)Q_1(A) =  \frac{1}{2} \begin{pmatrix} 3 & 0 & 1 & 0 \\ -1 & 2 & -1 & 0 \\  -3 & 0 & -1 & 0 \\ 3 & 0 & 3 & 2 \end{pmatrix}, \quad P_2(A)Q_2(A) = \frac{1}{2}\begin{pmatrix} -1 & 0 & -1 & 0 \\ 1 & 0 & 1 & 0 \\ 3 & 0 & 3 & 0 \\ -3 & 0 & -3 & 0 \end{pmatrix}$$ Thanks to the proof of \hyperref[dirsumdec]{Proposition \ref{dirsumdec}(2)} the space $E^{\sf gen}(1,A)$ is the column span of the first matrix, so spanned by the vectors $(0,1,0,0)^{\sf T}, (0,0,0,1)^{\sf T}$ and $(3,-1,-3,3)^{\sf T}$, and the space $E^{\sf gen}(-1,A)$ is the column span of the second matrix, so spanned by the vector $(-1,1,3,-3)^{\sf T}$.

Now I calculate $$A - I_4 = \begin{pmatrix} 0 & -1 & 0 & -1 \\ 0 & 1 & 0 & 1 \\ -2 & 1 & -2 & 1 \\ 2 & -1 & 2 & -1 \end{pmatrix}$$ The eigenspace $E(1, A)$ is then spanned by null vectors of this: say $(1,0,-1,0)^{\sf T}$ and $(0,1,0,-1)^{\sf T}$. I calculate again: $$(A-I_4)^2  = \begin{pmatrix} -2 & 0 & -2 & 0 \\ 2 & 0 & 2 & 0 \\ 6 & 0 & 6 & 0 \\ -6 & 0 & -6 & 0 \end{pmatrix}$$ This now has nullvectors spanned by $(1,0,-1,0)^{\sf T}$, $(0,1,0,-1)^{\sf T}$ and $(0,1,0,1)^{\sf T}$. This produces vectors in $E^{\sf gen}(1,A)$, and since I know that space to be $3$-dimensional by the previous paragraph, these are a basis for it.

Similarly,
$$A + I_4 = \begin{pmatrix} 2 & -1 & 0 & -1 \\ 0 & 3 & 0 & 1 \\ -2 & 1 & 0 & 1 \\ 2 & -1 & 2 & 1 \end{pmatrix}$$ which shows me that $E(-1,A)$ is spanned by $(-1,1,3,-3)^{\sf T}$. By the earlier work, I even know that this spans $E^{\sf gen}(-1,A)$ because that space is $1$-dimensional.

I'm now ready to start the algorithm. For $E^{\sf gen}(-1,A)$ there's not much to do. I pick $\vec{u} = (-1,1,3,-3)$ as my basis for $E^{\sf gen}(-1,A)$. I know that if I apply $A$ to that it gets multiplied by $-1$. Now I look to $E^{\sf gen}(1,A)$. I want to pick a vector $\vec{v}$ in this space such that $(A-I_4)^2\vec{v} = \vec{0}$ but $(A-I_4)\vec{v} \neq \vec{0}$. This just means that I have to avoid elements in $E^{\sf gen}(1,A)$ that are linear combinations of the eigenvectors $(1,0,-1,0)^{\sf T}$ and $(0,1,0,-1)^{\sf T}$. Let me take $\vec{v}_{2,1} = (0,1,0,0)^{\sf T}$. Now I need to calculate $(A-I_4)\vec{v}_{2,1}$: it is $(-1,1,1,-1)^{\sf T}$. This is, as it has to be according to the theory expounded in Step 2, an eigenvalue of $A-I_4$. I label it by $\vec{v}_{1,1}$. I then need to find another element in $E(1,A)$ that extends $\vec{v}_{1,1}$ to a basis: I'll take $\vec{v}_{1,2} = (1,0,-1,0)^{\sf T}$.

The theory now tells me that if use $(\vec{u}, \vec{v}_{1,1}, \vec{v}_{2,1}, \vec{v}_{1,2})$ as an ordered basis for $\mathbb{C}^4$ and then calculate $A\circ : \mathbb{C}^4 \to \mathbb{C}^4$ with respect to this basis the corresponding matrix will be in Jordan Normal Form. Explicitly this will mean $$P^{-1}AP = {\rm diag}(J(-1,1), J(2, 1), J(1,1))$$ where $P$ is the change of basis matrix from the above basis to the standard basis. Written out in gory detail, it states: $$\begin{pmatrix} -1 & -1 & 0 & 1  \\ 1 & 1 & 1 & 0   \\ 3 & 1 & 0  & -1  \\ -3 & -1  & 0 & 0 \end{pmatrix}^{-1} \begin{pmatrix} 1 & -1 & 0 & -1 \\ 0 & 2 & 0 & 1 \\ -2 & 1 & -1 & 1 \\ 2 & -1 & 2 &0 \end{pmatrix} \begin{pmatrix} -1 & -1 & 0 & 1  \\ 1 & 1 & 1 & 0   \\ 3 & 1 & 0  & -1  \\ -3 & -1  & 0 & 0 \end{pmatrix} = \left(\begin{array}{c | c  c | c}
 -1 & 0 & 0 & 0 \\ \hline 0 & 1 & 1 & 0 \\ 0 & 0 & 1 & 0 \\ \hline 0 & 0 & 0 & 1 \end{array}\right)$$

\section{A Brief Explanation of the Final Step in PageRank as an Application of the Jordan Normal Form}
After the proof of \hyperref[perron]{Theorem \ref{perron}} I mentioned that the {\it practical} key to finding $\vec{v}$, the positive eigenvector with eigenvalue $1$ of the Markov matrix $M$ there, was that for an arbitrary $\vec{w} \geqslant \vec{0}$ with $|\vec{w}| = 1$ $$\lim_{k\to \infty}M^k \vec{w} = \vec{v}$$  The proof of this is relatively straightforward using Jordan Normal Form, given the following Lemma whose proof is a variation on the methods used in the proof of \hyperref[perron]{Theorem \ref{perron}}.

\begin{lemma} \label{smalleval} If $M\in {\sf Mat}(n;\mathbb{R})$ is a Markov matrix all of whose entries are positive. Consider $M$ as a complex matrix, all of whose entries happen to be real. If $\lambda\in \mathbb{C}$ is an eigenvalue of $M$ then either $\lambda = 1$ or $|\lambda| <1$.
\end{lemma}
\begin{proof}[Sketch of Proof]
Assume that $\lambda \neq 1$. Let $\vec{w}\in \mathbb{C}^n$ be an eigenvector of $M$ with eigenvalue $\lambda$. Define $\vec{w}^+$ to be the vector whose $i$-th entry is $|w_i|$, the absolute value of $w_i$. Clearly, $\vec{w}^+ \in \mathbb{R}^n$ and in fact $\vec{w}^+ \geqslant 0$. Using a version of the triangle inequality, which you met in \hyperref[normcor]{Corollary \ref{normcor}}, and the positivity of $M$ it follows that:
$$ (M \vec{w}^+)_i  = \sum_{j=1}^n |M_{ij}||{w}_j| \geqslant | \sum_{j=1}^n M_{ij}w_j| = |\lambda w_i| = |\lambda|\vec{w}_i^+$$
The triangle inequality is an equality if and only if all the $w_i$ are on a single ray in the complex plane out from the origin. If this ray makes an angle of $\theta$ radians with the positive real axis, then I could rotate this line to the positive real line; rotation corresponds to multiplying by $e^{-\sqrt{-1}\theta}$, so I'd deduce that $e^{-\sqrt{-1}\theta} \vec{w} \in \mathbb{R}_{>0}^n$. In this case, $e^{-\sqrt{-1}\theta} \vec{w}$ is a positive real eigenvector and so Step 3 of the proof of \hyperref[perron]{Theorem \ref{perron}} shows that $\lambda =1$, a contradiction. Hence the triangle inequality gives a strict inequality above and  $M \vec{w}^+ > |\lambda| \vec{w}^+$. Thus, using the notation of Step 2 of the proof of \hyperref[perron]{Theorem \ref{perron}}, $$1 = R(\vec{v}) \geqslant R(\vec{w}^+) > |\lambda|$$ This confirms the claim!
\end{proof}

How does this help? Well take $W = \langle \vec{w} \in \mathbb{R}^n : |\vec{w}| = 0 \rangle$. Since $M$ is a Markov matrix, for any $\vec{w}\in W$ $$|M \vec{w}| = \sum_{i=1}^n (M\vec{w})_i = \sum_{i=1}^n \sum_{j=1}^n M_{ij}w_j = \sum_{j=1}^n \left( \sum_{i=1}^n M_{ij}\right) w_j = \sum_{j=1}^n  w_j = 0.$$ Thus $M(W) \subseteq W$ and so I can take a basis $\mathcal{B} = (\vec{v}, \vec{b}_1, \ldots , \vec{b}_{n-1})$ of $\mathbb{R}^n$ where $(\vec{b}_1, \ldots , \vec{b}_{n-1})$ is a basis of $W$ and then see that if $P$ is the change of basis matrix from $\mathcal{B}$ to the standard basis then $$M = P^{-1} K P$$ where $K$ is a block diagonal matrix $$K = \begin{pmatrix} 1 & 0 \\ 0 & L \end{pmatrix}$$ and $L$ is an $(n-1\times n-1)$-real matrix all of whose eigenvalues have absolute value less than $1$. If I choose the basis $(\vec{b}_1, \ldots , \vec{b}_{n-1})$ to be the basis appearing in the \hyperref[JNFtheorem]{Jordan Normal Form theorem} then I will even know that the matrix $L$ has the form $$L = {\rm diag}(J(r_1, \lambda_1), \ldots , J(r_t, \lambda_t))$$ where $r_1 + \cdots + r_t = n-1$ and where the $\lambda_i \in \mathbb{C}$ are the eigenvalues of $M$ not equal to $1$. In particular $L = D+N$ where $N$ is nilpotent, $D$ is a diagonal matrix with $\lambda_i$'s along the diagonal, and $DN = ND$. Therefore $$\lim_{k\to \infty} L^k= \lim_{k\to \infty} (D+N)^k = \lim_{k\to \infty} \left(\sum_{j=1}^k \binom{n}{j} D^j N^{k-j}\right)$$ Now, since $N$ is nilpotent, it must be that $N^{n-1} = 0$ and therefore this limit is actually $$\lim_{k\to \infty} \left(\sum_{j=k-n+1}^k \binom{n}{j} D^j N^{k-j}\right)$$ But $D^j$ is a diagonal matrix whose entries are powers $\lambda_i^j$ and by \hyperref[smalleval]{Lemma \ref{smalleval}}, each $\lambda_i$ has absolute value strictly less than $1$. So as $j$ increases the absolute value of all the non-zero entries of $D^j$ tend to zero. This proves that $\lim_{k\to \infty}L^k = 0$. From this, I deduce the claim $$\lim_{k\to \infty} M^k = \lim_{k\to \infty} P^{-1} \begin{pmatrix} 1 & 0 \\ 0 & N^k \end{pmatrix} P = \lim_{k\to \infty} P^{-1} \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} P$$ Now take $\vec{w} \geqslant \vec{0}$ with $|\vec{w}| = 1$. Decompose it with respect to the basis $\mathcal{B}$ to get $\vec{w} = c\vec{v} + \sum_{i=1}^{n-1} c_i \vec{b}_i$ with $c, c_i \in \mathbb{R}$. This implies that $1 = |\vec{w}| = |c| |\vec{v}| + \sum_{i=1}^{n-1} |c_i||\vec{v}_i| = |c|$ so that $c = \pm 1$. Positivity of $\vec{w}$ implies that $c=1$ rather than $-1$. Thus $P\vec{w} = (1, c_1, \ldots , c_{n-1})^{\sf T}$. Finally, $$ P^{-1} \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} P\vec{w} = P^{-1} \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix} (1, c_1, \ldots , c_{n-1})^{\sf T} = P^{-1} (1, 0, \ldots, 0 )^{\sf T} = \vec{v}.$$ Therefore, as claimed $$\lim_{k\to \infty}M^k \vec{w} = \vec{v}$$
So the world of mathematics is in order.
\vspace{1cm}

\begin{center} {\large LONG MAY THAT LAST!} \end{center}

\end{document}
